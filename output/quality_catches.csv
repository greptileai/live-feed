repo,pr_number,pr_title,pr_url,score,comment_body,comment_url,reply_body,created_at,bug_category,severity,llm_reasoning,evaluated_at
PostHog/posthog,45413,chore: Use pre-migrated database and update comment when containers start,https://github.com/PostHog/posthog/pull/45413,,"**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: bin/hobby-ci.py
Line: 1045:1046

Comment:
**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45413#discussion_r2706357612,"Fixed in aa3a4c5. The function now creates the comment if it doesn't exist yet (similar to `update_smoke_test_comment()`), ensuring users get the early notification that the instance is accessible even when called before the final smoke test comment is created.",2026-01-20T00:08:16+00:00,logic,medium,"Greptile correctly identified that the `update_containers_started_comment()` function would silently fail when no existing comment is found - the code does have an else branch that creates a comment, but Greptile's analysis that the comment doesn't exist yet at the time cloud-init finishes (line 678) is validated by the developer's reply confirming they fixed this issue. The developer explicitly acknowledged and implemented the suggested fix.",2026-01-20T23:34:12.362182+00:00
PostHog/posthog,44605,feat: EAV property materialization for HogQL,https://github.com/PostHog/posthog/pull/44605,,"The writable table is created on `NodeRole.INGESTION_SMALL` nodes only. However, the backfill activity in `posthog/temporal/eav_backfill/activities.py` line 98 writes to `writable_event_properties` from a worker node (not an ingestion node).

This will fail if:
1. Worker nodes don't have access to `writable_event_properties` table
2. The distributed table routing expects writes from specific nodes

Verify that worker nodes can access this table, or modify the backfill to write to a table accessible from worker nodes. The DMAT backfill has the same pattern, so check if this is already tested and working.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/clickhouse/migrations/0195_event_properties_eav_table.py
Line: 23:27

Comment:
The writable table is created on `NodeRole.INGESTION_SMALL` nodes only. However, the backfill activity in `posthog/temporal/eav_backfill/activities.py` line 98 writes to `writable_event_properties` from a worker node (not an ingestion node).

This will fail if:
1. Worker nodes don't have access to `writable_event_properties` table
2. The distributed table routing expects writes from specific nodes

Verify that worker nodes can access this table, or modify the backfill to write to a table accessible from worker nodes. The DMAT backfill has the same pattern, so check if this is already tested and working.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/44605#discussion_r2685409067,This looks legit,2026-01-13T08:37:21+00:00,runtime,high,"Greptile correctly identified a cross-cutting infrastructure issue: the writable table is only created on INGESTION_SMALL nodes, but the backfill activity runs on worker nodes which may not have access to this table. The developer confirmed this is a legitimate concern with 'This looks legit'. This is a non-obvious distributed systems issue that could cause the backfill to fail at runtime.",2026-01-20T23:34:12.362224+00:00
PostHog/posthog,44605,feat: EAV property materialization for HogQL,https://github.com/PostHog/posthog/pull/44605,,"**logic:** Using `timestamp AS _timestamp` means backfilled rows can overwrite newer real-time data. ReplacingMergeTree uses `_timestamp` as the version column (ver=""_timestamp"" in table definition). If Kafka ingests a row during backfill with an earlier timestamp, the backfilled row wins despite being stale.

Use `now() AS _timestamp` to ensure real-time ingestion always wins:

```suggestion
            now() AS _timestamp,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/eav_backfill/activities.py
Line: 116:116

Comment:
**logic:** Using `timestamp AS _timestamp` means backfilled rows can overwrite newer real-time data. ReplacingMergeTree uses `_timestamp` as the version column (ver=""_timestamp"" in table definition). If Kafka ingests a row during backfill with an earlier timestamp, the backfilled row wins despite being stale.

Use `now() AS _timestamp` to ensure real-time ingestion always wins:

```suggestion
            now() AS _timestamp,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/44605#discussion_r2688468884,,2026-01-13T23:45:08+00:00,data_integrity,high,"Greptile correctly identifies a subtle data integrity issue: when backfilling historical data into a ReplacingMergeTree table, using the event's original timestamp as the version column means backfilled rows could overwrite fresher real-time data if their timestamps happen to be later. Using now() ensures backfilled data is treated as 'old' and won't override concurrent real-time ingestion. This is a non-obvious issue specific to ClickHouse's ReplacingMergeTree semantics that could cause data loss.",2026-01-20T23:34:12.362247+00:00
PostHog/posthog,44605,feat: EAV property materialization for HogQL,https://github.com/PostHog/posthog/pull/44605,,"**logic:** waiting before backfill creates data inconsistency window

current flow:
1. slot created (state=BACKFILL)
2. wait 3 minutes for cache refresh (lines 60-67)
3. backfill historical data (lines 69-88)
4. set state=READY (lines 92-101)

during the 3-minute wait, plugin-server cache refreshes and starts writing NEW events to `event_properties`, but historical events haven't been backfilled yet. queries using this EAV property will see incomplete data (new events yes, old events no).

move backfill before cache wait:
1. backfill historical data immediately
2. wait for cache refresh
3. set state=READY

this ensures all historical data exists before plugin-server starts materializing new events

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/eav_backfill/workflows.py
Line: 60:67

Comment:
**logic:** waiting before backfill creates data inconsistency window

current flow:
1. slot created (state=BACKFILL)
2. wait 3 minutes for cache refresh (lines 60-67)
3. backfill historical data (lines 69-88)
4. set state=READY (lines 92-101)

during the 3-minute wait, plugin-server cache refreshes and starts writing NEW events to `event_properties`, but historical events haven't been backfilled yet. queries using this EAV property will see incomplete data (new events yes, old events no).

move backfill before cache wait:
1. backfill historical data immediately
2. wait for cache refresh
3. set state=READY

this ensures all historical data exists before plugin-server starts materializing new events

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/44605#discussion_r2688518516,,2026-01-14T00:16:50+00:00,data_integrity,medium,"Greptile correctly identifies a data inconsistency window in the workflow. The code waits 3 minutes before backfilling, during which the plugin-server cache refreshes and starts writing NEW events to event_properties while historical data hasn't been backfilled yet. This creates a window where queries could see incomplete/inconsistent data. The suggested fix to backfill first, then wait for cache refresh, is a valid solution to prevent this race condition.",2026-01-20T23:34:12.362272+00:00
PostHog/posthog,44605,feat: EAV property materialization for HogQL,https://github.com/PostHog/posthog/pull/44605,,"**logic:** Using `timestamp AS _timestamp` means backfilled rows can overwrite newer real-time data.

ReplacingMergeTree uses `_timestamp` as the version column (`ver=""_timestamp""` in table definition). If Kafka ingests a row during backfill with timestamp T1, but the backfilled row has timestamp T2 where T2 &gt; T1, the backfilled row wins even though it's stale.

The original issue suggested using `now() AS _timestamp` to ensure real-time ingestion always wins over backfill. However, using event timestamp is more correct IF backfill always runs before ingestion starts (which would be true if the workflow order is fixed per previous comment). Should backfilled rows use event timestamp or now() as the version? This depends on whether backfill always completes before ingestion starts.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/eav_backfill/activities.py
Line: 118:120

Comment:
**logic:** Using `timestamp AS _timestamp` means backfilled rows can overwrite newer real-time data.

ReplacingMergeTree uses `_timestamp` as the version column (`ver=""_timestamp""` in table definition). If Kafka ingests a row during backfill with timestamp T1, but the backfilled row has timestamp T2 where T2 &gt; T1, the backfilled row wins even though it's stale.

The original issue suggested using `now() AS _timestamp` to ensure real-time ingestion always wins over backfill. However, using event timestamp is more correct IF backfill always runs before ingestion starts (which would be true if the workflow order is fixed per previous comment). Should backfilled rows use event timestamp or now() as the version? This depends on whether backfill always completes before ingestion starts.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/44605#discussion_r2697429680,,2026-01-16T08:13:09+00:00,data_integrity,medium,"Greptile correctly identifies a subtle data integrity issue with ReplacingMergeTree versioning. Using `timestamp AS _timestamp` (event timestamp) as the version column means if backfill runs concurrently with real-time ingestion, older backfilled rows could overwrite newer real-time data when the event timestamp is later but the data is actually stale. This is a non-obvious race condition that depends on understanding ClickHouse's ReplacingMergeTree semantics.",2026-01-20T23:34:12.362379+00:00
posthog/posthog,45527,chore(ux): remove sidepanel part 2 (Posthog AI),https://github.com/PostHog/posthog/pull/45527,,"**logic:** Duplicate tab opening: The `onClick` handler calls `newInternalTab()`, but the `Link` component with `target=""_blank""` also calls `newInternalTab()` internally (see `Link.tsx:198-202`). This could open two tabs.

Either remove `newInternalTab()` from the onClick and rely on Link's default behavior, or add `event.preventDefault()`:

```suggestion
                            onClick={(event) => {
                                event.preventDefault()
                                closeSidePanel()
                                newInternalTab(urls.ai(conversationId ?? undefined))
                            }}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/max/Max.tsx
Line: 230:233

Comment:
**logic:** Duplicate tab opening: The `onClick` handler calls `newInternalTab()`, but the `Link` component with `target=""_blank""` also calls `newInternalTab()` internally (see `Link.tsx:198-202`). This could open two tabs.

Either remove `newInternalTab()` from the onClick and rely on Link's default behavior, or add `event.preventDefault()`:

```suggestion
                            onClick={(event) => {
                                event.preventDefault()
                                closeSidePanel()
                                newInternalTab(urls.ai(conversationId ?? undefined))
                            }}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45527#discussion_r2710443166,,2026-01-20T23:38:56+00:00,logic,medium,"Greptile correctly identifies that the Link component with target=""_blank"" combined with an onClick handler that also calls newInternalTab() would result in duplicate tab opening. The Link component's internal handling of target=""_blank"" (referenced at Link.tsx:198-202) plus the explicit newInternalTab() call creates a real bug that would confuse users by opening two tabs. This is non-obvious as it requires understanding the Link component's internal behavior.",2026-01-20T23:51:02.074241+00:00
nvidia/earth2studio,635,CorrDiff CMIP6 Refactor,https://github.com/NVIDIA/earth2studio/pull/635,,"**syntax:** typo in test environment name - should be `cmip6` not `cmip4`

```suggestion
[testenv:test-corrdiff-cmip6]
```",https://github.com/NVIDIA/earth2studio/pull/635#discussion_r2710448052,,2026-01-20T23:41:45+00:00,logic,medium,Greptile correctly identified a typo: the test environment is named 'test-corrdiff-cmip4' but the description says 'CorrDiff CMIP6->ERA5' and it runs 'test_corrdiff_cmip6.py'. This mismatch would cause confusion when running tests and could lead to the wrong test environment being invoked or missed entirely when developers try to run CMIP6 tests.,2026-01-20T23:51:02.074339+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** The nvw dependency points to an SSH-based internal GitLab repository which will fail for external users who don't have access to NVIDIA's internal infrastructure. Is this nvw dependency actually used by Atlas or can it be made optional for external users?,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352000,,2026-01-20T22:52:03+00:00,logic,high,"Greptile correctly identifies that the nvw dependency uses an SSH URL to an internal NVIDIA GitLab repository (gitlab-master.nvidia.com), which would cause installation failures for any external users trying to use the Atlas model. This is a real accessibility/usability bug that could easily be overlooked in an internal code review but would break the feature for the open-source community.",2026-01-20T23:51:02.074377+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** `find_num_heads` function is not defined anywhere in this file or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352046,,2026-01-20T22:52:04+00:00,runtime,high,"I verified that `find_num_heads` is called on line 832 in the `Attention.__init__` method when `num_heads is None`, but this function is never defined in the file nor imported from any module. This would cause a `NameError` at runtime when `Attention` is instantiated without specifying `num_heads`, making it a real bug that's easy to miss in a large file.",2026-01-20T23:51:02.074419+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** Functions `conservative_interpolate` and `bilinear_interpolate` are referenced but not defined or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352076,,2026-01-20T22:52:04+00:00,runtime,high,"Greptile correctly identified that `conservative_interpolate` and `bilinear_interpolate` are referenced in the `EquiangularInterpolator.set_method()` function (lines 571-577) but are never defined or imported anywhere in the file. This would cause a `NameError` at runtime when those interpolation methods are selected, making this a real bug that could be non-obvious during code review.",2026-01-20T23:51:02.074454+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** `RangeNormalizer` class is referenced but not defined or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352094,,2026-01-20T22:52:04+00:00,runtime,medium,"Greptile correctly identified that `RangeNormalizer` is used in the `BaseProcessor.__init__` method (lines 1358-1360) but is never defined or imported anywhere in the file. This would cause a `NameError` at runtime when `normalization.lower() == 'range'`, making the code fail when that code path is executed.",2026-01-20T23:51:02.074490+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** `PatchResample` class is referenced but not defined or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352108,,2026-01-20T22:52:05+00:00,runtime,high,"Greptile correctly identified that `PatchResample` is used in the `NattenCombineDiT` class (lines around 1147-1152) when `patch_processing == 'resample'`, but this class is never defined or imported anywhere in the file. This would cause a `NameError` at runtime when using resample processing mode.",2026-01-20T23:51:02.074522+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** `DateEmbedder` class is referenced but not defined or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352138,,2026-01-20T22:52:06+00:00,runtime,medium,Greptile correctly identified that `DateEmbedder` is used on line 1234 (`self.date_embedder = DateEmbedder(hidden_channels)`) but is never defined or imported anywhere in the file. This would cause a `NameError` at runtime when `date_condition=True` is passed to `NattenCombineDiT`.,2026-01-20T23:51:02.074550+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,"**syntax:** OptionalDependencyFailure should be raised, not called as a function - use `raise OptionalDependencyFailure(""atlas"")`

```suggestion
except:
    raise OptionalDependencyFailure(""atlas"")
```",https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352221,,2026-01-20T22:52:07+00:00,runtime,high,"Greptile correctly identified that OptionalDependencyFailure is instantiated but never raised at line 39-40. Looking at the code, when the optional dependencies (timm, natten, physicsnemo, torch_harmonics) fail to import, the exception is caught but the error is silently swallowed instead of being raised, which would cause confusing NameError exceptions later when trying to use undefined classes like InverseRealSHT or NeighborhoodAttention2D. This is a real bug that would be easy to miss.",2026-01-20T23:51:02.074580+00:00
assistant-ui/assistant-ui,3084,"Revert ""fix(react): accept URL-based images in sanitizeImageContent (#3069)""",https://github.com/assistant-ui/assistant-ui/pull/3084,,"**logic:** Reverting this breaks `CloudFileAttachmentAdapter` which uploads images to cloud storage and returns HTTPS URLs. On page reload, images stored via `CloudFileAttachmentAdapter.send()` (line 83 in CloudFileAttachmentAdapter.ts) will be filtered out since they don't match the base64 regex.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react/src/legacy-runtime/runtime-cores/external-store/ThreadMessageLike.tsx
Line: 88:93

Comment:
**logic:** Reverting this breaks `CloudFileAttachmentAdapter` which uploads images to cloud storage and returns HTTPS URLs. On page reload, images stored via `CloudFileAttachmentAdapter.send()` (line 83 in CloudFileAttachmentAdapter.ts) will be filtered out since they don't match the base64 regex.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3084#discussion_r2710488009,,2026-01-21T00:02:54+00:00,logic,high,"Greptile correctly identifies that reverting this change removes support for HTTPS URLs in image validation. The diff clearly shows the removal of the `isUrl` check (`/^https?:\/\//`) that previously allowed HTTP/HTTPS URLs, which would break any functionality relying on cloud-stored images. This is a non-obvious cross-component regression that a typical reviewer might miss when looking at what appears to be a simple revert PR.",2026-01-21T00:44:23.663757+00:00
onyx-dot-app/onyx,7608,feat(desktop): Properly Sign Mac App,https://github.com/onyx-dot-app/onyx/pull/7608,,"**logic:** Missing `APPLE_ID` and `APPLE_PASSWORD` environment variables needed for macOS notarization

```suggestion
      - uses: tauri-apps/tauri-action@73fb865345c54760d875b94642314f8c0c894afa # ratchet:tauri-apps/tauri-action@action-v0.6.1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          APPLE_ID: ${{ env.APPLE_ID }}
          APPLE_PASSWORD: ${{ env.APPLE_PASSWORD }}
          APPLE_SIGNING_IDENTITY: ${{ env.CERT_ID }}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/deployment.yml
Line: 342:344

Comment:
**logic:** Missing `APPLE_ID` and `APPLE_PASSWORD` environment variables needed for macOS notarization

```suggestion
      - uses: tauri-apps/tauri-action@73fb865345c54760d875b94642314f8c0c894afa # ratchet:tauri-apps/tauri-action@action-v0.6.1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          APPLE_ID: ${{ env.APPLE_ID }}
          APPLE_PASSWORD: ${{ env.APPLE_PASSWORD }}
          APPLE_SIGNING_IDENTITY: ${{ env.CERT_ID }}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7608#discussion_r2710634935,,2026-01-21T01:29:48+00:00,logic,high,"The PR adds steps to fetch Apple credentials from AWS Secrets Manager and import certificates, but the tauri-action step only has GITHUB_TOKEN in its env block. For macOS notarization to work, APPLE_ID, APPLE_PASSWORD, and APPLE_SIGNING_IDENTITY must be passed to the tauri-action. Without these, the notarization step will fail silently or error out, defeating the purpose of the entire PR which is to 'Properly Sign Mac App'.",2026-01-21T02:59:01.660416+00:00
nvidia/fuser,5854,ir container composition,https://github.com/NVIDIA/Fuser/pull/5854,,"**logic:** Move constructor should check if container is null before calling setParent(). If other was in a moved-from state with null container, this will crash.

```suggestion
IrInterface::IrInterface(IrInterface&& other) noexcept
    : container_(std::move(other.container_)) {
  if (container_) {
    container()->setParent(this);
  }
}
```",https://github.com/NVIDIA/Fuser/pull/5854#discussion_r2710690151,,2026-01-21T02:04:00+00:00,runtime,medium,"Greptile correctly identifies that the move constructor doesn't check for null before calling container()->setParent(this). If 'other' was previously moved-from (leaving its container_ null), this would cause a null pointer dereference crash. The move assignment operator at line 48-55 already has this null check, demonstrating inconsistency in the code. This is a non-obvious defensive programming issue that could cause crashes in certain usage patterns.",2026-01-21T02:59:01.660520+00:00
nvidia/nvflare,3993,[2.7] FedAvg Merge with FedAvgEarlyStopping + InTimeAggregation,https://github.com/NVIDIA/NVFlare/pull/3993,,"**logic:** Temporarily modifying model.params creates risk if an exception occurs between lines 124-126, leaving the model in an inconsistent state without restored parameters

```suggestion
        # Save FLModel metadata (metrics, params_type, etc.) separately
        params = model.params
        try:
            model.params = {}  # Temporarily remove params to save only metadata
            fobs.dumpf(model, f""{filepath}.metadata"")
        finally:
            model.params = params  # Restore params
```",https://github.com/NVIDIA/NVFlare/pull/3993#discussion_r2710662081,,2026-01-21T01:47:08+00:00,data_integrity,medium,"Greptile correctly identifies that if fobs.dumpf() raises an exception on line 125, the model.params will remain as an empty dict since the restoration on line 126 won't execute. This leaves the FLModel in an inconsistent state with lost parameters. The try/finally pattern is the correct fix for this pattern of temporarily modifying then restoring object state.",2026-01-21T02:59:01.660563+00:00
nvidia/nvflare,3994,[2.7] Cherry pick Add recipe for xgboost,https://github.com/NVIDIA/NVFlare/pull/3994,,"**syntax:** referencing `self.data_split_mode` but the attribute is `_data_split_mode` (with underscore)

```suggestion
        dtrain = xgb.DMatrix(X_train, label=y_train, data_split_mode=self._data_split_mode)
```",https://github.com/NVIDIA/NVFlare/pull/3994#discussion_r2710644523,,2026-01-21T01:36:19+00:00,runtime,medium,Greptile correctly identified that line 73 uses `self.data_split_mode` but the attribute is defined as `_data_split_mode` (with underscore) on line 55. This would cause an AttributeError at runtime when the test tries to create the DMatrix. This is a real bug that could be easily missed since it's a subtle naming inconsistency.,2026-01-21T02:59:01.660607+00:00
nvidia/nvflare,3994,[2.7] Cherry pick Add recipe for xgboost,https://github.com/NVIDIA/NVFlare/pull/3994,,"**syntax:** same issue - referencing `self.data_split_mode` but the attribute is `_data_split_mode` (with underscore)

```suggestion
        dval = xgb.DMatrix(X_val, label=y_val, data_split_mode=self._data_split_mode)
```",https://github.com/NVIDIA/NVFlare/pull/3994#discussion_r2710644527,,2026-01-21T01:36:20+00:00,runtime,medium,Greptile correctly identified that line 74 uses `self.data_split_mode` but the attribute is defined as `self._data_split_mode` on line 53. This would cause an AttributeError at runtime when the test tries to create the DMatrix. This is a real bug that would cause test failures.,2026-01-21T02:59:01.660641+00:00
generaltranslation/gt,960,chore: upload shared id for static content,https://github.com/generaltranslation/gt/pull/960,,"**logic:** If any hash is `undefined`, it will become string `""undefined""` in the joined result, potentially causing incorrect shared static IDs

```suggestion
    const hashes = staticUpdates
      .map((update) => update.metadata.hash)
      .filter((hash): hash is string => hash !== undefined)
      .sort()
      .join('-');
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/react/parse/createInlineUpdates.ts
Line: 185:188

Comment:
**logic:** If any hash is `undefined`, it will become string `""undefined""` in the joined result, potentially causing incorrect shared static IDs

```suggestion
    const hashes = staticUpdates
      .map((update) => update.metadata.hash)
      .filter((hash): hash is string => hash !== undefined)
      .sort()
      .join('-');
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/generaltranslation/gt/pull/960#discussion_r2710617817,,2026-01-21T01:19:18+00:00,logic,medium,"The code at line 185-186 maps hashes and joins them without filtering undefined values. In JavaScript, `undefined.toString()` in array join becomes the string 'undefined', which would create incorrect shared static IDs if any update lacks a hash. This is a non-obvious bug that could cause different static content groups to get the same or incorrect IDs, and the suggested fix with a type guard filter is correct and actionable.",2026-01-21T02:59:01.660659+00:00
onyx-dot-app/onyx,7619,feat: improve prompt shortcuts with uniqueness constraints and enhancements,https://github.com/onyx-dot-app/onyx/pull/7619,,"**logic:** Missing null safety check on `user?.preferences.shortcut_enabled`. If `user` is null, this will crash. Add optional chaining or a fallback check.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/chat/components/input/ChatInputBar.tsx
Line: 447:448

Comment:
**logic:** Missing null safety check on `user?.preferences.shortcut_enabled`. If `user` is null, this will crash. Add optional chaining or a fallback check.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7619#discussion_r2710885490,Fixed.,2026-01-21T04:06:04+00:00,runtime,medium,"Looking at line 448, the code uses `user?.preferences?.shortcut_enabled` but in the diff context around line 491-494, only `user?.preferences?.shortcut_enabled` is shown with proper optional chaining. However, at line 448 shown in the original position, accessing `preferences.shortcut_enabled` without optional chaining on `preferences` could cause a crash if `user` exists but `preferences` is null/undefined. The developer confirmed and fixed the issue.",2026-01-21T04:33:00.225411+00:00
onyx-dot-app/onyx,7612,Better connector permissioning,https://github.com/onyx-dot-app/onyx/pull/7612,,"**logic:** Raising an error here will break for ConnectorCredentialPairs created without a user (e.g., in seeding flows where `user=None` is passed to `add_credential_to_connector`). The `creator_id` field is nullable in the database schema (line 575 of `connector_credential_pair.py` shows `creator_id=user.id if user else None`).

```suggestion
                    creator_id = index_attempt.connector_credential_pair.creator_id
                    if creator_id is None:
                        logger.warning(
                            f""ConnectorCredentialPair {index_attempt.connector_credential_pair.id} ""
                            ""has no creator_id, using default user path for persistent document storage""
                        )
                        user_id_str: str = ""default""
                    else:
                        user_id_str: str = str(creator_id)
```

 Are there existing ConnectorCredentialPairs in production with null creator_id values from seeding flows?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/background/indexing/run_docfetching.py
Line: 647:651

Comment:
**logic:** Raising an error here will break for ConnectorCredentialPairs created without a user (e.g., in seeding flows where `user=None` is passed to `add_credential_to_connector`). The `creator_id` field is nullable in the database schema (line 575 of `connector_credential_pair.py` shows `creator_id=user.id if user else None`).

```suggestion
                    creator_id = index_attempt.connector_credential_pair.creator_id
                    if creator_id is None:
                        logger.warning(
                            f""ConnectorCredentialPair {index_attempt.connector_credential_pair.id} ""
                            ""has no creator_id, using default user path for persistent document storage""
                        )
                        user_id_str: str = ""default""
                    else:
                        user_id_str: str = str(creator_id)
```

 Are there existing ConnectorCredentialPairs in production with null creator_id values from seeding flows?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7612#discussion_r2710766939,,2026-01-21T02:51:05+00:00,runtime,high,"Greptile correctly identifies that the new code raises a ValueError when creator_id is None, but the database schema explicitly allows null values for creator_id (as shown by 'creator_id=user.id if user else None' logic). This would cause runtime crashes for existing ConnectorCredentialPairs created through seeding flows or without a user, breaking production functionality. This is a non-obvious catch since a typical reviewer might not trace back to verify the nullability of the field.",2026-01-21T04:33:00.225507+00:00
silimate/yosys,95,Register annotation,https://github.com/Silimate/yosys/pull/95,,"**logic:** Missing bounds check for find('[') - if no '[' is found, find() returns string::npos which could cause unexpected behavior with substr()

```suggestion
					if (blast) {
					std::string base_name = cell->name.str();
					size_t bracket_pos = base_name.find('[');
					if (bracket_pos != std::string::npos) {
						base_name = base_name.substr(0, bracket_pos);
					}
					slice_name = module->uniquify(base_name + stringf(
						""%c%d%c"", format[0], slice_lsb, format[1]));
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: passes/cmds/splitcells.cc
Line: 175:176

Comment:
**logic:** Missing bounds check for find('[') - if no '[' is found, find() returns string::npos which could cause unexpected behavior with substr()

```suggestion
					if (blast) {
					std::string base_name = cell->name.str();
					size_t bracket_pos = base_name.find('[');
					if (bracket_pos != std::string::npos) {
						base_name = base_name.substr(0, bracket_pos);
					}
					slice_name = module->uniquify(base_name + stringf(
						""%c%d%c"", format[0], slice_lsb, format[1]));
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Silimate/yosys/pull/95#discussion_r2710813579,,2026-01-21T03:19:56+00:00,runtime,medium,"Greptile correctly identified that when `blast` is true, if the cell name doesn't contain '[', `find('[')` returns `std::string::npos` (-1 as size_t), and `substr(0, npos)` would return the entire string (which is actually safe in C++), but the issue is real in that the code assumes there's always a '[' in the name when blasting, which may not be guaranteed. This is a subtle edge case that could cause unexpected naming behavior.",2026-01-21T04:33:00.225610+00:00
eventual-inc/daft,6071,feat: Support default alias for non-column refs for sql select,https://github.com/Eventual-Inc/Daft/pull/6071,,"**logic:** test expects ""Expressions in aggregations"" error message, but the actual error message from `mod.rs:302` is ""Expected aggregation (or a foldable literal) in select() only when aggregation expressions are present without groupby.""

```suggestion
    with pytest.raises(ValueError, match=""Expected aggregation""):
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: tests/dataframe/test_select_global_agg.py
Line: 58:58

Comment:
**logic:** test expects ""Expressions in aggregations"" error message, but the actual error message from `mod.rs:302` is ""Expected aggregation (or a foldable literal) in select() only when aggregation expressions are present without groupby.""

```suggestion
    with pytest.raises(ValueError, match=""Expected aggregation""):
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Eventual-Inc/Daft/pull/6071#discussion_r2710979143,,2026-01-21T05:03:49+00:00,logic,medium,Greptile correctly identified that the test's expected error message 'Expressions in aggregations' doesn't match the actual error message from the Rust code which is 'Expected aggregation (or a foldable literal) in select() only when aggregation expressions are present without groupby.' This would cause the test to fail since pytest.raises with match parameter uses regex matching. This is a real bug that would cause test failures and is non-obvious since it requires cross-referencing the Rust source code.,2026-01-21T06:21:26.602652+00:00
democratizedspace/dspace,2932,Stabilize process creation persistence and service worker e2e runs,https://github.com/democratizedspace/dspace/pull/2932,,"**logic:** calling `db.processes.add()` with an existing ID will cause a duplicate key error in IndexedDB because `createProcess` already called `db.processes.add()` with this ID

The issue: `createProcess` (line 450-460 in customcontent.js) generates an ID and calls `db.processes.add()`. If that succeeds but the follow-up `getProcess` fails (e.g., timing issue), this retry attempts to add the *same ID again*, which will throw a constraint violation.

```suggestion
                if (!persistedProcess) {
                    console.warn('Process was created but not immediately readable, retrying persistence check');
                    // Wait briefly and retry the read instead of re-adding
                    await new Promise(resolve => setTimeout(resolve, 100));
                    persistedProcess = await getProcess(createdId).catch(() => null);
                    
                    if (!persistedProcess) {
                        console.error('Process creation may have failed - process not found after retry');
                    }
                }
```",https://github.com/democratizedspace/dspace/pull/2932#discussion_r2711113991,,2026-01-21T06:13:13+00:00,data_integrity,medium,"Greptile correctly identifies that if createProcess() successfully adds a record with createdId but getProcess() fails for some reason (timing, transient error), the fallback code attempts db.processes.add() with the same ID, which will throw a duplicate key constraint error in IndexedDB. The suggested fix of retrying the read instead of re-adding is appropriate.",2026-01-21T06:21:26.602747+00:00
assistant-ui/assistant-ui,3088,docs: tap/docs (scaffolding),https://github.com/assistant-ui/assistant-ui/pull/3088,,"**logic:** incorrect path for tap-docs content - should be `tap-docs` not `docs`

```suggestion
  const path = `apps/docs/content/tap-docs/${page.path}`;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/docs/app/tap/docs/[[...slug]]/page.tsx
Line: 40:40

Comment:
**logic:** incorrect path for tap-docs content - should be `tap-docs` not `docs`

```suggestion
  const path = `apps/docs/content/tap-docs/${page.path}`;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3088#discussion_r2711621105,,2026-01-21T09:08:00+00:00,logic,medium,"The file is located at `apps/docs/app/tap/docs/[[...slug]]/page.tsx` which is the tap-docs section, and it uses `tapDocs` source. The path variable should point to `content/tap-docs/` to match the tap-docs content directory, but it incorrectly points to `content/docs/`. This would cause the GitHub edit URL to point to the wrong file location, breaking the 'edit on GitHub' functionality for tap-docs pages.",2026-01-21T10:18:52.552730+00:00
democratizedspace/dspace,2971,Hide zero-count inventory items by default,https://github.com/democratizedspace/dspace/pull/2971,,"**logic:** Missing `fullItemList` in reactive dependencies - if it updates after `onMount`, `allItems` won't rebuild

```suggestion
    $: {
        if (isClientSide && fullItemList) {
            allItems = buildAllItems($state.inventory);
            currentInventory = showAllItems
                ? allItems
                : filterNonZeroInventory($state.inventory);
        }
    }
```",https://github.com/democratizedspace/dspace/pull/2971#discussion_r2711632757,,2026-01-21T09:10:42+00:00,logic,medium,"Greptile correctly identified that `buildAllItems` depends on `fullItemList` (which is set asynchronously in `onMount`), but the reactive block only tracks `isClientSide`, `$state.inventory`, and `showAllItems`. If `fullItemList` updates after the reactive block first runs, `allItems` won't rebuild with the complete item list. Adding `fullItemList` to the reactive dependencies ensures the inventory rebuilds when the full item catalog becomes available.",2026-01-21T10:18:52.552815+00:00
alpic-ai/skybridge,351,feat: add repo option to create command,https://github.com/alpic-ai/skybridge/pull/351,,"**logic:** When downloading from a repo, the `package.json` name field isn't updated to match the user's chosen project name. The template path (lines 155-160) updates the name, but this path doesn't. Users scaffolding `my-app` from `ecom-carousel` will have `""name"": ""ecom-carousel""` in their `package.json` instead of `""name"": ""my-app""`.

```suggestion
  if (argRepo) {
    prompts.log.step(`Downloading ${argRepo}...`);
    try {
      await downloadTemplate(argRepo, { dir: root });
      // Update project name in package.json
      const name = path.basename(root);
      const pkgPath = path.join(root, ""package.json"");
      if (fs.existsSync(pkgPath)) {
        const pkg = fs.readFileSync(pkgPath, ""utf-8"");
        const pkgJson = JSON.parse(pkg);
        pkgJson.name = name;
        fs.writeFileSync(pkgPath, JSON.stringify(pkgJson, null, 2) + ""\n"");
      }
      prompts.log.success(`Project created in ${root}`);
    } catch (error) {
      prompts.log.error(""Failed to download repository"");
      console.error(error);
      process.exit(1);
    }
  }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/create-skybridge/src/index.ts
Line: 129:138

Comment:
**logic:** When downloading from a repo, the `package.json` name field isn't updated to match the user's chosen project name. The template path (lines 155-160) updates the name, but this path doesn't. Users scaffolding `my-app` from `ecom-carousel` will have `""name"": ""ecom-carousel""` in their `package.json` instead of `""name"": ""my-app""`.

```suggestion
  if (argRepo) {
    prompts.log.step(`Downloading ${argRepo}...`);
    try {
      await downloadTemplate(argRepo, { dir: root });
      // Update project name in package.json
      const name = path.basename(root);
      const pkgPath = path.join(root, ""package.json"");
      if (fs.existsSync(pkgPath)) {
        const pkg = fs.readFileSync(pkgPath, ""utf-8"");
        const pkgJson = JSON.parse(pkg);
        pkgJson.name = name;
        fs.writeFileSync(pkgPath, JSON.stringify(pkgJson, null, 2) + ""\n"");
      }
      prompts.log.success(`Project created in ${root}`);
    } catch (error) {
      prompts.log.error(""Failed to download repository"");
      console.error(error);
      process.exit(1);
    }
  }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/alpic-ai/skybridge/pull/351#discussion_r2711777368,,2026-01-21T09:48:34+00:00,logic,medium,"Greptile correctly identified that the `else` branch (lines 145-168) updates the package.json name field to match the target directory, but the `if (argRepo)` branch (lines 129-138) does not perform this update. This means users creating projects with `--repo` flag will have mismatched package names. This is a non-obvious inconsistency that would likely be missed in review since the logic exists in one code path but not the parallel one.",2026-01-21T10:18:52.552865+00:00
isaac-sim/isaaclab,4406,[Newton] Articulation & Rigid Object Full Tests,https://github.com/isaac-sim/IsaacLab/pull/4406,,"**logic:** `self._asset` is referenced before being assigned (line 39)

```suggestion
            raise ValueError(f""Unsupported asset type: {asset.__class__.__name__}"")
```",https://github.com/isaac-sim/IsaacLab/pull/4406#discussion_r2712002500,,2026-01-21T10:46:07+00:00,runtime,medium,"Greptile correctly identified that on line 37, the error message references `self._asset` but `self._asset` is only assigned later on line 39. At the point of the exception, `self._asset` doesn't exist yet, which would cause an AttributeError. The fix to use `asset` instead is correct.",2026-01-21T12:23:31.942372+00:00
mellanox/libxlio,554,[CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,"**syntax:** Missing closing parenthesis for `build()` function call. Line 44 should be part of the `build()` call, not a separate statement.

```suggestion
        propagate: false
      )
      env.LINUX_BUILD_URL = build.absoluteUrl
```",https://github.com/Mellanox/libxlio/pull/554#discussion_r2711979353,,2026-01-21T10:39:59+00:00,runtime,high,"Greptile correctly identified a missing closing parenthesis for the `build()` function call. Looking at the code, line 19 starts with `def build = build job:` and the parameters list ends at line 43 with `propagate: false` but there's no closing `)` before line 45's `env.LINUX_BUILD_URL = build.absoluteUrl`. This would cause a Groovy syntax error and the CI job would fail to run. This is non-obvious because the indentation is misleading and the file is relatively long.",2026-01-21T12:23:31.942436+00:00
twentyhq/twenty,17303,[Dashboards] fix line chart duplicate widget bleed by hashing series IDs,https://github.com/twentyhq/twenty/pull/17303,,"**logic:** `JSON.stringify` doesn't guarantee consistent key ordering across JavaScript engines, which could cause different hashes for the same configuration object

Consider sorting the keys before stringifying to ensure deterministic hashing:
```suggestion
    .update(JSON.stringify(configuration, Object.keys(configuration).sort()))
```

Or use a deterministic serialization approach similar to `create-deterministic-uuid.util.ts` in this codebase

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/modules/dashboard/chart-data/utils/build-line-chart-series-id-prefix.util.ts
Line: 12:12

Comment:
**logic:** `JSON.stringify` doesn't guarantee consistent key ordering across JavaScript engines, which could cause different hashes for the same configuration object

Consider sorting the keys before stringifying to ensure deterministic hashing:
```suggestion
    .update(JSON.stringify(configuration, Object.keys(configuration).sort()))
```

Or use a deterministic serialization approach similar to `create-deterministic-uuid.util.ts` in this codebase

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17303#discussion_r2712767563,,2026-01-21T14:15:23+00:00,logic,medium,"Greptile correctly identifies that JSON.stringify doesn't guarantee consistent key ordering, which could cause the same configuration object to produce different hashes in different environments or after object manipulation. This is a real bug that could cause the 'duplicate widget bleed' issue mentioned in the PR title to resurface, and the non-deterministic nature makes it particularly hard to debug. The suggestion to use deterministic serialization is actionable and references existing code patterns in the codebase.",2026-01-21T14:23:47.434065+00:00
assistant-ui/assistant-ui,3091,[WIP] feat: add tool ui runtime platform,https://github.com/assistant-ui/assistant-ui/pull/3091,,"**logic:** Calling `close()` twice will throw due to the ""closed"" → any transition being invalid. Make this method idempotent by checking current state first.

```suggestion
  public close(): void {
    if (this._state.lifecycle === ""closed"" || this._state.lifecycle === ""closing"") {
      return;
    }
    this.transition(""closing"");
    this.transition(""closed"");
  }
```

**Context Used:** Context from `dashboard` - Ensure that calling close() on controllers is safe from double-closing by checking if the controller... ([source](https://app.greptile.com/review/custom-context?memory=034c6f0b-0068-4fcb-a5b5-9be36d94d66e))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/tool-ui-runtime/src/core/instance.ts
Line: 73:76

Comment:
**logic:** Calling `close()` twice will throw due to the ""closed"" → any transition being invalid. Make this method idempotent by checking current state first.

```suggestion
  public close(): void {
    if (this._state.lifecycle === ""closed"" || this._state.lifecycle === ""closing"") {
      return;
    }
    this.transition(""closing"");
    this.transition(""closed"");
  }
```

**Context Used:** Context from `dashboard` - Ensure that calling close() on controllers is safe from double-closing by checking if the controller... ([source](https://app.greptile.com/review/custom-context?memory=034c6f0b-0068-4fcb-a5b5-9be36d94d66e))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3091#discussion_r2712693898,,2026-01-21T13:57:34+00:00,logic,medium,"The close() method performs two transitions (closing → closed), and the assertValidToolUILifecycleTransition function would throw if close() is called again when already in 'closed' state since 'closed' → 'closing' would be invalid. This is a real bug that could cause runtime crashes in scenarios where close() might be called multiple times (common in cleanup/unmount scenarios), and the fix to make it idempotent is correct and actionable.",2026-01-21T14:23:47.434172+00:00
posthog/posthog-js,2943,fix(rollup): fix plugin execution order,https://github.com/PostHog/posthog-js/pull/2943,,"**logic:** When `options.file` is used, `chunks` is never populated, so `chunk.code` isn't updated. This means the fix doesn't apply to single-file output configurations.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/rollup-plugin/src/index.ts
Line: 68:71

Comment:
**logic:** When `options.file` is used, `chunks` is never populated, so `chunk.code` isn't updated. This means the fix doesn't apply to single-file output configurations.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog-js/pull/2943#discussion_r2712672684,,2026-01-21T13:52:13+00:00,logic,medium,"Greptile correctly identified that the code only populates the `chunks` object inside the `if (options.dir)` block's for-loop, but when `options.file` is set (single-file output), the old code used to handle it with `args.push('--file', filePath)` which was removed. Now the for-loop still runs but without `options.dir`, basePaths only has the dirname of `options.file`, yet chunks still needs proper population for the final `chunk.code` update to work correctly for single-file configurations.",2026-01-21T14:23:47.434263+00:00
elephant-xyz/counties-trasform-scripts,649,Update hamilton transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649,,"**logic:** `sales.indexOf(rec)` inside a forEach has O(n²) complexity and returns incorrect indices if duplicate sales exist. Use the loop index instead.

```suggestion
          companyRelationshipsMap.get(key).push({ type: 'sale', saleIndex: idx + 1 });
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hamilton/scripts/data_extractor.js
Line: 1516:1516

Comment:
**logic:** `sales.indexOf(rec)` inside a forEach has O(n²) complexity and returns incorrect indices if duplicate sales exist. Use the loop index instead.

```suggestion
          companyRelationshipsMap.get(key).push({ type: 'sale', saleIndex: idx + 1 });
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649#discussion_r2712374297,,2026-01-21T12:32:20+00:00,logic,medium,"Greptile correctly identifies that using `sales.indexOf(rec)` inside the forEach callback will return incorrect indices if there are duplicate sale objects in the array, since indexOf returns the first matching index. Using the loop index parameter would be more reliable. While the forEach loop does provide an index parameter that should be used instead, this is a subtle bug that could cause data integrity issues when duplicate sales exist.",2026-01-21T14:23:47.434301+00:00
elephant-xyz/counties-trasform-scripts,649,Update hamilton transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649,,"**logic:** Company names are now stored in UPPERCASE instead of their original case. The old code preserved original casing by using `(o.name || """").trim()` in the Set. Store the original name as the value in the map, not just the key.

```suggestion
  // Only create company files for companies that have relationships
  companies = Array.from(companyRelationshipsMap.entries()).map(([_, data]) => ({
    ...appendSourceInfo(seed),
    name: data.originalName,
  }));
```

 Should company names preserve their original casing from the source data, or is uppercase normalization intentional?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hamilton/scripts/data_extractor.js
Line: 1547:1550

Comment:
**logic:** Company names are now stored in UPPERCASE instead of their original case. The old code preserved original casing by using `(o.name || """").trim()` in the Set. Store the original name as the value in the map, not just the key.

```suggestion
  // Only create company files for companies that have relationships
  companies = Array.from(companyRelationshipsMap.entries()).map(([_, data]) => ({
    ...appendSourceInfo(seed),
    name: data.originalName,
  }));
```

 Should company names preserve their original casing from the source data, or is uppercase normalization intentional?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649#discussion_r2712374358,,2026-01-21T12:32:21+00:00,data_integrity,medium,"Greptile correctly identified that the new code stores company names using uppercase keys (`(o.name || """").trim().toUpperCase()`) in `companyRelationshipsMap`, and then uses `Array.from(companyRelationshipsMap.keys())` to create company records. This means company names will be stored in UPPERCASE rather than preserving their original casing from source data, which is a data integrity issue that could affect downstream systems expecting properly-cased company names.",2026-01-21T14:23:47.434333+00:00
elephant-xyz/counties-trasform-scripts,649,Update hamilton transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649,,"**logic:** Store the original company name to preserve casing, similar to how person names are handled in `personRelationshipsMap`.

```suggestion
        if ((o.name || """").trim()) {
          const key = (o.name || """").trim().toUpperCase();
          if (!companyRelationshipsMap.has(key)) {
            companyRelationshipsMap.set(key, { originalName: (o.name || """").trim() });
          }
        }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hamilton/scripts/data_extractor.js
Line: 1511:1517

Comment:
**logic:** Store the original company name to preserve casing, similar to how person names are handled in `personRelationshipsMap`.

```suggestion
        if ((o.name || """").trim()) {
          const key = (o.name || """").trim().toUpperCase();
          if (!companyRelationshipsMap.has(key)) {
            companyRelationshipsMap.set(key, { originalName: (o.name || """").trim() });
          }
        }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649#discussion_r2712374405,,2026-01-21T12:32:22+00:00,data_integrity,medium,"Greptile correctly identifies that `personRelationshipsMap` stores original names (first_name, last_name, etc.) while `companyRelationshipsMap` only stores the uppercase key. Later at line 1548, `companies` is created from `companyRelationshipsMap.keys()` which returns uppercase names, losing the original casing. This is inconsistent with person handling and could cause data quality issues in output files.",2026-01-21T14:23:47.434405+00:00
elephant-xyz/counties-trasform-scripts,649,Update hamilton transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649,,"**logic:** Store original company name here too to preserve casing.

```suggestion
        if (!alreadyOnFirstSale) {
          if (!companyRelationshipsMap.has(key)) {
            companyRelationshipsMap.set(key, { originalName: (owner.name || """").trim() });
          }
        }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hamilton/scripts/data_extractor.js
Line: 1537:1541

Comment:
**logic:** Store original company name here too to preserve casing.

```suggestion
        if (!alreadyOnFirstSale) {
          if (!companyRelationshipsMap.has(key)) {
            companyRelationshipsMap.set(key, { originalName: (owner.name || """").trim() });
          }
        }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649#discussion_r2712374442,,2026-01-21T12:32:23+00:00,data_integrity,medium,"Greptile correctly identifies that the first pass (lines 1507-1520) stores the original company name with `companyRelationshipsMap.set(key, [])`, but the second pass (lines 1537-1541) just stores an empty array without the original name. Later at line 1555, the code uses `Array.from(companyRelationshipsMap.keys())` which returns the uppercased keys, losing the original casing. This is inconsistent with how persons preserve their original names and would result in company names being stored in all uppercase.",2026-01-21T14:23:47.434472+00:00
elephant-xyz/counties-trasform-scripts,651,Update brevard transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/651,,"**syntax:** `layoutsData` is assigned without declaration - the `const` declaration at line 3628 is commented out. This will cause a `ReferenceError` in strict mode or create an unintended global variable.

```suggestion
  let layoutsData = readJSON(layoutsPath);
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: brevard/scripts/data_extractor.js
Line: 3646:3646

Comment:
**syntax:** `layoutsData` is assigned without declaration - the `const` declaration at line 3628 is commented out. This will cause a `ReferenceError` in strict mode or create an unintended global variable.

```suggestion
  let layoutsData = readJSON(layoutsPath);
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/651#discussion_r2712370322,,2026-01-21T12:31:13+00:00,runtime,medium,"Greptile correctly identified that `layoutsData` is assigned at line 3646 but its declaration at line 3628 is commented out (`// const layoutsData = null;`). This will cause a ReferenceError in strict mode or create an implicit global variable, which is a real bug that could be easily missed during review since the commented-out declaration appears nearby.",2026-01-21T14:23:47.434511+00:00
elephant-xyz/counties-trasform-scripts,654,Update hillsborough transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/654,,"**logic:** Regex pattern has double backslash escaping issue. `/(mid\\s*century|...)` creates a literal backslash-s sequence instead of matching whitespace. This will not match ""mid century"" with spaces in the text. Should use `/mid\s*century/` to properly match the optional space(s).

```suggestion
  if (/(mid\s*century|mid-century|mcmod)/.test(text)) return ""MidCenturyModern"";
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hillsborough/scripts/structureMapping.js
Line: 125:125

Comment:
**logic:** Regex pattern has double backslash escaping issue. `/(mid\\s*century|...)` creates a literal backslash-s sequence instead of matching whitespace. This will not match ""mid century"" with spaces in the text. Should use `/mid\s*century/` to properly match the optional space(s).

```suggestion
  if (/(mid\s*century|mid-century|mcmod)/.test(text)) return ""MidCenturyModern"";
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/654#discussion_r2712368245,,2026-01-21T12:30:38+00:00,logic,medium,"Greptile correctly identified a regex escaping bug at line 125. The pattern `/(mid\s*century|mid-century|mcmod)/` uses double backslash which creates a literal backslash character followed by 's' instead of the whitespace metacharacter \s. This means 'mid century modern' style homes would not be detected correctly, causing incorrect architectural style classification.",2026-01-21T14:23:47.434542+00:00
twentyhq/twenty,17306,[Apps] Get rid of .yarn binaries in apps,https://github.com/twentyhq/twenty/pull/17306,,"**logic:** `.yarn` directory not added to gitignore. The PR description says ""add .yarn to gitignore"" but only an empty line was added. This means developers will accidentally commit `.yarn` directories.

```suggestion
generated
.yarn
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-apps/.gitignore
Line: 1:2

Comment:
**logic:** `.yarn` directory not added to gitignore. The PR description says ""add .yarn to gitignore"" but only an empty line was added. This means developers will accidentally commit `.yarn` directories.

```suggestion
generated
.yarn
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17306#discussion_r2712987528,,2026-01-21T15:06:16+00:00,logic,medium,"Greptile correctly identified that the PR description says 'add .yarn to gitignore' but the diff only shows an empty line being added, not the actual '.yarn' entry. This is a real oversight where the intended change wasn't actually made, which would lead to .yarn directories being accidentally committed to the repository.",2026-01-21T16:25:30.192758+00:00
twentyhq/twenty,17305,Rework SDK watcher,https://github.com/twentyhq/twenty/pull/17305,,"**logic:** Missing regex pattern for aliased imports like `import { something as entryName }`. This was present in the old code but is missing here.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-sdk/src/cli/utilities/build/manifest/manifest-extract-from-file-server.ts
Line: 127:129

Comment:
**logic:** Missing regex pattern for aliased imports like `import { something as entryName }`. This was present in the old code but is missing here.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17305#discussion_r2712993061,,2026-01-21T15:07:32+00:00,logic,medium,"The regex patterns at lines 120-123 only handle direct named imports `{entryName}` and default imports, but miss aliased imports like `import { something as entryName }`. This is a real functional gap that would cause the code to fail silently (returning null) when users use aliased imports, which is a common JavaScript/TypeScript pattern. This is non-obvious as it requires understanding both the regex patterns and import syntax edge cases.",2026-01-21T16:25:30.192838+00:00
onyx-dot-app/onyx,7629,Fix email addresses to use mailto links instead of https redirects,https://github.com/onyx-dot-app/onyx/pull/7629,,"**logic:** The regex incorrectly matches URLs that already have `mailto:` prefix. For example, `isEmail(""mailto:test@example.com"")` returns `true` because `mailto:test` matches `[^\s@]+` before the `@`.

When GFM autolinks a bare email like `support@example.com`, it creates `href=""mailto:support@example.com""`. This fix would then produce `mailto:mailto:support@example.com`.

Consider checking for existing `mailto:` prefix first:

```suggestion
// Simple regex to detect valid email addresses
const isEmail = (value: string | undefined): boolean => {
  if (!value) return false;
  if (value.startsWith(""mailto:"")) return false;
  return /^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(value);
};
```

Alternatively, use the existing `ensureHrefProtocol` utility from `@/lib/utils` which already handles `mailto:` links correctly.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/chat/message/MemoizedTextComponents.tsx
Line: 17:21

Comment:
**logic:** The regex incorrectly matches URLs that already have `mailto:` prefix. For example, `isEmail(""mailto:test@example.com"")` returns `true` because `mailto:test` matches `[^\s@]+` before the `@`.

When GFM autolinks a bare email like `support@example.com`, it creates `href=""mailto:support@example.com""`. This fix would then produce `mailto:mailto:support@example.com`.

Consider checking for existing `mailto:` prefix first:

```suggestion
// Simple regex to detect valid email addresses
const isEmail = (value: string | undefined): boolean => {
  if (!value) return false;
  if (value.startsWith(""mailto:"")) return false;
  return /^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(value);
};
```

Alternatively, use the existing `ensureHrefProtocol` utility from `@/lib/utils` which already handles `mailto:` links correctly.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7629#discussion_r2713270509,,2026-01-21T16:07:42+00:00,logic,medium,"Greptile correctly identifies that the regex `/^[^\s@]+@[^\s@]+\.[^\s@]+$/` would match 'mailto:test@example.com' (where 'mailto:test' is the part before @), causing a double-prefixing bug resulting in 'mailto:mailto:support@example.com'. This is a real bug that would cause broken email links when GFM auto-links emails, and the edge case is non-obvious.",2026-01-21T16:25:30.192893+00:00
mellanox/libxlio,554,[CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,"**logic:** `readFile()` will fail if `check_chaos_results.sh` doesn't create `chaos_results.txt` (e.g., if the check step fails or is skipped). Pipeline will error trying to send email.

```suggestion
    def envFile = fileExists(""${env.WORKSPACE}/chaos_results.txt"") ? readFile(""${env.WORKSPACE}/chaos_results.txt"") : ""status=UNKNOWN\nexpected_failures=\nunexpected_passes=""
```",https://github.com/Mellanox/libxlio/pull/554#discussion_r2712797885,,2026-01-21T14:21:54+00:00,runtime,medium,"Greptile correctly identifies that the pipeline_stop block uses readFile() on chaos_results.txt without checking if the file exists. If the 'Check Results' step fails or the script doesn't create the file, the readFile() call will throw an exception, preventing the notification email from being sent - which is precisely when you'd want notification the most. This is a non-obvious edge case in CI pipeline error handling.",2026-01-21T16:25:30.192983+00:00
mellanox/libxlio,554,[CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,"**logic:** `.ci/scripts/run_step.sh` does not exist in this PR. All CI steps will fail immediately when trying to execute this missing script. You must add this file to the PR.

The README mentions ""Wrap CI steps with `run_step.sh`"" but the script was never included.",https://github.com/Mellanox/libxlio/pull/554#discussion_r2712939446,,2026-01-21T14:55:07+00:00,runtime,critical,"The diff shows all CI steps being modified to call `.ci/scripts/run_step.sh`, but this script is not included anywhere in the diff. If this file doesn't exist in the repository and wasn't added in this PR, every single CI step would fail immediately with a 'file not found' error, completely breaking the CI pipeline.",2026-01-21T16:25:30.193023+00:00
delta-exchange/chatwoot,136,Background Jobs Optimisation and cleaup,https://github.com/delta-exchange/chatwoot/pull/136,,"**logic:** Missing `primary_actor_type` filter - this will delete notifications from other notification types (like SLA events) that happen to share the same `primary_actor_id`. Should filter by both `primary_actor_type` and `primary_actor_id`.

```suggestion
    Notification.where(
      user_id: notification.user_id,
      primary_actor_type: notification.primary_actor_type,
      primary_actor_id: notification.primary_actor_id
    ).where.not(id: notification.id).delete_all
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: app/jobs/notification/delivery_job.rb
Line: 30:34

Comment:
**logic:** Missing `primary_actor_type` filter - this will delete notifications from other notification types (like SLA events) that happen to share the same `primary_actor_id`. Should filter by both `primary_actor_type` and `primary_actor_id`.

```suggestion
    Notification.where(
      user_id: notification.user_id,
      primary_actor_type: notification.primary_actor_type,
      primary_actor_id: notification.primary_actor_id
    ).where.not(id: notification.id).delete_all
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/delta-exchange/chatwoot/pull/136#discussion_r2712919668,,2026-01-21T14:50:22+00:00,data_integrity,high,"Greptile correctly identifies that the `remove_duplicate_notifications` method only filters by `user_id` and `primary_actor_id`, missing the `primary_actor_type` filter. This means if a user has notifications for different entity types (e.g., a Conversation with id=5 and an SLA event with id=5), this code would incorrectly delete the SLA notification when processing the Conversation notification, causing data loss. This is a non-obvious polymorphic association bug that could silently delete legitimate notifications.",2026-01-21T16:25:30.193057+00:00
alpaca-network/railway-grafana-stack,85,"Improve alert provisioning, FastAPI schemas, and telemetry observability",https://github.com/Alpaca-Network/railway-grafana-stack/pull/85,,"**logic:** `host.docker.internal` doesn't work on Linux. In local Docker Compose, services should connect via service name `fastapi_app` (as defined in the original logic). This breaks local Linux setups.

```suggestion
    TARGET=""fastapi_app:8000""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: prometheus/entrypoint.sh
Line: 32:32

Comment:
**logic:** `host.docker.internal` doesn't work on Linux. In local Docker Compose, services should connect via service name `fastapi_app` (as defined in the original logic). This breaks local Linux setups.

```suggestion
    TARGET=""fastapi_app:8000""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/85#discussion_r2712952298,,2026-01-21T14:58:07+00:00,logic,medium,"Greptile correctly identifies that `host.docker.internal` doesn't work on native Linux Docker (only Mac/Windows). The original code used `fastapi_app:8000` for local Docker Compose, which works across all platforms via Docker's internal DNS. This change would break local development on Linux, a common deployment platform, and the fix is straightforward - revert to the original service name approach.",2026-01-21T16:25:30.193145+00:00
alpaca-network/railway-grafana-stack,84,"Created new abnormal alarm policy that will send alerts within anomalies such as latency, traffic, etc. Refactored current gateway page:",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84,,"**logic:** Traffic spike warning route is missing severity matcher unlike the critical route - this will catch all traffic_spike alerts regardless of severity. Should this warning route only match alerts without a severity label, or should it include a severity matcher for consistency?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: grafana/provisioning/alerting/notification_policies.yml
Line: 39:51

Comment:
**logic:** Traffic spike warning route is missing severity matcher unlike the critical route - this will catch all traffic_spike alerts regardless of severity. Should this warning route only match alerts without a severity label, or should it include a severity matcher for consistency?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84#discussion_r2712891889,,2026-01-21T14:44:02+00:00,logic,medium,"Greptile correctly identified that the traffic spike warning route (lines 39-51) lacks a severity matcher while the critical route (lines 21-37) explicitly matches `severity: critical`. This means all traffic_spike alerts, including critical ones, will also match the warning route and potentially be routed to the wrong receiver. The same pattern issue exists for error_rate_spike, latency_anomaly, and availability_drop routes where warning routes don't filter by severity.",2026-01-21T16:25:30.193180+00:00
alpaca-network/railway-grafana-stack,84,"Created new abnormal alarm policy that will send alerts within anomalies such as latency, traffic, etc. Refactored current gateway page:",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84,,"**logic:** Division by $B (baseline) could cause issues if the baseline value is zero or very close to zero, potentially causing alert evaluation errors.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: grafana/provisioning/alerting/rules/latency_anomalies.yml
Line: 62:62

Comment:
**logic:** Division by $B (baseline) could cause issues if the baseline value is zero or very close to zero, potentially causing alert evaluation errors.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84#discussion_r2712891963,,2026-01-21T14:44:02+00:00,runtime,medium,"Greptile correctly identifies that the expression '($A / $B > 2) && ($A > 1.0)' at line 62 (and similar at line 110) performs division by $B (the baseline latency), which could be zero or near-zero during startup, data gaps, or low-traffic periods, causing alert evaluation errors or undefined behavior. This is a non-obvious edge case that could cause false alerts or alert system failures.",2026-01-21T16:25:30.193214+00:00
alpaca-network/railway-grafana-stack,84,"Created new abnormal alarm policy that will send alerts within anomalies such as latency, traffic, etc. Refactored current gateway page:",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84,,"**logic:** Division by zero could occur if baseline (B.Value) is 0. Consider adding a safety check like `(div (max $values.A.Value 0.001) (max $values.B.Value 0.001))`.

```suggestion
            This is {{ printf ""%.1f"" (div (max $values.A.Value 0.001) (max $values.B.Value 0.001)) }}x normal error rate.
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: grafana/provisioning/alerting/rules/error_rate_anomalies.yml
Line: 65:65

Comment:
**logic:** Division by zero could occur if baseline (B.Value) is 0. Consider adding a safety check like `(div (max $values.A.Value 0.001) (max $values.B.Value 0.001))`.

```suggestion
            This is {{ printf ""%.1f"" (div (max $values.A.Value 0.001) (max $values.B.Value 0.001)) }}x normal error rate.
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84#discussion_r2712892032,,2026-01-21T14:44:03+00:00,runtime,medium,"Greptile correctly identified a division by zero risk at line 65 where `(div $values.A.Value $values.B.Value)` is used in the annotation template. If the 24h average baseline ($values.B.Value) is 0 (which could happen for new deployments or services with historically zero errors), this division would fail or produce undefined behavior when the alert fires.",2026-01-21T16:25:30.193240+00:00
alpaca-network/railway-grafana-stack,84,"Created new abnormal alarm policy that will send alerts within anomalies such as latency, traffic, etc. Refactored current gateway page:",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84,,"**logic:** Division by zero could occur if $B is 0. Consider adding a safety check like `($B > 0) && ($A / $B > 3) && ($A <= 10)`.

```suggestion
              expression: '($B > 0) && ($A / $B > 3) && ($A <= 10)'
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: grafana/provisioning/alerting/rules/error_rate_anomalies.yml
Line: 87:87

Comment:
**logic:** Division by zero could occur if $B is 0. Consider adding a safety check like `($B > 0) && ($A / $B > 3) && ($A <= 10)`.

```suggestion
              expression: '($B > 0) && ($A / $B > 3) && ($A <= 10)'
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84#discussion_r2712892109,,2026-01-21T14:44:03+00:00,runtime,medium,"The expression '$A / $B > 3' on line 87 will cause a division by zero error if the 24-hour average error rate ($B) is 0, which is a realistic scenario when there have been no errors in the baseline period. This is a non-obvious bug that could cause the alert rule to fail or behave unexpectedly in production.",2026-01-21T16:25:30.193268+00:00
useautumn/autumn,604,feat: 🎸 api customer v5 (stack layer 2: API V2.1),https://github.com/useautumn/autumn/pull/604,,"**logic:** Potential logic issue: using `||` operator means if `fromBreakdown` is 0 (valid value), it will fallback to `legacyData?.prepaid_quantity`. Use nullish coalescing (`??`) instead to only fallback when `fromBreakdown` is null/undefined

```suggestion
		prepaidQuantity = fromBreakdown ?? legacyData?.prepaid_quantity ?? 0;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: shared/api/customers/cusFeatures/changes/V1.2_CusFeatureChange.ts
Line: 128:128

Comment:
**logic:** Potential logic issue: using `||` operator means if `fromBreakdown` is 0 (valid value), it will fallback to `legacyData?.prepaid_quantity`. Use nullish coalescing (`??`) instead to only fallback when `fromBreakdown` is null/undefined

```suggestion
		prepaidQuantity = fromBreakdown ?? legacyData?.prepaid_quantity ?? 0;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/useautumn/autumn/pull/604#discussion_r2713194929,,2026-01-21T15:50:34+00:00,logic,medium,Greptile correctly identifies that using `||` instead of `??` will cause `fromBreakdown` values of 0 (a perfectly valid prepaid quantity) to incorrectly fall back to `legacyData?.prepaid_quantity`. This is a subtle but real bug that could cause incorrect balance calculations when a user has exactly 0 prepaid quantity.,2026-01-21T16:25:30.193294+00:00
alpic-ai/skybridge,357,Introduce two hooks for dev command : typescript check and nodemon,https://github.com/alpic-ai/skybridge/pull/357,,"**syntax:** `qsdqs` is not exported by `generateHelpers()` and appears to be test/debug code

```suggestion
export const { useToolInfo, useCallTool } = generateHelpers<AppType>();
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/create-skybridge/template/web/src/helpers.ts
Line: 4:4

Comment:
**syntax:** `qsdqs` is not exported by `generateHelpers()` and appears to be test/debug code

```suggestion
export const { useToolInfo, useCallTool } = generateHelpers<AppType>();
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/alpic-ai/skybridge/pull/357#discussion_r2713234092,,2026-01-21T15:59:13+00:00,runtime,medium,"Greptile correctly identified that 'qsdqs' appears to be debug/test code that was accidentally left in - it's a nonsensical variable name being destructured from generateHelpers() which would either cause a runtime error or result in an undefined value. This is in a template file that gets generated for new projects, making it particularly important to catch.",2026-01-21T16:25:30.193358+00:00
alpic-ai/skybridge,357,Introduce two hooks for dev command : typescript check and nodemon,https://github.com/alpic-ai/skybridge/pull/357,,"**logic:** New `Writable` stream created on every `readable` event without cleanup, causing potential memory leak. Set up stream once in useEffect instead. Is there a specific reason the stream needs to be recreated on each readable event rather than set up once?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/core/src/cli/use-nodemon.ts
Line: 30:45

Comment:
**logic:** New `Writable` stream created on every `readable` event without cleanup, causing potential memory leak. Set up stream once in useEffect instead. Is there a specific reason the stream needs to be recreated on each readable event rather than set up once?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/alpic-ai/skybridge/pull/357#discussion_r2713234138,,2026-01-21T15:59:14+00:00,resource_leak,medium,"Greptile correctly identifies that a new Writable stream is created every time the 'readable' event fires (line 30-43), but these streams are never cleaned up or destroyed. The 'readable' event can fire multiple times during nodemon's lifecycle, and each time a new stream is piped to stdout without removing the previous one, causing accumulated memory usage. This is a non-obvious memory leak that a typical reviewer might miss.",2026-01-21T16:25:30.193385+00:00
credal-ai/fern-docs,300,repair some broken links,https://github.com/Credal-ai/fern-docs/pull/300,,"**logic:** The link text says ""uploadDocumentContents API endpoint"" but the URL points to the `metadata` endpoint. Should be `/api-reference/v-0/document-catalog/upload-document-contents` instead.

```suggestion
   - The Name must match the name of a metadata field you have on the documents in your collection. That metadata can be set when uploading the data via the [uploadDocumentContents API endpoint](/api-reference/v-0/document-catalog/upload-document-contents) or patched in after the fact using the [metadata endpoint](/api-reference/v-0/document-catalog/metadata).
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: fern/docs/pages/platform/agents/in-depth-overview/agent-configure-sections/tools.mdx
Line: 31:31

Comment:
**logic:** The link text says ""uploadDocumentContents API endpoint"" but the URL points to the `metadata` endpoint. Should be `/api-reference/v-0/document-catalog/upload-document-contents` instead.

```suggestion
   - The Name must match the name of a metadata field you have on the documents in your collection. That metadata can be set when uploading the data via the [uploadDocumentContents API endpoint](/api-reference/v-0/document-catalog/upload-document-contents) or patched in after the fact using the [metadata endpoint](/api-reference/v-0/document-catalog/metadata).
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Credal-ai/fern-docs/pull/300#discussion_r2713269435,,2026-01-21T16:07:27+00:00,logic,low,"Greptile correctly identified that the link text says 'uploadDocumentContents API endpoint' but points to the metadata endpoint. The PR was already fixing broken links by removing 'api-reference/' from paths, but this inconsistency between link text and destination remained. While documentation errors are typically less critical, this is a valid catch that could confuse users trying to find the correct API endpoint.",2026-01-21T16:25:30.193411+00:00
