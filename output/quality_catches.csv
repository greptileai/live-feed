repo,pr_number,pr_title,pr_url,score,comment_body,comment_url,reply_body,created_at,bug_category,severity,llm_reasoning,evaluated_at
PostHog/posthog,45413,chore: Use pre-migrated database and update comment when containers start,https://github.com/PostHog/posthog/pull/45413,,"**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: bin/hobby-ci.py
Line: 1045:1046

Comment:
**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45413#discussion_r2706357612,"Fixed in aa3a4c5. The function now creates the comment if it doesn't exist yet (similar to `update_smoke_test_comment()`), ensuring users get the early notification that the instance is accessible even when called before the final smoke test comment is created.",2026-01-20T00:08:16+00:00,logic,medium,"Greptile correctly identified that the `update_containers_started_comment()` function would silently fail when no existing comment is found - the code does have an else branch that creates a comment, but Greptile's analysis that the comment doesn't exist yet at the time cloud-init finishes (line 678) is validated by the developer's reply confirming they fixed this issue. The developer explicitly acknowledged and implemented the suggested fix.",2026-01-20T23:34:12.362182+00:00
PostHog/posthog,44605,feat: EAV property materialization for HogQL,https://github.com/PostHog/posthog/pull/44605,,"The writable table is created on `NodeRole.INGESTION_SMALL` nodes only. However, the backfill activity in `posthog/temporal/eav_backfill/activities.py` line 98 writes to `writable_event_properties` from a worker node (not an ingestion node).

This will fail if:
1. Worker nodes don't have access to `writable_event_properties` table
2. The distributed table routing expects writes from specific nodes

Verify that worker nodes can access this table, or modify the backfill to write to a table accessible from worker nodes. The DMAT backfill has the same pattern, so check if this is already tested and working.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/clickhouse/migrations/0195_event_properties_eav_table.py
Line: 23:27

Comment:
The writable table is created on `NodeRole.INGESTION_SMALL` nodes only. However, the backfill activity in `posthog/temporal/eav_backfill/activities.py` line 98 writes to `writable_event_properties` from a worker node (not an ingestion node).

This will fail if:
1. Worker nodes don't have access to `writable_event_properties` table
2. The distributed table routing expects writes from specific nodes

Verify that worker nodes can access this table, or modify the backfill to write to a table accessible from worker nodes. The DMAT backfill has the same pattern, so check if this is already tested and working.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/44605#discussion_r2685409067,This looks legit,2026-01-13T08:37:21+00:00,runtime,high,"Greptile correctly identified a cross-cutting infrastructure issue: the writable table is only created on INGESTION_SMALL nodes, but the backfill activity runs on worker nodes which may not have access to this table. The developer confirmed this is a legitimate concern with 'This looks legit'. This is a non-obvious distributed systems issue that could cause the backfill to fail at runtime.",2026-01-20T23:34:12.362224+00:00
PostHog/posthog,44605,feat: EAV property materialization for HogQL,https://github.com/PostHog/posthog/pull/44605,,"**logic:** Using `timestamp AS _timestamp` means backfilled rows can overwrite newer real-time data. ReplacingMergeTree uses `_timestamp` as the version column (ver=""_timestamp"" in table definition). If Kafka ingests a row during backfill with an earlier timestamp, the backfilled row wins despite being stale.

Use `now() AS _timestamp` to ensure real-time ingestion always wins:

```suggestion
            now() AS _timestamp,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/eav_backfill/activities.py
Line: 116:116

Comment:
**logic:** Using `timestamp AS _timestamp` means backfilled rows can overwrite newer real-time data. ReplacingMergeTree uses `_timestamp` as the version column (ver=""_timestamp"" in table definition). If Kafka ingests a row during backfill with an earlier timestamp, the backfilled row wins despite being stale.

Use `now() AS _timestamp` to ensure real-time ingestion always wins:

```suggestion
            now() AS _timestamp,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/44605#discussion_r2688468884,,2026-01-13T23:45:08+00:00,data_integrity,high,"Greptile correctly identifies a subtle data integrity issue: when backfilling historical data into a ReplacingMergeTree table, using the event's original timestamp as the version column means backfilled rows could overwrite fresher real-time data if their timestamps happen to be later. Using now() ensures backfilled data is treated as 'old' and won't override concurrent real-time ingestion. This is a non-obvious issue specific to ClickHouse's ReplacingMergeTree semantics that could cause data loss.",2026-01-20T23:34:12.362247+00:00
PostHog/posthog,44605,feat: EAV property materialization for HogQL,https://github.com/PostHog/posthog/pull/44605,,"**logic:** waiting before backfill creates data inconsistency window

current flow:
1. slot created (state=BACKFILL)
2. wait 3 minutes for cache refresh (lines 60-67)
3. backfill historical data (lines 69-88)
4. set state=READY (lines 92-101)

during the 3-minute wait, plugin-server cache refreshes and starts writing NEW events to `event_properties`, but historical events haven't been backfilled yet. queries using this EAV property will see incomplete data (new events yes, old events no).

move backfill before cache wait:
1. backfill historical data immediately
2. wait for cache refresh
3. set state=READY

this ensures all historical data exists before plugin-server starts materializing new events

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/eav_backfill/workflows.py
Line: 60:67

Comment:
**logic:** waiting before backfill creates data inconsistency window

current flow:
1. slot created (state=BACKFILL)
2. wait 3 minutes for cache refresh (lines 60-67)
3. backfill historical data (lines 69-88)
4. set state=READY (lines 92-101)

during the 3-minute wait, plugin-server cache refreshes and starts writing NEW events to `event_properties`, but historical events haven't been backfilled yet. queries using this EAV property will see incomplete data (new events yes, old events no).

move backfill before cache wait:
1. backfill historical data immediately
2. wait for cache refresh
3. set state=READY

this ensures all historical data exists before plugin-server starts materializing new events

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/44605#discussion_r2688518516,,2026-01-14T00:16:50+00:00,data_integrity,medium,"Greptile correctly identifies a data inconsistency window in the workflow. The code waits 3 minutes before backfilling, during which the plugin-server cache refreshes and starts writing NEW events to event_properties while historical data hasn't been backfilled yet. This creates a window where queries could see incomplete/inconsistent data. The suggested fix to backfill first, then wait for cache refresh, is a valid solution to prevent this race condition.",2026-01-20T23:34:12.362272+00:00
PostHog/posthog,44605,feat: EAV property materialization for HogQL,https://github.com/PostHog/posthog/pull/44605,,"**logic:** Using `timestamp AS _timestamp` means backfilled rows can overwrite newer real-time data.

ReplacingMergeTree uses `_timestamp` as the version column (`ver=""_timestamp""` in table definition). If Kafka ingests a row during backfill with timestamp T1, but the backfilled row has timestamp T2 where T2 &gt; T1, the backfilled row wins even though it's stale.

The original issue suggested using `now() AS _timestamp` to ensure real-time ingestion always wins over backfill. However, using event timestamp is more correct IF backfill always runs before ingestion starts (which would be true if the workflow order is fixed per previous comment). Should backfilled rows use event timestamp or now() as the version? This depends on whether backfill always completes before ingestion starts.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/eav_backfill/activities.py
Line: 118:120

Comment:
**logic:** Using `timestamp AS _timestamp` means backfilled rows can overwrite newer real-time data.

ReplacingMergeTree uses `_timestamp` as the version column (`ver=""_timestamp""` in table definition). If Kafka ingests a row during backfill with timestamp T1, but the backfilled row has timestamp T2 where T2 &gt; T1, the backfilled row wins even though it's stale.

The original issue suggested using `now() AS _timestamp` to ensure real-time ingestion always wins over backfill. However, using event timestamp is more correct IF backfill always runs before ingestion starts (which would be true if the workflow order is fixed per previous comment). Should backfilled rows use event timestamp or now() as the version? This depends on whether backfill always completes before ingestion starts.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/44605#discussion_r2697429680,,2026-01-16T08:13:09+00:00,data_integrity,medium,"Greptile correctly identifies a subtle data integrity issue with ReplacingMergeTree versioning. Using `timestamp AS _timestamp` (event timestamp) as the version column means if backfill runs concurrently with real-time ingestion, older backfilled rows could overwrite newer real-time data when the event timestamp is later but the data is actually stale. This is a non-obvious race condition that depends on understanding ClickHouse's ReplacingMergeTree semantics.",2026-01-20T23:34:12.362379+00:00
posthog/posthog,45527,chore(ux): remove sidepanel part 2 (Posthog AI),https://github.com/PostHog/posthog/pull/45527,,"**logic:** Duplicate tab opening: The `onClick` handler calls `newInternalTab()`, but the `Link` component with `target=""_blank""` also calls `newInternalTab()` internally (see `Link.tsx:198-202`). This could open two tabs.

Either remove `newInternalTab()` from the onClick and rely on Link's default behavior, or add `event.preventDefault()`:

```suggestion
                            onClick={(event) => {
                                event.preventDefault()
                                closeSidePanel()
                                newInternalTab(urls.ai(conversationId ?? undefined))
                            }}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/max/Max.tsx
Line: 230:233

Comment:
**logic:** Duplicate tab opening: The `onClick` handler calls `newInternalTab()`, but the `Link` component with `target=""_blank""` also calls `newInternalTab()` internally (see `Link.tsx:198-202`). This could open two tabs.

Either remove `newInternalTab()` from the onClick and rely on Link's default behavior, or add `event.preventDefault()`:

```suggestion
                            onClick={(event) => {
                                event.preventDefault()
                                closeSidePanel()
                                newInternalTab(urls.ai(conversationId ?? undefined))
                            }}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45527#discussion_r2710443166,,2026-01-20T23:38:56+00:00,logic,medium,"Greptile correctly identifies that the Link component with target=""_blank"" combined with an onClick handler that also calls newInternalTab() would result in duplicate tab opening. The Link component's internal handling of target=""_blank"" (referenced at Link.tsx:198-202) plus the explicit newInternalTab() call creates a real bug that would confuse users by opening two tabs. This is non-obvious as it requires understanding the Link component's internal behavior.",2026-01-20T23:51:02.074241+00:00
nvidia/earth2studio,635,CorrDiff CMIP6 Refactor,https://github.com/NVIDIA/earth2studio/pull/635,,"**syntax:** typo in test environment name - should be `cmip6` not `cmip4`

```suggestion
[testenv:test-corrdiff-cmip6]
```",https://github.com/NVIDIA/earth2studio/pull/635#discussion_r2710448052,,2026-01-20T23:41:45+00:00,logic,medium,Greptile correctly identified a typo: the test environment is named 'test-corrdiff-cmip4' but the description says 'CorrDiff CMIP6->ERA5' and it runs 'test_corrdiff_cmip6.py'. This mismatch would cause confusion when running tests and could lead to the wrong test environment being invoked or missed entirely when developers try to run CMIP6 tests.,2026-01-20T23:51:02.074339+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** The nvw dependency points to an SSH-based internal GitLab repository which will fail for external users who don't have access to NVIDIA's internal infrastructure. Is this nvw dependency actually used by Atlas or can it be made optional for external users?,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352000,,2026-01-20T22:52:03+00:00,logic,high,"Greptile correctly identifies that the nvw dependency uses an SSH URL to an internal NVIDIA GitLab repository (gitlab-master.nvidia.com), which would cause installation failures for any external users trying to use the Atlas model. This is a real accessibility/usability bug that could easily be overlooked in an internal code review but would break the feature for the open-source community.",2026-01-20T23:51:02.074377+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** `find_num_heads` function is not defined anywhere in this file or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352046,,2026-01-20T22:52:04+00:00,runtime,high,"I verified that `find_num_heads` is called on line 832 in the `Attention.__init__` method when `num_heads is None`, but this function is never defined in the file nor imported from any module. This would cause a `NameError` at runtime when `Attention` is instantiated without specifying `num_heads`, making it a real bug that's easy to miss in a large file.",2026-01-20T23:51:02.074419+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** Functions `conservative_interpolate` and `bilinear_interpolate` are referenced but not defined or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352076,,2026-01-20T22:52:04+00:00,runtime,high,"Greptile correctly identified that `conservative_interpolate` and `bilinear_interpolate` are referenced in the `EquiangularInterpolator.set_method()` function (lines 571-577) but are never defined or imported anywhere in the file. This would cause a `NameError` at runtime when those interpolation methods are selected, making this a real bug that could be non-obvious during code review.",2026-01-20T23:51:02.074454+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** `RangeNormalizer` class is referenced but not defined or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352094,,2026-01-20T22:52:04+00:00,runtime,medium,"Greptile correctly identified that `RangeNormalizer` is used in the `BaseProcessor.__init__` method (lines 1358-1360) but is never defined or imported anywhere in the file. This would cause a `NameError` at runtime when `normalization.lower() == 'range'`, making the code fail when that code path is executed.",2026-01-20T23:51:02.074490+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** `PatchResample` class is referenced but not defined or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352108,,2026-01-20T22:52:05+00:00,runtime,high,"Greptile correctly identified that `PatchResample` is used in the `NattenCombineDiT` class (lines around 1147-1152) when `patch_processing == 'resample'`, but this class is never defined or imported anywhere in the file. This would cause a `NameError` at runtime when using resample processing mode.",2026-01-20T23:51:02.074522+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,**logic:** `DateEmbedder` class is referenced but not defined or imported,https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352138,,2026-01-20T22:52:06+00:00,runtime,medium,Greptile correctly identified that `DateEmbedder` is used on line 1234 (`self.date_embedder = DateEmbedder(hidden_channels)`) but is never defined or imported anywhere in the file. This would cause a `NameError` at runtime when `date_condition=True` is passed to `NattenCombineDiT`.,2026-01-20T23:51:02.074550+00:00
nvidia/earth2studio,634,Add Atlas model,https://github.com/NVIDIA/earth2studio/pull/634,,"**syntax:** OptionalDependencyFailure should be raised, not called as a function - use `raise OptionalDependencyFailure(""atlas"")`

```suggestion
except:
    raise OptionalDependencyFailure(""atlas"")
```",https://github.com/NVIDIA/earth2studio/pull/634#discussion_r2710352221,,2026-01-20T22:52:07+00:00,runtime,high,"Greptile correctly identified that OptionalDependencyFailure is instantiated but never raised at line 39-40. Looking at the code, when the optional dependencies (timm, natten, physicsnemo, torch_harmonics) fail to import, the exception is caught but the error is silently swallowed instead of being raised, which would cause confusing NameError exceptions later when trying to use undefined classes like InverseRealSHT or NeighborhoodAttention2D. This is a real bug that would be easy to miss.",2026-01-20T23:51:02.074580+00:00
assistant-ui/assistant-ui,3084,"Revert ""fix(react): accept URL-based images in sanitizeImageContent (#3069)""",https://github.com/assistant-ui/assistant-ui/pull/3084,,"**logic:** Reverting this breaks `CloudFileAttachmentAdapter` which uploads images to cloud storage and returns HTTPS URLs. On page reload, images stored via `CloudFileAttachmentAdapter.send()` (line 83 in CloudFileAttachmentAdapter.ts) will be filtered out since they don't match the base64 regex.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react/src/legacy-runtime/runtime-cores/external-store/ThreadMessageLike.tsx
Line: 88:93

Comment:
**logic:** Reverting this breaks `CloudFileAttachmentAdapter` which uploads images to cloud storage and returns HTTPS URLs. On page reload, images stored via `CloudFileAttachmentAdapter.send()` (line 83 in CloudFileAttachmentAdapter.ts) will be filtered out since they don't match the base64 regex.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3084#discussion_r2710488009,,2026-01-21T00:02:54+00:00,logic,high,"Greptile correctly identifies that reverting this change removes support for HTTPS URLs in image validation. The diff clearly shows the removal of the `isUrl` check (`/^https?:\/\//`) that previously allowed HTTP/HTTPS URLs, which would break any functionality relying on cloud-stored images. This is a non-obvious cross-component regression that a typical reviewer might miss when looking at what appears to be a simple revert PR.",2026-01-21T00:44:23.663757+00:00
onyx-dot-app/onyx,7608,feat(desktop): Properly Sign Mac App,https://github.com/onyx-dot-app/onyx/pull/7608,,"**logic:** Missing `APPLE_ID` and `APPLE_PASSWORD` environment variables needed for macOS notarization

```suggestion
      - uses: tauri-apps/tauri-action@73fb865345c54760d875b94642314f8c0c894afa # ratchet:tauri-apps/tauri-action@action-v0.6.1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          APPLE_ID: ${{ env.APPLE_ID }}
          APPLE_PASSWORD: ${{ env.APPLE_PASSWORD }}
          APPLE_SIGNING_IDENTITY: ${{ env.CERT_ID }}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/deployment.yml
Line: 342:344

Comment:
**logic:** Missing `APPLE_ID` and `APPLE_PASSWORD` environment variables needed for macOS notarization

```suggestion
      - uses: tauri-apps/tauri-action@73fb865345c54760d875b94642314f8c0c894afa # ratchet:tauri-apps/tauri-action@action-v0.6.1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          APPLE_ID: ${{ env.APPLE_ID }}
          APPLE_PASSWORD: ${{ env.APPLE_PASSWORD }}
          APPLE_SIGNING_IDENTITY: ${{ env.CERT_ID }}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7608#discussion_r2710634935,,2026-01-21T01:29:48+00:00,logic,high,"The PR adds steps to fetch Apple credentials from AWS Secrets Manager and import certificates, but the tauri-action step only has GITHUB_TOKEN in its env block. For macOS notarization to work, APPLE_ID, APPLE_PASSWORD, and APPLE_SIGNING_IDENTITY must be passed to the tauri-action. Without these, the notarization step will fail silently or error out, defeating the purpose of the entire PR which is to 'Properly Sign Mac App'.",2026-01-21T02:59:01.660416+00:00
nvidia/fuser,5854,ir container composition,https://github.com/NVIDIA/Fuser/pull/5854,,"**logic:** Move constructor should check if container is null before calling setParent(). If other was in a moved-from state with null container, this will crash.

```suggestion
IrInterface::IrInterface(IrInterface&& other) noexcept
    : container_(std::move(other.container_)) {
  if (container_) {
    container()->setParent(this);
  }
}
```",https://github.com/NVIDIA/Fuser/pull/5854#discussion_r2710690151,,2026-01-21T02:04:00+00:00,runtime,medium,"Greptile correctly identifies that the move constructor doesn't check for null before calling container()->setParent(this). If 'other' was previously moved-from (leaving its container_ null), this would cause a null pointer dereference crash. The move assignment operator at line 48-55 already has this null check, demonstrating inconsistency in the code. This is a non-obvious defensive programming issue that could cause crashes in certain usage patterns.",2026-01-21T02:59:01.660520+00:00
nvidia/nvflare,3993,[2.7] FedAvg Merge with FedAvgEarlyStopping + InTimeAggregation,https://github.com/NVIDIA/NVFlare/pull/3993,,"**logic:** Temporarily modifying model.params creates risk if an exception occurs between lines 124-126, leaving the model in an inconsistent state without restored parameters

```suggestion
        # Save FLModel metadata (metrics, params_type, etc.) separately
        params = model.params
        try:
            model.params = {}  # Temporarily remove params to save only metadata
            fobs.dumpf(model, f""{filepath}.metadata"")
        finally:
            model.params = params  # Restore params
```",https://github.com/NVIDIA/NVFlare/pull/3993#discussion_r2710662081,,2026-01-21T01:47:08+00:00,data_integrity,medium,"Greptile correctly identifies that if fobs.dumpf() raises an exception on line 125, the model.params will remain as an empty dict since the restoration on line 126 won't execute. This leaves the FLModel in an inconsistent state with lost parameters. The try/finally pattern is the correct fix for this pattern of temporarily modifying then restoring object state.",2026-01-21T02:59:01.660563+00:00
nvidia/nvflare,3994,[2.7] Cherry pick Add recipe for xgboost,https://github.com/NVIDIA/NVFlare/pull/3994,,"**syntax:** referencing `self.data_split_mode` but the attribute is `_data_split_mode` (with underscore)

```suggestion
        dtrain = xgb.DMatrix(X_train, label=y_train, data_split_mode=self._data_split_mode)
```",https://github.com/NVIDIA/NVFlare/pull/3994#discussion_r2710644523,,2026-01-21T01:36:19+00:00,runtime,medium,Greptile correctly identified that line 73 uses `self.data_split_mode` but the attribute is defined as `_data_split_mode` (with underscore) on line 55. This would cause an AttributeError at runtime when the test tries to create the DMatrix. This is a real bug that could be easily missed since it's a subtle naming inconsistency.,2026-01-21T02:59:01.660607+00:00
nvidia/nvflare,3994,[2.7] Cherry pick Add recipe for xgboost,https://github.com/NVIDIA/NVFlare/pull/3994,,"**syntax:** same issue - referencing `self.data_split_mode` but the attribute is `_data_split_mode` (with underscore)

```suggestion
        dval = xgb.DMatrix(X_val, label=y_val, data_split_mode=self._data_split_mode)
```",https://github.com/NVIDIA/NVFlare/pull/3994#discussion_r2710644527,,2026-01-21T01:36:20+00:00,runtime,medium,Greptile correctly identified that line 74 uses `self.data_split_mode` but the attribute is defined as `self._data_split_mode` on line 53. This would cause an AttributeError at runtime when the test tries to create the DMatrix. This is a real bug that would cause test failures.,2026-01-21T02:59:01.660641+00:00
generaltranslation/gt,960,chore: upload shared id for static content,https://github.com/generaltranslation/gt/pull/960,,"**logic:** If any hash is `undefined`, it will become string `""undefined""` in the joined result, potentially causing incorrect shared static IDs

```suggestion
    const hashes = staticUpdates
      .map((update) => update.metadata.hash)
      .filter((hash): hash is string => hash !== undefined)
      .sort()
      .join('-');
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/react/parse/createInlineUpdates.ts
Line: 185:188

Comment:
**logic:** If any hash is `undefined`, it will become string `""undefined""` in the joined result, potentially causing incorrect shared static IDs

```suggestion
    const hashes = staticUpdates
      .map((update) => update.metadata.hash)
      .filter((hash): hash is string => hash !== undefined)
      .sort()
      .join('-');
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/generaltranslation/gt/pull/960#discussion_r2710617817,,2026-01-21T01:19:18+00:00,logic,medium,"The code at line 185-186 maps hashes and joins them without filtering undefined values. In JavaScript, `undefined.toString()` in array join becomes the string 'undefined', which would create incorrect shared static IDs if any update lacks a hash. This is a non-obvious bug that could cause different static content groups to get the same or incorrect IDs, and the suggested fix with a type guard filter is correct and actionable.",2026-01-21T02:59:01.660659+00:00
onyx-dot-app/onyx,7619,feat: improve prompt shortcuts with uniqueness constraints and enhancements,https://github.com/onyx-dot-app/onyx/pull/7619,,"**logic:** Missing null safety check on `user?.preferences.shortcut_enabled`. If `user` is null, this will crash. Add optional chaining or a fallback check.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/chat/components/input/ChatInputBar.tsx
Line: 447:448

Comment:
**logic:** Missing null safety check on `user?.preferences.shortcut_enabled`. If `user` is null, this will crash. Add optional chaining or a fallback check.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7619#discussion_r2710885490,Fixed.,2026-01-21T04:06:04+00:00,runtime,medium,"Looking at line 448, the code uses `user?.preferences?.shortcut_enabled` but in the diff context around line 491-494, only `user?.preferences?.shortcut_enabled` is shown with proper optional chaining. However, at line 448 shown in the original position, accessing `preferences.shortcut_enabled` without optional chaining on `preferences` could cause a crash if `user` exists but `preferences` is null/undefined. The developer confirmed and fixed the issue.",2026-01-21T04:33:00.225411+00:00
onyx-dot-app/onyx,7612,Better connector permissioning,https://github.com/onyx-dot-app/onyx/pull/7612,,"**logic:** Raising an error here will break for ConnectorCredentialPairs created without a user (e.g., in seeding flows where `user=None` is passed to `add_credential_to_connector`). The `creator_id` field is nullable in the database schema (line 575 of `connector_credential_pair.py` shows `creator_id=user.id if user else None`).

```suggestion
                    creator_id = index_attempt.connector_credential_pair.creator_id
                    if creator_id is None:
                        logger.warning(
                            f""ConnectorCredentialPair {index_attempt.connector_credential_pair.id} ""
                            ""has no creator_id, using default user path for persistent document storage""
                        )
                        user_id_str: str = ""default""
                    else:
                        user_id_str: str = str(creator_id)
```

 Are there existing ConnectorCredentialPairs in production with null creator_id values from seeding flows?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/background/indexing/run_docfetching.py
Line: 647:651

Comment:
**logic:** Raising an error here will break for ConnectorCredentialPairs created without a user (e.g., in seeding flows where `user=None` is passed to `add_credential_to_connector`). The `creator_id` field is nullable in the database schema (line 575 of `connector_credential_pair.py` shows `creator_id=user.id if user else None`).

```suggestion
                    creator_id = index_attempt.connector_credential_pair.creator_id
                    if creator_id is None:
                        logger.warning(
                            f""ConnectorCredentialPair {index_attempt.connector_credential_pair.id} ""
                            ""has no creator_id, using default user path for persistent document storage""
                        )
                        user_id_str: str = ""default""
                    else:
                        user_id_str: str = str(creator_id)
```

 Are there existing ConnectorCredentialPairs in production with null creator_id values from seeding flows?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7612#discussion_r2710766939,,2026-01-21T02:51:05+00:00,runtime,high,"Greptile correctly identifies that the new code raises a ValueError when creator_id is None, but the database schema explicitly allows null values for creator_id (as shown by 'creator_id=user.id if user else None' logic). This would cause runtime crashes for existing ConnectorCredentialPairs created through seeding flows or without a user, breaking production functionality. This is a non-obvious catch since a typical reviewer might not trace back to verify the nullability of the field.",2026-01-21T04:33:00.225507+00:00
silimate/yosys,95,Register annotation,https://github.com/Silimate/yosys/pull/95,,"**logic:** Missing bounds check for find('[') - if no '[' is found, find() returns string::npos which could cause unexpected behavior with substr()

```suggestion
					if (blast) {
					std::string base_name = cell->name.str();
					size_t bracket_pos = base_name.find('[');
					if (bracket_pos != std::string::npos) {
						base_name = base_name.substr(0, bracket_pos);
					}
					slice_name = module->uniquify(base_name + stringf(
						""%c%d%c"", format[0], slice_lsb, format[1]));
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: passes/cmds/splitcells.cc
Line: 175:176

Comment:
**logic:** Missing bounds check for find('[') - if no '[' is found, find() returns string::npos which could cause unexpected behavior with substr()

```suggestion
					if (blast) {
					std::string base_name = cell->name.str();
					size_t bracket_pos = base_name.find('[');
					if (bracket_pos != std::string::npos) {
						base_name = base_name.substr(0, bracket_pos);
					}
					slice_name = module->uniquify(base_name + stringf(
						""%c%d%c"", format[0], slice_lsb, format[1]));
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Silimate/yosys/pull/95#discussion_r2710813579,,2026-01-21T03:19:56+00:00,runtime,medium,"Greptile correctly identified that when `blast` is true, if the cell name doesn't contain '[', `find('[')` returns `std::string::npos` (-1 as size_t), and `substr(0, npos)` would return the entire string (which is actually safe in C++), but the issue is real in that the code assumes there's always a '[' in the name when blasting, which may not be guaranteed. This is a subtle edge case that could cause unexpected naming behavior.",2026-01-21T04:33:00.225610+00:00
eventual-inc/daft,6071,feat: Support default alias for non-column refs for sql select,https://github.com/Eventual-Inc/Daft/pull/6071,,"**logic:** test expects ""Expressions in aggregations"" error message, but the actual error message from `mod.rs:302` is ""Expected aggregation (or a foldable literal) in select() only when aggregation expressions are present without groupby.""

```suggestion
    with pytest.raises(ValueError, match=""Expected aggregation""):
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: tests/dataframe/test_select_global_agg.py
Line: 58:58

Comment:
**logic:** test expects ""Expressions in aggregations"" error message, but the actual error message from `mod.rs:302` is ""Expected aggregation (or a foldable literal) in select() only when aggregation expressions are present without groupby.""

```suggestion
    with pytest.raises(ValueError, match=""Expected aggregation""):
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Eventual-Inc/Daft/pull/6071#discussion_r2710979143,,2026-01-21T05:03:49+00:00,logic,medium,Greptile correctly identified that the test's expected error message 'Expressions in aggregations' doesn't match the actual error message from the Rust code which is 'Expected aggregation (or a foldable literal) in select() only when aggregation expressions are present without groupby.' This would cause the test to fail since pytest.raises with match parameter uses regex matching. This is a real bug that would cause test failures and is non-obvious since it requires cross-referencing the Rust source code.,2026-01-21T06:21:26.602652+00:00
democratizedspace/dspace,2932,Stabilize process creation persistence and service worker e2e runs,https://github.com/democratizedspace/dspace/pull/2932,,"**logic:** calling `db.processes.add()` with an existing ID will cause a duplicate key error in IndexedDB because `createProcess` already called `db.processes.add()` with this ID

The issue: `createProcess` (line 450-460 in customcontent.js) generates an ID and calls `db.processes.add()`. If that succeeds but the follow-up `getProcess` fails (e.g., timing issue), this retry attempts to add the *same ID again*, which will throw a constraint violation.

```suggestion
                if (!persistedProcess) {
                    console.warn('Process was created but not immediately readable, retrying persistence check');
                    // Wait briefly and retry the read instead of re-adding
                    await new Promise(resolve => setTimeout(resolve, 100));
                    persistedProcess = await getProcess(createdId).catch(() => null);
                    
                    if (!persistedProcess) {
                        console.error('Process creation may have failed - process not found after retry');
                    }
                }
```",https://github.com/democratizedspace/dspace/pull/2932#discussion_r2711113991,,2026-01-21T06:13:13+00:00,data_integrity,medium,"Greptile correctly identifies that if createProcess() successfully adds a record with createdId but getProcess() fails for some reason (timing, transient error), the fallback code attempts db.processes.add() with the same ID, which will throw a duplicate key constraint error in IndexedDB. The suggested fix of retrying the read instead of re-adding is appropriate.",2026-01-21T06:21:26.602747+00:00
assistant-ui/assistant-ui,3088,docs: tap/docs (scaffolding),https://github.com/assistant-ui/assistant-ui/pull/3088,,"**logic:** incorrect path for tap-docs content - should be `tap-docs` not `docs`

```suggestion
  const path = `apps/docs/content/tap-docs/${page.path}`;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/docs/app/tap/docs/[[...slug]]/page.tsx
Line: 40:40

Comment:
**logic:** incorrect path for tap-docs content - should be `tap-docs` not `docs`

```suggestion
  const path = `apps/docs/content/tap-docs/${page.path}`;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3088#discussion_r2711621105,,2026-01-21T09:08:00+00:00,logic,medium,"The file is located at `apps/docs/app/tap/docs/[[...slug]]/page.tsx` which is the tap-docs section, and it uses `tapDocs` source. The path variable should point to `content/tap-docs/` to match the tap-docs content directory, but it incorrectly points to `content/docs/`. This would cause the GitHub edit URL to point to the wrong file location, breaking the 'edit on GitHub' functionality for tap-docs pages.",2026-01-21T10:18:52.552730+00:00
democratizedspace/dspace,2971,Hide zero-count inventory items by default,https://github.com/democratizedspace/dspace/pull/2971,,"**logic:** Missing `fullItemList` in reactive dependencies - if it updates after `onMount`, `allItems` won't rebuild

```suggestion
    $: {
        if (isClientSide && fullItemList) {
            allItems = buildAllItems($state.inventory);
            currentInventory = showAllItems
                ? allItems
                : filterNonZeroInventory($state.inventory);
        }
    }
```",https://github.com/democratizedspace/dspace/pull/2971#discussion_r2711632757,,2026-01-21T09:10:42+00:00,logic,medium,"Greptile correctly identified that `buildAllItems` depends on `fullItemList` (which is set asynchronously in `onMount`), but the reactive block only tracks `isClientSide`, `$state.inventory`, and `showAllItems`. If `fullItemList` updates after the reactive block first runs, `allItems` won't rebuild with the complete item list. Adding `fullItemList` to the reactive dependencies ensures the inventory rebuilds when the full item catalog becomes available.",2026-01-21T10:18:52.552815+00:00
alpic-ai/skybridge,351,feat: add repo option to create command,https://github.com/alpic-ai/skybridge/pull/351,,"**logic:** When downloading from a repo, the `package.json` name field isn't updated to match the user's chosen project name. The template path (lines 155-160) updates the name, but this path doesn't. Users scaffolding `my-app` from `ecom-carousel` will have `""name"": ""ecom-carousel""` in their `package.json` instead of `""name"": ""my-app""`.

```suggestion
  if (argRepo) {
    prompts.log.step(`Downloading ${argRepo}...`);
    try {
      await downloadTemplate(argRepo, { dir: root });
      // Update project name in package.json
      const name = path.basename(root);
      const pkgPath = path.join(root, ""package.json"");
      if (fs.existsSync(pkgPath)) {
        const pkg = fs.readFileSync(pkgPath, ""utf-8"");
        const pkgJson = JSON.parse(pkg);
        pkgJson.name = name;
        fs.writeFileSync(pkgPath, JSON.stringify(pkgJson, null, 2) + ""\n"");
      }
      prompts.log.success(`Project created in ${root}`);
    } catch (error) {
      prompts.log.error(""Failed to download repository"");
      console.error(error);
      process.exit(1);
    }
  }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/create-skybridge/src/index.ts
Line: 129:138

Comment:
**logic:** When downloading from a repo, the `package.json` name field isn't updated to match the user's chosen project name. The template path (lines 155-160) updates the name, but this path doesn't. Users scaffolding `my-app` from `ecom-carousel` will have `""name"": ""ecom-carousel""` in their `package.json` instead of `""name"": ""my-app""`.

```suggestion
  if (argRepo) {
    prompts.log.step(`Downloading ${argRepo}...`);
    try {
      await downloadTemplate(argRepo, { dir: root });
      // Update project name in package.json
      const name = path.basename(root);
      const pkgPath = path.join(root, ""package.json"");
      if (fs.existsSync(pkgPath)) {
        const pkg = fs.readFileSync(pkgPath, ""utf-8"");
        const pkgJson = JSON.parse(pkg);
        pkgJson.name = name;
        fs.writeFileSync(pkgPath, JSON.stringify(pkgJson, null, 2) + ""\n"");
      }
      prompts.log.success(`Project created in ${root}`);
    } catch (error) {
      prompts.log.error(""Failed to download repository"");
      console.error(error);
      process.exit(1);
    }
  }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/alpic-ai/skybridge/pull/351#discussion_r2711777368,,2026-01-21T09:48:34+00:00,logic,medium,"Greptile correctly identified that the `else` branch (lines 145-168) updates the package.json name field to match the target directory, but the `if (argRepo)` branch (lines 129-138) does not perform this update. This means users creating projects with `--repo` flag will have mismatched package names. This is a non-obvious inconsistency that would likely be missed in review since the logic exists in one code path but not the parallel one.",2026-01-21T10:18:52.552865+00:00
isaac-sim/isaaclab,4406,[Newton] Articulation & Rigid Object Full Tests,https://github.com/isaac-sim/IsaacLab/pull/4406,,"**logic:** `self._asset` is referenced before being assigned (line 39)

```suggestion
            raise ValueError(f""Unsupported asset type: {asset.__class__.__name__}"")
```",https://github.com/isaac-sim/IsaacLab/pull/4406#discussion_r2712002500,,2026-01-21T10:46:07+00:00,runtime,medium,"Greptile correctly identified that on line 37, the error message references `self._asset` but `self._asset` is only assigned later on line 39. At the point of the exception, `self._asset` doesn't exist yet, which would cause an AttributeError. The fix to use `asset` instead is correct.",2026-01-21T12:23:31.942372+00:00
mellanox/libxlio,554,[CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,"**syntax:** Missing closing parenthesis for `build()` function call. Line 44 should be part of the `build()` call, not a separate statement.

```suggestion
        propagate: false
      )
      env.LINUX_BUILD_URL = build.absoluteUrl
```",https://github.com/Mellanox/libxlio/pull/554#discussion_r2711979353,,2026-01-21T10:39:59+00:00,runtime,high,"Greptile correctly identified a missing closing parenthesis for the `build()` function call. Looking at the code, line 19 starts with `def build = build job:` and the parameters list ends at line 43 with `propagate: false` but there's no closing `)` before line 45's `env.LINUX_BUILD_URL = build.absoluteUrl`. This would cause a Groovy syntax error and the CI job would fail to run. This is non-obvious because the indentation is misleading and the file is relatively long.",2026-01-21T12:23:31.942436+00:00
twentyhq/twenty,17303,[Dashboards] fix line chart duplicate widget bleed by hashing series IDs,https://github.com/twentyhq/twenty/pull/17303,,"**logic:** `JSON.stringify` doesn't guarantee consistent key ordering across JavaScript engines, which could cause different hashes for the same configuration object

Consider sorting the keys before stringifying to ensure deterministic hashing:
```suggestion
    .update(JSON.stringify(configuration, Object.keys(configuration).sort()))
```

Or use a deterministic serialization approach similar to `create-deterministic-uuid.util.ts` in this codebase

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/modules/dashboard/chart-data/utils/build-line-chart-series-id-prefix.util.ts
Line: 12:12

Comment:
**logic:** `JSON.stringify` doesn't guarantee consistent key ordering across JavaScript engines, which could cause different hashes for the same configuration object

Consider sorting the keys before stringifying to ensure deterministic hashing:
```suggestion
    .update(JSON.stringify(configuration, Object.keys(configuration).sort()))
```

Or use a deterministic serialization approach similar to `create-deterministic-uuid.util.ts` in this codebase

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17303#discussion_r2712767563,,2026-01-21T14:15:23+00:00,logic,medium,"Greptile correctly identifies that JSON.stringify doesn't guarantee consistent key ordering, which could cause the same configuration object to produce different hashes in different environments or after object manipulation. This is a real bug that could cause the 'duplicate widget bleed' issue mentioned in the PR title to resurface, and the non-deterministic nature makes it particularly hard to debug. The suggestion to use deterministic serialization is actionable and references existing code patterns in the codebase.",2026-01-21T14:23:47.434065+00:00
assistant-ui/assistant-ui,3091,[WIP] feat: add tool ui runtime platform,https://github.com/assistant-ui/assistant-ui/pull/3091,,"**logic:** Calling `close()` twice will throw due to the ""closed"" → any transition being invalid. Make this method idempotent by checking current state first.

```suggestion
  public close(): void {
    if (this._state.lifecycle === ""closed"" || this._state.lifecycle === ""closing"") {
      return;
    }
    this.transition(""closing"");
    this.transition(""closed"");
  }
```

**Context Used:** Context from `dashboard` - Ensure that calling close() on controllers is safe from double-closing by checking if the controller... ([source](https://app.greptile.com/review/custom-context?memory=034c6f0b-0068-4fcb-a5b5-9be36d94d66e))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/tool-ui-runtime/src/core/instance.ts
Line: 73:76

Comment:
**logic:** Calling `close()` twice will throw due to the ""closed"" → any transition being invalid. Make this method idempotent by checking current state first.

```suggestion
  public close(): void {
    if (this._state.lifecycle === ""closed"" || this._state.lifecycle === ""closing"") {
      return;
    }
    this.transition(""closing"");
    this.transition(""closed"");
  }
```

**Context Used:** Context from `dashboard` - Ensure that calling close() on controllers is safe from double-closing by checking if the controller... ([source](https://app.greptile.com/review/custom-context?memory=034c6f0b-0068-4fcb-a5b5-9be36d94d66e))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3091#discussion_r2712693898,,2026-01-21T13:57:34+00:00,logic,medium,"The close() method performs two transitions (closing → closed), and the assertValidToolUILifecycleTransition function would throw if close() is called again when already in 'closed' state since 'closed' → 'closing' would be invalid. This is a real bug that could cause runtime crashes in scenarios where close() might be called multiple times (common in cleanup/unmount scenarios), and the fix to make it idempotent is correct and actionable.",2026-01-21T14:23:47.434172+00:00
posthog/posthog-js,2943,fix(rollup): fix plugin execution order,https://github.com/PostHog/posthog-js/pull/2943,,"**logic:** When `options.file` is used, `chunks` is never populated, so `chunk.code` isn't updated. This means the fix doesn't apply to single-file output configurations.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/rollup-plugin/src/index.ts
Line: 68:71

Comment:
**logic:** When `options.file` is used, `chunks` is never populated, so `chunk.code` isn't updated. This means the fix doesn't apply to single-file output configurations.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog-js/pull/2943#discussion_r2712672684,,2026-01-21T13:52:13+00:00,logic,medium,"Greptile correctly identified that the code only populates the `chunks` object inside the `if (options.dir)` block's for-loop, but when `options.file` is set (single-file output), the old code used to handle it with `args.push('--file', filePath)` which was removed. Now the for-loop still runs but without `options.dir`, basePaths only has the dirname of `options.file`, yet chunks still needs proper population for the final `chunk.code` update to work correctly for single-file configurations.",2026-01-21T14:23:47.434263+00:00
elephant-xyz/counties-trasform-scripts,649,Update hamilton transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649,,"**logic:** `sales.indexOf(rec)` inside a forEach has O(n²) complexity and returns incorrect indices if duplicate sales exist. Use the loop index instead.

```suggestion
          companyRelationshipsMap.get(key).push({ type: 'sale', saleIndex: idx + 1 });
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hamilton/scripts/data_extractor.js
Line: 1516:1516

Comment:
**logic:** `sales.indexOf(rec)` inside a forEach has O(n²) complexity and returns incorrect indices if duplicate sales exist. Use the loop index instead.

```suggestion
          companyRelationshipsMap.get(key).push({ type: 'sale', saleIndex: idx + 1 });
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649#discussion_r2712374297,,2026-01-21T12:32:20+00:00,logic,medium,"Greptile correctly identifies that using `sales.indexOf(rec)` inside the forEach callback will return incorrect indices if there are duplicate sale objects in the array, since indexOf returns the first matching index. Using the loop index parameter would be more reliable. While the forEach loop does provide an index parameter that should be used instead, this is a subtle bug that could cause data integrity issues when duplicate sales exist.",2026-01-21T14:23:47.434301+00:00
elephant-xyz/counties-trasform-scripts,649,Update hamilton transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649,,"**logic:** Company names are now stored in UPPERCASE instead of their original case. The old code preserved original casing by using `(o.name || """").trim()` in the Set. Store the original name as the value in the map, not just the key.

```suggestion
  // Only create company files for companies that have relationships
  companies = Array.from(companyRelationshipsMap.entries()).map(([_, data]) => ({
    ...appendSourceInfo(seed),
    name: data.originalName,
  }));
```

 Should company names preserve their original casing from the source data, or is uppercase normalization intentional?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hamilton/scripts/data_extractor.js
Line: 1547:1550

Comment:
**logic:** Company names are now stored in UPPERCASE instead of their original case. The old code preserved original casing by using `(o.name || """").trim()` in the Set. Store the original name as the value in the map, not just the key.

```suggestion
  // Only create company files for companies that have relationships
  companies = Array.from(companyRelationshipsMap.entries()).map(([_, data]) => ({
    ...appendSourceInfo(seed),
    name: data.originalName,
  }));
```

 Should company names preserve their original casing from the source data, or is uppercase normalization intentional?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649#discussion_r2712374358,,2026-01-21T12:32:21+00:00,data_integrity,medium,"Greptile correctly identified that the new code stores company names using uppercase keys (`(o.name || """").trim().toUpperCase()`) in `companyRelationshipsMap`, and then uses `Array.from(companyRelationshipsMap.keys())` to create company records. This means company names will be stored in UPPERCASE rather than preserving their original casing from source data, which is a data integrity issue that could affect downstream systems expecting properly-cased company names.",2026-01-21T14:23:47.434333+00:00
elephant-xyz/counties-trasform-scripts,649,Update hamilton transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649,,"**logic:** Store the original company name to preserve casing, similar to how person names are handled in `personRelationshipsMap`.

```suggestion
        if ((o.name || """").trim()) {
          const key = (o.name || """").trim().toUpperCase();
          if (!companyRelationshipsMap.has(key)) {
            companyRelationshipsMap.set(key, { originalName: (o.name || """").trim() });
          }
        }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hamilton/scripts/data_extractor.js
Line: 1511:1517

Comment:
**logic:** Store the original company name to preserve casing, similar to how person names are handled in `personRelationshipsMap`.

```suggestion
        if ((o.name || """").trim()) {
          const key = (o.name || """").trim().toUpperCase();
          if (!companyRelationshipsMap.has(key)) {
            companyRelationshipsMap.set(key, { originalName: (o.name || """").trim() });
          }
        }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649#discussion_r2712374405,,2026-01-21T12:32:22+00:00,data_integrity,medium,"Greptile correctly identifies that `personRelationshipsMap` stores original names (first_name, last_name, etc.) while `companyRelationshipsMap` only stores the uppercase key. Later at line 1548, `companies` is created from `companyRelationshipsMap.keys()` which returns uppercase names, losing the original casing. This is inconsistent with person handling and could cause data quality issues in output files.",2026-01-21T14:23:47.434405+00:00
elephant-xyz/counties-trasform-scripts,649,Update hamilton transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649,,"**logic:** Store original company name here too to preserve casing.

```suggestion
        if (!alreadyOnFirstSale) {
          if (!companyRelationshipsMap.has(key)) {
            companyRelationshipsMap.set(key, { originalName: (owner.name || """").trim() });
          }
        }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hamilton/scripts/data_extractor.js
Line: 1537:1541

Comment:
**logic:** Store original company name here too to preserve casing.

```suggestion
        if (!alreadyOnFirstSale) {
          if (!companyRelationshipsMap.has(key)) {
            companyRelationshipsMap.set(key, { originalName: (owner.name || """").trim() });
          }
        }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/649#discussion_r2712374442,,2026-01-21T12:32:23+00:00,data_integrity,medium,"Greptile correctly identifies that the first pass (lines 1507-1520) stores the original company name with `companyRelationshipsMap.set(key, [])`, but the second pass (lines 1537-1541) just stores an empty array without the original name. Later at line 1555, the code uses `Array.from(companyRelationshipsMap.keys())` which returns the uppercased keys, losing the original casing. This is inconsistent with how persons preserve their original names and would result in company names being stored in all uppercase.",2026-01-21T14:23:47.434472+00:00
elephant-xyz/counties-trasform-scripts,651,Update brevard transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/651,,"**syntax:** `layoutsData` is assigned without declaration - the `const` declaration at line 3628 is commented out. This will cause a `ReferenceError` in strict mode or create an unintended global variable.

```suggestion
  let layoutsData = readJSON(layoutsPath);
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: brevard/scripts/data_extractor.js
Line: 3646:3646

Comment:
**syntax:** `layoutsData` is assigned without declaration - the `const` declaration at line 3628 is commented out. This will cause a `ReferenceError` in strict mode or create an unintended global variable.

```suggestion
  let layoutsData = readJSON(layoutsPath);
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/651#discussion_r2712370322,,2026-01-21T12:31:13+00:00,runtime,medium,"Greptile correctly identified that `layoutsData` is assigned at line 3646 but its declaration at line 3628 is commented out (`// const layoutsData = null;`). This will cause a ReferenceError in strict mode or create an implicit global variable, which is a real bug that could be easily missed during review since the commented-out declaration appears nearby.",2026-01-21T14:23:47.434511+00:00
elephant-xyz/counties-trasform-scripts,654,Update hillsborough transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/654,,"**logic:** Regex pattern has double backslash escaping issue. `/(mid\\s*century|...)` creates a literal backslash-s sequence instead of matching whitespace. This will not match ""mid century"" with spaces in the text. Should use `/mid\s*century/` to properly match the optional space(s).

```suggestion
  if (/(mid\s*century|mid-century|mcmod)/.test(text)) return ""MidCenturyModern"";
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hillsborough/scripts/structureMapping.js
Line: 125:125

Comment:
**logic:** Regex pattern has double backslash escaping issue. `/(mid\\s*century|...)` creates a literal backslash-s sequence instead of matching whitespace. This will not match ""mid century"" with spaces in the text. Should use `/mid\s*century/` to properly match the optional space(s).

```suggestion
  if (/(mid\s*century|mid-century|mcmod)/.test(text)) return ""MidCenturyModern"";
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/654#discussion_r2712368245,,2026-01-21T12:30:38+00:00,logic,medium,"Greptile correctly identified a regex escaping bug at line 125. The pattern `/(mid\s*century|mid-century|mcmod)/` uses double backslash which creates a literal backslash character followed by 's' instead of the whitespace metacharacter \s. This means 'mid century modern' style homes would not be detected correctly, causing incorrect architectural style classification.",2026-01-21T14:23:47.434542+00:00
twentyhq/twenty,17306,[Apps] Get rid of .yarn binaries in apps,https://github.com/twentyhq/twenty/pull/17306,,"**logic:** `.yarn` directory not added to gitignore. The PR description says ""add .yarn to gitignore"" but only an empty line was added. This means developers will accidentally commit `.yarn` directories.

```suggestion
generated
.yarn
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-apps/.gitignore
Line: 1:2

Comment:
**logic:** `.yarn` directory not added to gitignore. The PR description says ""add .yarn to gitignore"" but only an empty line was added. This means developers will accidentally commit `.yarn` directories.

```suggestion
generated
.yarn
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17306#discussion_r2712987528,,2026-01-21T15:06:16+00:00,logic,medium,"Greptile correctly identified that the PR description says 'add .yarn to gitignore' but the diff only shows an empty line being added, not the actual '.yarn' entry. This is a real oversight where the intended change wasn't actually made, which would lead to .yarn directories being accidentally committed to the repository.",2026-01-21T16:25:30.192758+00:00
twentyhq/twenty,17305,Rework SDK watcher,https://github.com/twentyhq/twenty/pull/17305,,"**logic:** Missing regex pattern for aliased imports like `import { something as entryName }`. This was present in the old code but is missing here.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-sdk/src/cli/utilities/build/manifest/manifest-extract-from-file-server.ts
Line: 127:129

Comment:
**logic:** Missing regex pattern for aliased imports like `import { something as entryName }`. This was present in the old code but is missing here.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17305#discussion_r2712993061,,2026-01-21T15:07:32+00:00,logic,medium,"The regex patterns at lines 120-123 only handle direct named imports `{entryName}` and default imports, but miss aliased imports like `import { something as entryName }`. This is a real functional gap that would cause the code to fail silently (returning null) when users use aliased imports, which is a common JavaScript/TypeScript pattern. This is non-obvious as it requires understanding both the regex patterns and import syntax edge cases.",2026-01-21T16:25:30.192838+00:00
onyx-dot-app/onyx,7629,Fix email addresses to use mailto links instead of https redirects,https://github.com/onyx-dot-app/onyx/pull/7629,,"**logic:** The regex incorrectly matches URLs that already have `mailto:` prefix. For example, `isEmail(""mailto:test@example.com"")` returns `true` because `mailto:test` matches `[^\s@]+` before the `@`.

When GFM autolinks a bare email like `support@example.com`, it creates `href=""mailto:support@example.com""`. This fix would then produce `mailto:mailto:support@example.com`.

Consider checking for existing `mailto:` prefix first:

```suggestion
// Simple regex to detect valid email addresses
const isEmail = (value: string | undefined): boolean => {
  if (!value) return false;
  if (value.startsWith(""mailto:"")) return false;
  return /^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(value);
};
```

Alternatively, use the existing `ensureHrefProtocol` utility from `@/lib/utils` which already handles `mailto:` links correctly.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/chat/message/MemoizedTextComponents.tsx
Line: 17:21

Comment:
**logic:** The regex incorrectly matches URLs that already have `mailto:` prefix. For example, `isEmail(""mailto:test@example.com"")` returns `true` because `mailto:test` matches `[^\s@]+` before the `@`.

When GFM autolinks a bare email like `support@example.com`, it creates `href=""mailto:support@example.com""`. This fix would then produce `mailto:mailto:support@example.com`.

Consider checking for existing `mailto:` prefix first:

```suggestion
// Simple regex to detect valid email addresses
const isEmail = (value: string | undefined): boolean => {
  if (!value) return false;
  if (value.startsWith(""mailto:"")) return false;
  return /^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(value);
};
```

Alternatively, use the existing `ensureHrefProtocol` utility from `@/lib/utils` which already handles `mailto:` links correctly.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7629#discussion_r2713270509,,2026-01-21T16:07:42+00:00,logic,medium,"Greptile correctly identifies that the regex `/^[^\s@]+@[^\s@]+\.[^\s@]+$/` would match 'mailto:test@example.com' (where 'mailto:test' is the part before @), causing a double-prefixing bug resulting in 'mailto:mailto:support@example.com'. This is a real bug that would cause broken email links when GFM auto-links emails, and the edge case is non-obvious.",2026-01-21T16:25:30.192893+00:00
mellanox/libxlio,554,[CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,"**logic:** `readFile()` will fail if `check_chaos_results.sh` doesn't create `chaos_results.txt` (e.g., if the check step fails or is skipped). Pipeline will error trying to send email.

```suggestion
    def envFile = fileExists(""${env.WORKSPACE}/chaos_results.txt"") ? readFile(""${env.WORKSPACE}/chaos_results.txt"") : ""status=UNKNOWN\nexpected_failures=\nunexpected_passes=""
```",https://github.com/Mellanox/libxlio/pull/554#discussion_r2712797885,,2026-01-21T14:21:54+00:00,runtime,medium,"Greptile correctly identifies that the pipeline_stop block uses readFile() on chaos_results.txt without checking if the file exists. If the 'Check Results' step fails or the script doesn't create the file, the readFile() call will throw an exception, preventing the notification email from being sent - which is precisely when you'd want notification the most. This is a non-obvious edge case in CI pipeline error handling.",2026-01-21T16:25:30.192983+00:00
mellanox/libxlio,554,[CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,"**logic:** `.ci/scripts/run_step.sh` does not exist in this PR. All CI steps will fail immediately when trying to execute this missing script. You must add this file to the PR.

The README mentions ""Wrap CI steps with `run_step.sh`"" but the script was never included.",https://github.com/Mellanox/libxlio/pull/554#discussion_r2712939446,,2026-01-21T14:55:07+00:00,runtime,critical,"The diff shows all CI steps being modified to call `.ci/scripts/run_step.sh`, but this script is not included anywhere in the diff. If this file doesn't exist in the repository and wasn't added in this PR, every single CI step would fail immediately with a 'file not found' error, completely breaking the CI pipeline.",2026-01-21T16:25:30.193023+00:00
delta-exchange/chatwoot,136,Background Jobs Optimisation and cleaup,https://github.com/delta-exchange/chatwoot/pull/136,,"**logic:** Missing `primary_actor_type` filter - this will delete notifications from other notification types (like SLA events) that happen to share the same `primary_actor_id`. Should filter by both `primary_actor_type` and `primary_actor_id`.

```suggestion
    Notification.where(
      user_id: notification.user_id,
      primary_actor_type: notification.primary_actor_type,
      primary_actor_id: notification.primary_actor_id
    ).where.not(id: notification.id).delete_all
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: app/jobs/notification/delivery_job.rb
Line: 30:34

Comment:
**logic:** Missing `primary_actor_type` filter - this will delete notifications from other notification types (like SLA events) that happen to share the same `primary_actor_id`. Should filter by both `primary_actor_type` and `primary_actor_id`.

```suggestion
    Notification.where(
      user_id: notification.user_id,
      primary_actor_type: notification.primary_actor_type,
      primary_actor_id: notification.primary_actor_id
    ).where.not(id: notification.id).delete_all
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/delta-exchange/chatwoot/pull/136#discussion_r2712919668,,2026-01-21T14:50:22+00:00,data_integrity,high,"Greptile correctly identifies that the `remove_duplicate_notifications` method only filters by `user_id` and `primary_actor_id`, missing the `primary_actor_type` filter. This means if a user has notifications for different entity types (e.g., a Conversation with id=5 and an SLA event with id=5), this code would incorrectly delete the SLA notification when processing the Conversation notification, causing data loss. This is a non-obvious polymorphic association bug that could silently delete legitimate notifications.",2026-01-21T16:25:30.193057+00:00
alpaca-network/railway-grafana-stack,85,"Improve alert provisioning, FastAPI schemas, and telemetry observability",https://github.com/Alpaca-Network/railway-grafana-stack/pull/85,,"**logic:** `host.docker.internal` doesn't work on Linux. In local Docker Compose, services should connect via service name `fastapi_app` (as defined in the original logic). This breaks local Linux setups.

```suggestion
    TARGET=""fastapi_app:8000""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: prometheus/entrypoint.sh
Line: 32:32

Comment:
**logic:** `host.docker.internal` doesn't work on Linux. In local Docker Compose, services should connect via service name `fastapi_app` (as defined in the original logic). This breaks local Linux setups.

```suggestion
    TARGET=""fastapi_app:8000""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/85#discussion_r2712952298,,2026-01-21T14:58:07+00:00,logic,medium,"Greptile correctly identifies that `host.docker.internal` doesn't work on native Linux Docker (only Mac/Windows). The original code used `fastapi_app:8000` for local Docker Compose, which works across all platforms via Docker's internal DNS. This change would break local development on Linux, a common deployment platform, and the fix is straightforward - revert to the original service name approach.",2026-01-21T16:25:30.193145+00:00
alpaca-network/railway-grafana-stack,84,"Created new abnormal alarm policy that will send alerts within anomalies such as latency, traffic, etc. Refactored current gateway page:",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84,,"**logic:** Traffic spike warning route is missing severity matcher unlike the critical route - this will catch all traffic_spike alerts regardless of severity. Should this warning route only match alerts without a severity label, or should it include a severity matcher for consistency?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: grafana/provisioning/alerting/notification_policies.yml
Line: 39:51

Comment:
**logic:** Traffic spike warning route is missing severity matcher unlike the critical route - this will catch all traffic_spike alerts regardless of severity. Should this warning route only match alerts without a severity label, or should it include a severity matcher for consistency?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84#discussion_r2712891889,,2026-01-21T14:44:02+00:00,logic,medium,"Greptile correctly identified that the traffic spike warning route (lines 39-51) lacks a severity matcher while the critical route (lines 21-37) explicitly matches `severity: critical`. This means all traffic_spike alerts, including critical ones, will also match the warning route and potentially be routed to the wrong receiver. The same pattern issue exists for error_rate_spike, latency_anomaly, and availability_drop routes where warning routes don't filter by severity.",2026-01-21T16:25:30.193180+00:00
alpaca-network/railway-grafana-stack,84,"Created new abnormal alarm policy that will send alerts within anomalies such as latency, traffic, etc. Refactored current gateway page:",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84,,"**logic:** Division by $B (baseline) could cause issues if the baseline value is zero or very close to zero, potentially causing alert evaluation errors.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: grafana/provisioning/alerting/rules/latency_anomalies.yml
Line: 62:62

Comment:
**logic:** Division by $B (baseline) could cause issues if the baseline value is zero or very close to zero, potentially causing alert evaluation errors.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84#discussion_r2712891963,,2026-01-21T14:44:02+00:00,runtime,medium,"Greptile correctly identifies that the expression '($A / $B > 2) && ($A > 1.0)' at line 62 (and similar at line 110) performs division by $B (the baseline latency), which could be zero or near-zero during startup, data gaps, or low-traffic periods, causing alert evaluation errors or undefined behavior. This is a non-obvious edge case that could cause false alerts or alert system failures.",2026-01-21T16:25:30.193214+00:00
alpaca-network/railway-grafana-stack,84,"Created new abnormal alarm policy that will send alerts within anomalies such as latency, traffic, etc. Refactored current gateway page:",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84,,"**logic:** Division by zero could occur if baseline (B.Value) is 0. Consider adding a safety check like `(div (max $values.A.Value 0.001) (max $values.B.Value 0.001))`.

```suggestion
            This is {{ printf ""%.1f"" (div (max $values.A.Value 0.001) (max $values.B.Value 0.001)) }}x normal error rate.
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: grafana/provisioning/alerting/rules/error_rate_anomalies.yml
Line: 65:65

Comment:
**logic:** Division by zero could occur if baseline (B.Value) is 0. Consider adding a safety check like `(div (max $values.A.Value 0.001) (max $values.B.Value 0.001))`.

```suggestion
            This is {{ printf ""%.1f"" (div (max $values.A.Value 0.001) (max $values.B.Value 0.001)) }}x normal error rate.
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84#discussion_r2712892032,,2026-01-21T14:44:03+00:00,runtime,medium,"Greptile correctly identified a division by zero risk at line 65 where `(div $values.A.Value $values.B.Value)` is used in the annotation template. If the 24h average baseline ($values.B.Value) is 0 (which could happen for new deployments or services with historically zero errors), this division would fail or produce undefined behavior when the alert fires.",2026-01-21T16:25:30.193240+00:00
alpaca-network/railway-grafana-stack,84,"Created new abnormal alarm policy that will send alerts within anomalies such as latency, traffic, etc. Refactored current gateway page:",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84,,"**logic:** Division by zero could occur if $B is 0. Consider adding a safety check like `($B > 0) && ($A / $B > 3) && ($A <= 10)`.

```suggestion
              expression: '($B > 0) && ($A / $B > 3) && ($A <= 10)'
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: grafana/provisioning/alerting/rules/error_rate_anomalies.yml
Line: 87:87

Comment:
**logic:** Division by zero could occur if $B is 0. Consider adding a safety check like `($B > 0) && ($A / $B > 3) && ($A <= 10)`.

```suggestion
              expression: '($B > 0) && ($A / $B > 3) && ($A <= 10)'
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/84#discussion_r2712892109,,2026-01-21T14:44:03+00:00,runtime,medium,"The expression '$A / $B > 3' on line 87 will cause a division by zero error if the 24-hour average error rate ($B) is 0, which is a realistic scenario when there have been no errors in the baseline period. This is a non-obvious bug that could cause the alert rule to fail or behave unexpectedly in production.",2026-01-21T16:25:30.193268+00:00
useautumn/autumn,604,feat: 🎸 api customer v5 (stack layer 2: API V2.1),https://github.com/useautumn/autumn/pull/604,,"**logic:** Potential logic issue: using `||` operator means if `fromBreakdown` is 0 (valid value), it will fallback to `legacyData?.prepaid_quantity`. Use nullish coalescing (`??`) instead to only fallback when `fromBreakdown` is null/undefined

```suggestion
		prepaidQuantity = fromBreakdown ?? legacyData?.prepaid_quantity ?? 0;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: shared/api/customers/cusFeatures/changes/V1.2_CusFeatureChange.ts
Line: 128:128

Comment:
**logic:** Potential logic issue: using `||` operator means if `fromBreakdown` is 0 (valid value), it will fallback to `legacyData?.prepaid_quantity`. Use nullish coalescing (`??`) instead to only fallback when `fromBreakdown` is null/undefined

```suggestion
		prepaidQuantity = fromBreakdown ?? legacyData?.prepaid_quantity ?? 0;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/useautumn/autumn/pull/604#discussion_r2713194929,,2026-01-21T15:50:34+00:00,logic,medium,Greptile correctly identifies that using `||` instead of `??` will cause `fromBreakdown` values of 0 (a perfectly valid prepaid quantity) to incorrectly fall back to `legacyData?.prepaid_quantity`. This is a subtle but real bug that could cause incorrect balance calculations when a user has exactly 0 prepaid quantity.,2026-01-21T16:25:30.193294+00:00
alpic-ai/skybridge,357,Introduce two hooks for dev command : typescript check and nodemon,https://github.com/alpic-ai/skybridge/pull/357,,"**syntax:** `qsdqs` is not exported by `generateHelpers()` and appears to be test/debug code

```suggestion
export const { useToolInfo, useCallTool } = generateHelpers<AppType>();
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/create-skybridge/template/web/src/helpers.ts
Line: 4:4

Comment:
**syntax:** `qsdqs` is not exported by `generateHelpers()` and appears to be test/debug code

```suggestion
export const { useToolInfo, useCallTool } = generateHelpers<AppType>();
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/alpic-ai/skybridge/pull/357#discussion_r2713234092,,2026-01-21T15:59:13+00:00,runtime,medium,"Greptile correctly identified that 'qsdqs' appears to be debug/test code that was accidentally left in - it's a nonsensical variable name being destructured from generateHelpers() which would either cause a runtime error or result in an undefined value. This is in a template file that gets generated for new projects, making it particularly important to catch.",2026-01-21T16:25:30.193358+00:00
alpic-ai/skybridge,357,Introduce two hooks for dev command : typescript check and nodemon,https://github.com/alpic-ai/skybridge/pull/357,,"**logic:** New `Writable` stream created on every `readable` event without cleanup, causing potential memory leak. Set up stream once in useEffect instead. Is there a specific reason the stream needs to be recreated on each readable event rather than set up once?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/core/src/cli/use-nodemon.ts
Line: 30:45

Comment:
**logic:** New `Writable` stream created on every `readable` event without cleanup, causing potential memory leak. Set up stream once in useEffect instead. Is there a specific reason the stream needs to be recreated on each readable event rather than set up once?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/alpic-ai/skybridge/pull/357#discussion_r2713234138,,2026-01-21T15:59:14+00:00,resource_leak,medium,"Greptile correctly identifies that a new Writable stream is created every time the 'readable' event fires (line 30-43), but these streams are never cleaned up or destroyed. The 'readable' event can fire multiple times during nodemon's lifecycle, and each time a new stream is piped to stdout without removing the previous one, causing accumulated memory usage. This is a non-obvious memory leak that a typical reviewer might miss.",2026-01-21T16:25:30.193385+00:00
credal-ai/fern-docs,300,repair some broken links,https://github.com/Credal-ai/fern-docs/pull/300,,"**logic:** The link text says ""uploadDocumentContents API endpoint"" but the URL points to the `metadata` endpoint. Should be `/api-reference/v-0/document-catalog/upload-document-contents` instead.

```suggestion
   - The Name must match the name of a metadata field you have on the documents in your collection. That metadata can be set when uploading the data via the [uploadDocumentContents API endpoint](/api-reference/v-0/document-catalog/upload-document-contents) or patched in after the fact using the [metadata endpoint](/api-reference/v-0/document-catalog/metadata).
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: fern/docs/pages/platform/agents/in-depth-overview/agent-configure-sections/tools.mdx
Line: 31:31

Comment:
**logic:** The link text says ""uploadDocumentContents API endpoint"" but the URL points to the `metadata` endpoint. Should be `/api-reference/v-0/document-catalog/upload-document-contents` instead.

```suggestion
   - The Name must match the name of a metadata field you have on the documents in your collection. That metadata can be set when uploading the data via the [uploadDocumentContents API endpoint](/api-reference/v-0/document-catalog/upload-document-contents) or patched in after the fact using the [metadata endpoint](/api-reference/v-0/document-catalog/metadata).
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Credal-ai/fern-docs/pull/300#discussion_r2713269435,,2026-01-21T16:07:27+00:00,logic,low,"Greptile correctly identified that the link text says 'uploadDocumentContents API endpoint' but points to the metadata endpoint. The PR was already fixing broken links by removing 'api-reference/' from paths, but this inconsistency between link text and destination remained. While documentation errors are typically less critical, this is a valid catch that could confuse users trying to find the correct API endpoint.",2026-01-21T16:25:30.193411+00:00
posthog/posthog,45611,feat(data-table): Implement table views feature in persons list,https://github.com/PostHog/posthog/pull/45611,,"**logic:** `updatedView` could be undefined if view not found in the list, causing `applyView` to receive undefined

```suggestion
        updateViewSuccess: ({ payload }) => {
            const updatedView = values.views.find((view) => view.id === payload?.id)
            if (updatedView) {
                actions.applyView(updatedView)
                lemonToast.success(`View ""${updatedView.name}"" updated`)
            }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/queries/nodes/DataTable/TableView/tableViewLogic.ts
Line: 199:202

Comment:
**logic:** `updatedView` could be undefined if view not found in the list, causing `applyView` to receive undefined

```suggestion
        updateViewSuccess: ({ payload }) => {
            const updatedView = values.views.find((view) => view.id === payload?.id)
            if (updatedView) {
                actions.applyView(updatedView)
                lemonToast.success(`View ""${updatedView.name}"" updated`)
            }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45611#discussion_r2713707360,,2026-01-21T17:55:21+00:00,runtime,medium,"The code uses `values.views.find()` which can return undefined, then immediately passes the result to `applyView(updatedView)` without checking if it exists. This could cause runtime errors or unexpected behavior when applyView tries to access properties of undefined. The fix is straightforward - add an existence check before calling applyView.",2026-01-21T18:30:48.617834+00:00
posthog/posthog,45611,feat(data-table): Implement table views feature in persons list,https://github.com/PostHog/posthog/pull/45611,,"**logic:** Destructively replaces `source` object, potentially losing other properties like `limit`, `offset`, `order`, etc. Preserve existing source properties:

```suggestion
        applyView: ({ view }) => {
            // Apply view while preserving other source properties
            const newQuery: DataTableNode = {
                ...props.query,
                source: {
                    ...props.query.source,
                    properties: view.filters,
                    select: view.columns,
                },
            }
            props.setQuery(newQuery)
        },
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/queries/nodes/DataTable/TableView/tableViewLogic.ts
Line: 177:188

Comment:
**logic:** Destructively replaces `source` object, potentially losing other properties like `limit`, `offset`, `order`, etc. Preserve existing source properties:

```suggestion
        applyView: ({ view }) => {
            // Apply view while preserving other source properties
            const newQuery: DataTableNode = {
                ...props.query,
                source: {
                    ...props.query.source,
                    properties: view.filters,
                    select: view.columns,
                },
            }
            props.setQuery(newQuery)
        },
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45611#discussion_r2713707400,,2026-01-21T17:55:21+00:00,data_integrity,medium,"Greptile correctly identified that the code explicitly constructs a new source object with only `kind`, `properties`, and `select` fields, which would discard any other existing properties like `limit`, `offset`, `orderBy`, etc. that may exist on `props.query.source`. The suggested fix using spread operator `...props.query.source` would preserve these properties while still applying the view's filters and columns.",2026-01-21T18:30:48.617916+00:00
posthog/posthog,45611,feat(data-table): Implement table views feature in persons list,https://github.com/PostHog/posthog/pull/45611,,"**logic:** Views are never loaded - `lazyLoaders` requires explicit action calls to load data, unlike regular `loaders`. Add an `afterMount` hook at the end of the logic definition to call `loadViews()` on component initialization.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/queries/nodes/DataTable/TableView/tableViewLogic.ts
Line: 39:89

Comment:
**logic:** Views are never loaded - `lazyLoaders` requires explicit action calls to load data, unlike regular `loaders`. Add an `afterMount` hook at the end of the logic definition to call `loadViews()` on component initialization.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45611#discussion_r2713707494,,2026-01-21T17:55:22+00:00,logic,medium,"Greptile correctly identifies that `lazyLoaders` (unlike regular `loaders`) does not automatically load data on mount - it requires explicit action calls. The code defines `loadViews` as a lazy loader but never calls it on initialization, meaning the views list will remain empty until something explicitly triggers the load. This is a non-obvious framework-specific behavior that would cause the feature to appear broken on first render.",2026-01-21T18:30:48.617993+00:00
posthog/posthog,45609,feat(data-table): Extend column configuration model to also save filters,https://github.com/PostHog/posthog/pull/45609,,"**logic:** `log_activity` is called with incorrect parameters. The function requires `organization_id`, `scope` (not `item_type`), `was_impersonated`, and `detail` as a `Detail` dataclass. See `products/data_warehouse/backend/api/saved_query.py:245-263` for correct usage pattern.

```suggestion
        try:
            from posthog.models.activity_logging.activity_log import Detail, log_activity
            from loginas.utils import is_impersonated_session

            log_activity(
                organization_id=self.organization.id,
                team_id=self.team_id,
                user=request.user,
                was_impersonated=is_impersonated_session(request),
                item_id=str(instance.id),
                scope=""ColumnConfiguration"",
                activity=""updated"",
                detail=Detail(name=instance.name, changes=None),
            )
        except Exception:
            pass
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/column_configuration.py
Line: 96:111

Comment:
**logic:** `log_activity` is called with incorrect parameters. The function requires `organization_id`, `scope` (not `item_type`), `was_impersonated`, and `detail` as a `Detail` dataclass. See `products/data_warehouse/backend/api/saved_query.py:245-263` for correct usage pattern.

```suggestion
        try:
            from posthog.models.activity_logging.activity_log import Detail, log_activity
            from loginas.utils import is_impersonated_session

            log_activity(
                organization_id=self.organization.id,
                team_id=self.team_id,
                user=request.user,
                was_impersonated=is_impersonated_session(request),
                item_id=str(instance.id),
                scope=""ColumnConfiguration"",
                activity=""updated"",
                detail=Detail(name=instance.name, changes=None),
            )
        except Exception:
            pass
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45609#discussion_r2713690136,,2026-01-21T17:50:30+00:00,logic,medium,"Greptile correctly identifies that the log_activity function is being called with incorrect parameters - the code uses 'item_type' instead of 'scope', passes 'detail' as a plain dict instead of a Detail dataclass, and is missing required parameters like 'organization_id' and 'was_impersonated'. The code even has a TODO comment acknowledging 'this is entirely wrong', validating Greptile's catch. This would cause the logging to silently fail (caught by the bare except).",2026-01-21T18:30:48.618049+00:00
posthog/posthog,45609,feat(data-table): Extend column configuration model to also save filters,https://github.com/PostHog/posthog/pull/45609,,"**logic:** Legacy configurations have `created_by=None` and `visibility='private'` (the default). This permission check will block updates to all legacy configs since `None == request.user` is False and `'private' == 'shared'` is False.

```suggestion
        if instance.created_by is not None and instance.created_by != request.user and instance.visibility != ""shared"":
            return Response({""error"": ""You don't have permission to edit this view""}, status=403)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/column_configuration.py
Line: 92:93

Comment:
**logic:** Legacy configurations have `created_by=None` and `visibility='private'` (the default). This permission check will block updates to all legacy configs since `None == request.user` is False and `'private' == 'shared'` is False.

```suggestion
        if instance.created_by is not None and instance.created_by != request.user and instance.visibility != ""shared"":
            return Response({""error"": ""You don't have permission to edit this view""}, status=403)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45609#discussion_r2713690203,,2026-01-21T17:50:31+00:00,logic,medium,"Greptile correctly identifies that legacy configurations have `created_by=None` (since `perform_create` only sets this in the new code). The permission check `instance.created_by == request.user` will be False when `created_by` is None, and since legacy configs default to `visibility='private'`, the second condition also fails, blocking all updates to legacy configurations. This is a non-obvious edge case that could break existing functionality.",2026-01-21T18:30:48.618185+00:00
posthog/posthog,45609,feat(data-table): Extend column configuration model to also save filters,https://github.com/PostHog/posthog/pull/45609,,"**logic:** The `update` method has permission checks, but there's no corresponding `destroy` override. Any team member can delete private views created by other users. Consider adding a `destroy` method with similar permission validation.

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/column_configuration.py
Line: 89:113

Comment:
**logic:** The `update` method has permission checks, but there's no corresponding `destroy` override. Any team member can delete private views created by other users. Consider adding a `destroy` method with similar permission validation.

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45609#discussion_r2713690242,,2026-01-21T17:50:32+00:00,security,high,"Greptile correctly identifies a security vulnerability: the `update` method at lines 89-113 checks if the user has permission (is the creator or visibility is 'shared'), but there's no corresponding `destroy` method override. This means the default ModelViewSet destroy behavior would allow any team member to delete any configuration, including private views owned by other users, bypassing the ownership validation pattern established in `update`.",2026-01-21T18:30:48.618254+00:00
raycast/extensions,24727,Update typst-symbols extension,https://github.com/raycast/extensions/pull/24727,,"**logic:** `item.title === query` compares original case but `cmdLower === q` uses lowercase - inconsistent comparison will miss matches where query case differs from title

```suggestion
    if (cmdLower === q || item.title.toLowerCase() === q) {
```",https://github.com/raycast/extensions/pull/24727#discussion_r2713305941,,2026-01-21T16:15:59+00:00,logic,low,"Greptile correctly identified an inconsistency: the code compares `cmdLower === q` (both lowercase) but `item.title === query` (original case), meaning title matches would fail if the user's query had different casing. This is a subtle logic bug that could cause missed search results.",2026-01-21T18:30:48.618320+00:00
onyx-dot-app/onyx,7630,feat: file uplod,https://github.com/onyx-dot-app/onyx/pull/7630,,"**logic:** `validate_mime_type` always returns `True, None` regardless of input - MIME type validation is not enforced

```suggestion
def validate_mime_type(content_type: str | None) -> tuple[bool, str | None]:
    """"""Validate MIME type against allowlist.

    Args:
        content_type: The Content-Type header value

    Returns:
        Tuple of (is_valid, error_message)
    """"""
    if not content_type:
        # Allow missing content type - we'll validate by extension
        return True, None

    # Extract base MIME type (ignore charset etc.)
    mime_type = content_type.split("";"")[0].strip().lower()

    # Validate against allowlist (permissive for generic types)
    if mime_type not in ALLOWED_MIME_TYPES and not mime_type.startswith(""text/""):
        return False, f""MIME type '{mime_type}' is not allowed""

    return True, None
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/utils.py
Line: 144:165

Comment:
**logic:** `validate_mime_type` always returns `True, None` regardless of input - MIME type validation is not enforced

```suggestion
def validate_mime_type(content_type: str | None) -> tuple[bool, str | None]:
    """"""Validate MIME type against allowlist.

    Args:
        content_type: The Content-Type header value

    Returns:
        Tuple of (is_valid, error_message)
    """"""
    if not content_type:
        # Allow missing content type - we'll validate by extension
        return True, None

    # Extract base MIME type (ignore charset etc.)
    mime_type = content_type.split("";"")[0].strip().lower()

    # Validate against allowlist (permissive for generic types)
    if mime_type not in ALLOWED_MIME_TYPES and not mime_type.startswith(""text/""):
        return False, f""MIME type '{mime_type}' is not allowed""

    return True, None
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7630#discussion_r2713794856,,2026-01-21T18:18:29+00:00,security,high,"Greptile correctly identified that the validate_mime_type function always returns True, None regardless of whether the MIME type is in the allowlist or not. Looking at lines 144-165, the function checks if mime_type is NOT in ALLOWED_MIME_TYPES, but then returns True, None anyway - the validation logic is completely non-functional. This is a security bug in file upload validation that could allow malicious file types to be uploaded.",2026-01-21T18:30:48.618341+00:00
mellanox/libxlio,526,[CI] issue: HPCINFRA-4039 Multi pod tests,https://github.com/Mellanox/libxlio/pull/526,,"**logic:** client expects 101 zero-copy completions but test sends 100 messages (`-n 100` in `run_pod_pair_client_tests.sh:17`). should be 100 not 101

```suggestion
        if [[ ""$(grep -c ""Zero-copy send completed"" ""${ULTRA_API_LOGS_DIR}/ultra_api_ping_pong_client.log"")"" -ne 100 ]]; then
```",https://github.com/Mellanox/libxlio/pull/526#discussion_r2713742467,,2026-01-21T18:04:56+00:00,logic,medium,"Greptile correctly identified an off-by-one error where the client check expects 101 zero-copy completions while the test configuration sends 100 messages (-n 100). This would cause the test to always fail incorrectly. The server side correctly checks for 100, making this inconsistency a real bug that's easy to miss during review.",2026-01-21T18:30:48.618412+00:00
nvidia/dali,6165,Default `num_threads` and `stream` for dynamic API,https://github.com/NVIDIA/DALI/pull/6165,,"**logic:** missing assignment of order after ctypes_void_ptr call

```suggestion
    } else if (py::hasattr(cuda_stream, ""value"")) {
      cudaStream_t stream = static_cast<cudaStream_t>(ctypes_void_ptr(cuda_stream));
      order = AccessOrder(stream);
```",https://github.com/NVIDIA/DALI/pull/6165#discussion_r2713595686,,2026-01-21T17:24:27+00:00,logic,high,"Looking at the diff, line 607 shows `order = AccessOrder(stream);` being added after `ctypes_void_ptr(cuda_stream)`. In the original code (before the diff's changes), the `py::hasattr(cuda_stream, ""value"")` branch computed a stream but never assigned it to `order`, meaning the function would return an uninitialized/default `order`. This is a real bug where the stream parameter would be silently ignored for ctypes objects, and it's non-obvious because it requires understanding the control flow and noticing a missing assignment.",2026-01-21T18:30:48.618449+00:00
mellanox/hw-mgmt,2259,hw-mgmt: scripts: ThermalControl: Improve logging subsystem,https://github.com/Mellanox/hw-mgmt/pull/2259,,"**logic:** Logic error: The `else` block at line 1252-1253 logs a ""clear"" message every time the value is NOT exceeding max, including on every valid reading. This causes incorrect repeat message clearing and log spam. The notice should only be called when transitioning from error to OK state, not every time validation passes. Compare to old code which only reset errors when conditions cleared.

Additionally, when `sensor_value < self.val_min`, the method returns `err_flag=False` even though the value is below minimum. The return value doesn't reflect this lower bound violation.",https://github.com/Mellanox/hw-mgmt/pull/2259#discussion_r2713592255,,2026-01-21T17:23:30+00:00,logic,medium,"Greptile correctly identifies that `self.log.notice(None, id=""{} value > max"".format(self.name))` at line 1252-1253 will be called on every valid reading (when sensor_value <= val_max), not just when transitioning from error to OK state. This causes unnecessary log spam and incorrect repeat message clearing behavior. The original code only logged when there was an actual condition change, making this a valid logic regression.",2026-01-21T18:30:48.618470+00:00
bitcoinppl/cove,519,Android: Fix dark mode navigation flash (#510),https://github.com/bitcoinppl/cove/pull/519,,"**logic:** parent theme should be `android:Theme.Material.NoActionBar` (without `.Light`) for dark mode

```suggestion
    <style name=""Theme.Cove"" parent=""android:Theme.Material.NoActionBar"">
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: android/app/src/main/res/values-night/themes.xml
Line: 3:3

Comment:
**logic:** parent theme should be `android:Theme.Material.NoActionBar` (without `.Light`) for dark mode

```suggestion
    <style name=""Theme.Cove"" parent=""android:Theme.Material.NoActionBar"">
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/bitcoinppl/cove/pull/519#discussion_r2713470361,,2026-01-21T16:53:17+00:00,logic,medium,"Greptile correctly identified that the dark mode theme file (values-night/themes.xml) incorrectly uses `Theme.Material.Light.NoActionBar` as its parent, which defeats the purpose of having a dark mode theme. The file should use `Theme.Material.NoActionBar` (the dark variant) to properly support dark mode. This is a non-obvious bug that could cause visual inconsistencies in dark mode.",2026-01-21T18:30:48.618493+00:00
greptileai/react-mirror,393,[WIP] use symbols for scheduler priority levels,https://github.com/greptileai/react-mirror/pull/393,,"**logic:** This change breaks `SchedulerProfiling.js:88` which stores `task.priorityLevel` in an `Int32Array`. Symbols coerce to `0` when stored in typed arrays, causing all profiling events to report `NoPriority`.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/scheduler/src/SchedulerPriorities.js
Line: 17:32

Comment:
**logic:** This change breaks `SchedulerProfiling.js:88` which stores `task.priorityLevel` in an `Int32Array`. Symbols coerce to `0` when stored in typed arrays, causing all profiling events to report `NoPriority`.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/393#discussion_r2714022222,,2026-01-21T19:25:25+00:00,data_integrity,high,"Greptile correctly identifies a breaking change where switching from numeric priority levels to symbols would break SchedulerProfiling.js which stores priorities in Int32Array. Symbols coerce to 0 in typed arrays, causing all profiling to incorrectly report NoPriority. This is a non-obvious cross-file compatibility issue that would corrupt profiling data.",2026-01-21T20:24:21.797781+00:00
greptileai/react-mirror,393,[WIP] use symbols for scheduler priority levels,https://github.com/greptileai/react-mirror/pull/393,,"**logic:** The `SchedulerProfiling-test.js:225` sorts tasks with `t1.priorityLevel - t2.priorityLevel`. Subtracting symbols returns `NaN`, breaking the sort order.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/scheduler/src/SchedulerPriorities.js
Line: 17:32

Comment:
**logic:** The `SchedulerProfiling-test.js:225` sorts tasks with `t1.priorityLevel - t2.priorityLevel`. Subtracting symbols returns `NaN`, breaking the sort order.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/393#discussion_r2714022277,,2026-01-21T19:25:26+00:00,logic,high,"Greptile correctly identifies that changing priority levels from numbers (0-5) to symbols will break any code that performs arithmetic operations like sorting with subtraction (t1.priorityLevel - t2.priorityLevel), since subtracting symbols returns NaN. This is a non-obvious breaking change that affects test code and potentially production code relying on numeric comparisons.",2026-01-21T20:24:21.797852+00:00
infisical/infisical,5225,feat(agent-sentinel): add gateway support for MCP servers,https://github.com/Infisical/infisical/pull/5225,,"**logic:** Potential DNS rebinding vulnerability. When `gatewayId` is provided, users can control the URL which determines `targetHost` used in gateway proxy. Ensure the target URL's hostname is validated before creating the proxy connection to prevent DNS manipulation attacks.

```suggestion
        const originalUrl = new URL(serverUrl);
        // Validate that the hostname matches expected patterns or whitelist
        await verifyHostInputValidity({ host: originalUrl.hostname, isGateway: true, isDynamicSecret: false });
        const targetHost = originalUrl.hostname;
        let targetPort = 80;
```",https://github.com/Infisical/infisical/pull/5225#discussion_r2713841304,,2026-01-21T18:31:11+00:00,security,high,"Greptile correctly identifies that in fetchMcpTools when gatewayId is provided, the serverUrl's hostname is used directly as targetHost without validation. While verifyHostInputValidity is called in other code paths (like discoverOAuthMetadata and createMcpServer), the fetchMcpTools function bypasses this validation when using gateway, allowing potential DNS rebinding or SSRF attacks where an attacker-controlled URL could be proxied through the gateway to access internal resources.",2026-01-21T20:24:21.797900+00:00
nvidia/fuser,5854,ir container composition,https://github.com/NVIDIA/Fuser/pull/5854,,"**logic:** Move constructor lacks null check before dereferencing. If `other` was in a moved-from state with null `container_`, calling `container()->setParent(this)` at line 28 will dereference a null pointer and crash. Should check if `other.container_` is non-null before calling `setParent()`.",https://github.com/NVIDIA/Fuser/pull/5854#discussion_r2713888346,,2026-01-21T18:44:37+00:00,runtime,medium,"Greptile correctly identifies that the move constructor at line 26-28 calls ir_storage()->setParent(this) without checking if ir_storage_ is null. If 'other' was previously moved-from, other.ir_storage_ would be null (since std::move transfers ownership of unique_ptr), and this->ir_storage_ would also be null after the move, causing a null pointer dereference. The move assignment operator at line 52-55 correctly includes this null check, making this inconsistency a real bug.",2026-01-21T20:24:21.797948+00:00
nvidia/fuser,5854,ir container composition,https://github.com/NVIDIA/Fuser/pull/5854,,"**logic:** Move constructor lacks null check before dereferencing. If `other` was in a moved-from state with null `ir_storage_`, calling `ir_storage()->setParent(this)` will dereference a null pointer and crash. Should check if `ir_storage_` is non-null before calling `setParent()`.",https://github.com/NVIDIA/Fuser/pull/5854#discussion_r2713998020,,2026-01-21T19:17:09+00:00,runtime,medium,"Greptile correctly identifies that the move constructor (lines 25-28) calls `ir_storage()->setParent(this)` without checking if `ir_storage_` is null after the move. If `other` was previously moved-from and has a null `ir_storage_`, this would cause a null pointer dereference crash. The move assignment operator at line 52-54 does include this null check, showing inconsistency and validating that the check should also be in the move constructor.",2026-01-21T20:24:21.797988+00:00
nvidia/fuser,5860,optimize mbarrier usage for multi-wave TMA kernels,https://github.com/NVIDIA/Fuser/pull/5860,,"**logic:** `insertNonCircularBufferedTmaMbarriers` can be called with `non_cb_tma_count = 0` if all TMA loads are circular buffered

This allocates a zero-sized tensor and performs unnecessary initialization/cleanup. Add guard:

```suggestion
    if (tma_info.size() > 0) {
      int64_t non_cb_tma_count =
          std::count_if(tma_info.begin(), tma_info.end(), [](const auto& pair) {
            return !pair.first->isCircularBuffered();
          });
      if (non_cb_tma_count > 0) {
        insertNonCircularBufferedTmaMbarriers(non_cb_tma_count, exprs.at(0));
      }
    }
```",https://github.com/NVIDIA/Fuser/pull/5860#discussion_r2714060994,,2026-01-21T19:38:52+00:00,logic,medium,"Greptile correctly identifies that when all TMA loads are circular buffered, `non_cb_tma_count` will be 0, causing `insertNonCircularBufferedTmaMbarriers` to allocate a zero-sized tensor and execute unnecessary initialization. This is a valid edge case bug that could cause undefined behavior or crashes when creating zero-sized shared memory allocations.",2026-01-21T20:24:21.798024+00:00
eventual-inc/daft,6073,feat: add custom date and timestamp formatting for CSV writes,https://github.com/Eventual-Inc/Daft/pull/6073,,"**logic:** Silently ignoring failed timezone parsing. If `tz_str` is present but invalid (e.g., a typo like ""America/New_Yrok""), the code falls back to UTC formatting without warning. Other parts of the codebase (like `src/daft-schema/src/time_unit.rs:180`) return an error for invalid timezones. Consider returning an error instead of silently using UTC.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/daft-writers/src/csv_writer.rs
Line: 261:262

Comment:
**logic:** Silently ignoring failed timezone parsing. If `tz_str` is present but invalid (e.g., a typo like ""America/New_Yrok""), the code falls back to UTC formatting without warning. Other parts of the codebase (like `src/daft-schema/src/time_unit.rs:180`) return an error for invalid timezones. Consider returning an error instead of silently using UTC.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Eventual-Inc/Daft/pull/6073#discussion_r2714088171,,2026-01-21T19:46:56+00:00,logic,medium,"Greptile correctly identified that line 261-262 uses `.and_then(|tz_str| tz_str.parse::<Tz>().ok())` which silently converts parsing failures to None, causing invalid timezone strings to fall back to UTC formatting without any warning or error. This is a data integrity issue where users could get incorrectly formatted timestamps without knowing their timezone was invalid, and it's inconsistent with how other parts of the codebase handle timezone parsing errors.",2026-01-21T20:24:21.798067+00:00
nvidia-nemo/curator,1375,Make vLLM Default for Semantic Workflow + Update Embedding Benchmark + Upate Semantic Tutorials to support cloud,https://github.com/NVIDIA-NeMo/Curator/pull/1375,,"**logic:** `--output-path` is now optional (no `required=True`), but `run_embedding_generation_benchmark` at line 115 requires it and will fail with `Path(None)` if not provided.

```suggestion
    parser.add_argument(
        ""--output-path"",
        required=True,
        type=Path,
        help=""Output directory for embeddings"",
    )
```",https://github.com/NVIDIA-NeMo/Curator/pull/1375#discussion_r2713921943,,2026-01-21T18:54:23+00:00,runtime,medium,"Greptile correctly identified that `--output-path` was changed from having a default value to being optional with no default and no `required=True`. At line 104, `output_path: Path` is required, and at line 115 it's used as `output_path=Path(output_path)` which would fail when `args.output_path` is `None`. This is a real runtime bug that would cause the benchmark to crash when users don't provide the flag.",2026-01-21T20:24:21.798162+00:00
reflex-dev/reflex,6093,handle empty getBackendUrl,https://github.com/reflex-dev/reflex/pull/6093,,"**logic:** Empty string `""""` won't be caught by this check and will cause `new URL("""")` to throw an error on line 96. Should also handle empty strings.

```suggestion
  if (!url_str) {
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: reflex/.templates/web/utils/state.js
Line: 92:92

Comment:
**logic:** Empty string `""""` won't be caught by this check and will cause `new URL("""")` to throw an error on line 96. Should also handle empty strings.

```suggestion
  if (!url_str) {
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/reflex-dev/reflex/pull/6093#discussion_r2713998805,,2026-01-21T19:17:24+00:00,runtime,medium,"Greptile correctly identifies that the check `(url_str ?? undefined) === undefined` only handles null/undefined values, not empty strings. An empty string `""""` would pass this check and then `new URL("""")` on line 96 would throw an error. The suggested fix `if (!url_str)` would correctly handle null, undefined, and empty strings. This is a non-obvious edge case that could cause runtime crashes.",2026-01-21T20:24:21.798209+00:00
constellation-labs/tessellation,1398,fix: improving mpt logic,https://github.com/Constellation-Labs/tessellation/pull/1398,,"**logic:** unsafe cast - `insertAt` can return any node type (`Leaf`, `Branch`, or `Extension`), but here it's assumed to always return `Branch`. If insertion creates a different node type, this will throw `ClassCastException`

Add validation:
```
        insertAt[F](ext.child, key, depth + shared.length, dataDigest).flatMap {
          case branch: MerklePatriciaNode.Branch => 
            ext.withUpdatedChild(branch).pure[F]
          case other => 
            Async[F].raiseError(new IllegalStateException(s""Expected Branch but got ${other.getClass.getSimpleName}""))
        }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: modules/shared/src/main/scala/io/constellationnetwork/schema/mpt/MptStore.scala
Line: 490:490

Comment:
**logic:** unsafe cast - `insertAt` can return any node type (`Leaf`, `Branch`, or `Extension`), but here it's assumed to always return `Branch`. If insertion creates a different node type, this will throw `ClassCastException`

Add validation:
```
        insertAt[F](ext.child, key, depth + shared.length, dataDigest).flatMap {
          case branch: MerklePatriciaNode.Branch => 
            ext.withUpdatedChild(branch).pure[F]
          case other => 
            Async[F].raiseError(new IllegalStateException(s""Expected Branch but got ${other.getClass.getSimpleName}""))
        }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Constellation-Labs/tessellation/pull/1398#discussion_r2714012937,,2026-01-21T19:22:15+00:00,runtime,medium,"Looking at line 490, `insertAt[F](ext.child, key, depth + shared.length, dataDigest).map { updatedChild => ext.withUpdatedChild(updatedChild.asInstanceOf[MerklePatriciaNode.Branch]) }` - the code does perform an unsafe cast with `asInstanceOf[MerklePatriciaNode.Branch]`. Since `insertAt` can return Leaf, Branch, or Extension nodes (as seen in the pattern match at lines 464-470), this cast could throw a ClassCastException at runtime if the insertion results in a non-Branch node type.",2026-01-21T20:24:21.798256+00:00
alpaca-network/gatewayz-backend,891,fix(braintrust): use logger.start_span() for proper project association,https://github.com/Alpaca-Network/gatewayz-backend/pull/891,,"**logic:** Test is incomplete - it patches init_logger but never calls the function being tested, making the test ineffective

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: tests/services/test_braintrust_service.py
Line: 136:150

Comment:
**logic:** Test is incomplete - it patches init_logger but never calls the function being tested, making the test ineffective

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-backend/pull/891#discussion_r2713930159,,2026-01-21T18:56:51+00:00,logic,low,"Greptile is correct - the test `test_initialize_calls_init_logger_with_correct_params` sets up mocks but never actually calls the function being tested (like `braintrust_service.initialize_braintrust()`). The test ends with an empty `with` block and no assertions, making it completely ineffective. This is a non-obvious issue in test code that could lead to false confidence in test coverage.",2026-01-21T20:24:21.798331+00:00
nvidia/numba-cuda,739,Fix compatibility with NumPy 2.4: np.trapz and np.in1d removed,https://github.com/NVIDIA/numba-cuda/pull/739,,"**syntax:** **NameError:** Function renamed to `np_trapezoid` on line 2299 but these lines still reference `np_trapz` which no longer exists.

```suggestion
    overload(np.trapezoid)(np_trapezoid)
if numpy_version < (2, 4):
    overload(np.trapz)(np_trapezoid)
```",https://github.com/NVIDIA/numba-cuda/pull/739#discussion_r2713981791,,2026-01-21T19:12:16+00:00,runtime,critical,"Greptile correctly identified a NameError bug. The function was renamed from `np_trapz` to `np_trapezoid` on line 2299, but lines 2319-2321 still reference `np_trapz` which no longer exists. This would cause a NameError at module import time, completely breaking the module.",2026-01-21T20:24:21.798356+00:00
credal-ai/credal-typescript-sdk,73,":herb: Fern Regeneration -- January 21, 2026",https://github.com/Credal-ai/credal-typescript-sdk/pull/73,,"**logic:** This breaks backward compatibility. The old logic filtered keys that start with `pagination.` (e.g., `pagination.cursor`, `pagination.limit`), but the new logic only filters exact matches. Nested pagination fields like `pagination.cursor` will no longer be ignored.

```suggestion
        const filteredMismatches = Object.keys(mismatches).filter((key) => !ignoredFields.some(field => key.startsWith(field)));
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: tests/mock-server/withJson.ts
Line: 45:45

Comment:
**logic:** This breaks backward compatibility. The old logic filtered keys that start with `pagination.` (e.g., `pagination.cursor`, `pagination.limit`), but the new logic only filters exact matches. Nested pagination fields like `pagination.cursor` will no longer be ignored.

```suggestion
        const filteredMismatches = Object.keys(mismatches).filter((key) => !ignoredFields.some(field => key.startsWith(field)));
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Credal-ai/credal-typescript-sdk/pull/73#discussion_r2714176986,,2026-01-21T20:15:10+00:00,logic,medium,"Greptile correctly identified a backward compatibility issue. The old code used `key.startsWith('pagination.')` to filter keys like 'pagination.cursor' and 'pagination.limit', but the new code uses `ignoredFields.includes(key)` which only matches exact strings. If someone passes `ignoredFields: ['pagination.']`, nested fields like 'pagination.cursor' won't be filtered since 'pagination.' !== 'pagination.cursor'. This is a subtle but real behavioral regression in test infrastructure.",2026-01-21T20:24:21.798373+00:00
simstudioai/sim,2924,fix(auth): improve reset password flow and consolidate brand detection,https://github.com/simstudioai/sim/pull/2924,,"**logic:** `EMAIL_NOT_VERIFIED` errors should set `errorHandled = true` to prevent the fallback error handler from showing a duplicate error message

```suggestion
            // EMAIL_NOT_VERIFIED is handled by the catch block which redirects to /verify
            if (ctx.error.code?.includes('EMAIL_NOT_VERIFIED')) {
              errorHandled = true
              return
            }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/sim/app/(auth)/login/login-form.tsx
Line: 218:220

Comment:
**logic:** `EMAIL_NOT_VERIFIED` errors should set `errorHandled = true` to prevent the fallback error handler from showing a duplicate error message

```suggestion
            // EMAIL_NOT_VERIFIED is handled by the catch block which redirects to /verify
            if (ctx.error.code?.includes('EMAIL_NOT_VERIFIED')) {
              errorHandled = true
              return
            }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/simstudioai/sim/pull/2924#discussion_r2714492841,,2026-01-21T21:54:59+00:00,logic,low,"Greptile correctly identified that when EMAIL_NOT_VERIFIED is handled, the code returns early without setting errorHandled = true. Later, the fallback error handler checks `if (!errorHandled)` and would show a duplicate error message since errorHandled remains false. The fix is straightforward but the bug is easy to miss in the control flow.",2026-01-21T22:20:14.434391+00:00
infisical/infisical,5227,feature: add a new option on PKI ACME enrollment method to skip EAB validation,https://github.com/Infisical/infisical/pull/5227,,"**logic:** ⚠️ **Critical Security Validation Missing**

The PR description states that ""skipping EAB validation cannot be combined with skipping domain validation"" for security reasons, but there's no schema validation to enforce this constraint. Both `skipDnsOwnershipVerification` and `skipEabBinding` can be set to `true` simultaneously, which could create significant security vulnerabilities.

**Fix:** Add a `.refine()` validation after line 138 to prevent both flags from being `true`:

```javascript
.refine(
  (data) => {
    if (data.acmeConfig?.skipEabBinding && data.acmeConfig?.skipDnsOwnershipVerification) {
      return false;
    }
    return true;
  },
  {
    message: ""Cannot skip both EAB binding and DNS ownership verification for security reasons""
  }
)
```

```suggestion
    acmeConfig: z
      .object({
        skipDnsOwnershipVerification: z.boolean().optional(),
        skipEabBinding: z.boolean().optional()
      })
      .refine(
        (config) => {
          if (config.skipEabBinding && config.skipDnsOwnershipVerification) {
            return false;
          }
          return true;
        },
        {
          message: ""Cannot skip both EAB binding and DNS ownership verification for security reasons""
        }
      )
      .optional()
```",https://github.com/Infisical/infisical/pull/5227#discussion_r2714383609,,2026-01-21T21:21:26+00:00,security,high,"Greptile correctly identifies that the schema allows both skipDnsOwnershipVerification and skipEabBinding to be true simultaneously, despite the PR description stating this combination should be prevented for security reasons. This is a real security validation gap that could lead to bypassing both verification mechanisms, and the suggested .refine() solution is accurate and actionable.",2026-01-21T22:20:14.434470+00:00
eventual-inc/daft,6073,feat: add custom date and timestamp formatting for CSV writes,https://github.com/Eventual-Inc/Daft/pull/6073,,"**logic:** Test expects `Date64` to use `timestamp_format`, but `csv_writer.rs:254-280` applies `date_format` to `Date64` types. The comment says ""Date64 is converted to Timestamp[ms] internally by Daft"", but the CSV writer has a separate match arm for `ArrowDataType::Date64` that uses `date_fmt`, not `timestamp_fmt`. Either the CSV writer logic needs updating or the test is incorrect.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: tests/io/test_csv.py
Line: 327:347

Comment:
**logic:** Test expects `Date64` to use `timestamp_format`, but `csv_writer.rs:254-280` applies `date_format` to `Date64` types. The comment says ""Date64 is converted to Timestamp[ms] internally by Daft"", but the CSV writer has a separate match arm for `ArrowDataType::Date64` that uses `date_fmt`, not `timestamp_fmt`. Either the CSV writer logic needs updating or the test is incorrect.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Eventual-Inc/Daft/pull/6073#discussion_r2714392736,,2026-01-21T21:24:50+00:00,logic,medium,"Greptile correctly identified a mismatch between the test expectation and implementation. The test at line 347 expects Date64 types to use `timestamp_format`, but according to Greptile's analysis of csv_writer.rs:254-280, Date64 has its own match arm that uses `date_format`. This is a real inconsistency that would cause the test to fail or document incorrect behavior, and it's non-obvious because it requires understanding both the test and the Rust implementation.",2026-01-21T22:20:14.434551+00:00
nvidia/nvflare,4003,Ignore downloder no ref_id errors,https://github.com/NVIDIA/NVFlare/pull/4003,,"**logic:** Missing `task = None` assignment before the warning. The original code set `task = None` in this else branch to ensure None is returned for non-OK return codes. Without this, when an error return code is received (but no exception occurs), the function returns the task object instead of None, which may cause the caller to incorrectly process an error response.

```suggestion
        else:
            task = None
            self.logger.warning(f""Failed to get_task from {parent_fqcn}. Will try it again."")
```",https://github.com/NVIDIA/NVFlare/pull/4003#discussion_r2714518866,,2026-01-21T22:04:25+00:00,logic,medium,"Greptile correctly identified that the original code had `task = None` in the else branch (line 411-412 in old code), but the diff removes this assignment while keeping the warning. Without `task = None`, when a non-OK return code is received (other than AUTHENTICATION_ERROR), the function returns the task object containing error data instead of None, potentially causing callers to incorrectly process error responses as valid tasks.",2026-01-21T22:20:14.434621+00:00
opentech1/openchat,542,feat: allow users to change their name in settings,https://github.com/tryosschat/openchat/pull/542,,"**logic:** Missing authorization check - any authenticated user can change any other user's name by passing their `userId`.

Compare with `updateTitle` in `chats.ts:418-420` which verifies ownership, and `deleteAccount` in `users.ts:467-469` which verifies the `externalId` matches.

```suggestion
export const updateName = mutation({
	args: {
		userId: v.id(""users""),
		name: v.string(),
	},
	returns: v.object({ success: v.boolean() }),
	handler: async (ctx, args) => {
		// Verify the authenticated user owns this account
		const user = await ctx.db.get(args.userId);
		if (!user) {
			throw new Error(""User not found"");
		}
		
		// Get the current authenticated user to verify authorization
		const authUser = await authComponent.getAuthUser(ctx as unknown as GenericCtx<DataModel>);
		if (!authUser || user.externalId !== authUser.id) {
			throw new Error(""Unauthorized: cannot update another user's name"");
		}

		// Validate name (1-100 chars, no excessive whitespace)
		const trimmedName = args.name.trim();
		if (trimmedName.length === 0 || trimmedName.length > 100) {
			throw new Error(""Name must be between 1 and 100 characters"");
		}

		const now = Date.now();

		// Update profile (primary location for name)
		const profile = await getOrCreateProfile(ctx, args.userId);
		await ctx.db.patch(profile._id, {
			name: trimmedName,
			updatedAt: now,
		});

		// Also update user table for backwards compatibility during migration
		await ctx.db.patch(args.userId, {
			name: trimmedName,
			updatedAt: now,
		});

		return { success: true };
	},
});
```",https://github.com/tryosschat/openchat/pull/542#discussion_r2714517187,,2026-01-21T22:03:44+00:00,security,critical,The code accepts a userId parameter and directly updates that user's name without verifying that the authenticated user has permission to modify that account. This is a classic Insecure Direct Object Reference (IDOR) vulnerability that allows any authenticated user to change any other user's name. Greptile correctly identifies this missing authorization check and provides a concrete fix with proper verification pattern.,2026-01-21T22:20:14.434687+00:00
nvidia/physicsnemo,1339,Figconvnet refactor,https://github.com/NVIDIA/physicsnemo/pull/1339,,"**logic:** The `to()` method doesn't modify tensors in-place. Should use `self._neighbors_index = self._neighbors_index.to(device)` to actually move the tensors.

```suggestion
        self._neighbors_index = self._neighbors_index.to(device)
        self._neighbors_row_splits = self._neighbors_row_splits.to(device)
```",https://github.com/NVIDIA/physicsnemo/pull/1339#discussion_r2714450072,,2026-01-21T21:42:54+00:00,logic,medium,"Greptile correctly identified that `tensor.to(device)` returns a new tensor and does not modify in-place. The current code `self._neighbors_index.to(device)` discards the return value, so the method appears to move tensors but actually does nothing. This is a real bug that would cause silent failures when trying to move data between devices.",2026-01-21T22:20:14.434809+00:00
nvidia/physicsnemo,1339,Figconvnet refactor,https://github.com/NVIDIA/physicsnemo/pull/1339,,"**syntax:** The `extend()` method expects an iterable, but you're passing a single module object. This should be wrapped in a list.

```suggestion
                down_block.extend([
                    GridFeatureConv2DBlocksAndIntraCommunication(
                        in_channels=hidden_channels[level + 1],
                        out_channels=hidden_channels[level + 1],
                        kernel_size=kernel_size,
                        stride=1,
                        compressed_spatial_dims=compressed_spatial_dims,
                        communication_types=communication_types,
                    )
                ])
```",https://github.com/NVIDIA/physicsnemo/pull/1339#discussion_r2714450166,,2026-01-21T21:42:56+00:00,runtime,high,Greptile correctly identified that `extend()` expects an iterable but is being passed a single `GridFeatureConv2DBlocksAndIntraCommunication` object. This would cause a TypeError at runtime since a module object is not iterable. The same bug appears twice in the diff - once for `down_block.extend()` and once for `up_block.extend()`. This is a subtle change from the original `append()` to `extend()` that introduces a bug.,2026-01-21T22:20:14.434850+00:00
nvidia/physicsnemo,1339,Figconvnet refactor,https://github.com/NVIDIA/physicsnemo/pull/1339,,"**syntax:** Same issue here - `extend()` needs an iterable, not a single module object.

```suggestion
                up_block.extend([
                    GridFeatureConv2DBlocksAndIntraCommunication(
                        in_channels=hidden_channels[level],
                        out_channels=hidden_channels[level],
                        kernel_size=kernel_size,
                        up_stride=1,
                        compressed_spatial_dims=compressed_spatial_dims,
                        communication_types=communication_types,
                    )
                ])
```",https://github.com/NVIDIA/physicsnemo/pull/1339#discussion_r2714450199,,2026-01-21T21:42:57+00:00,runtime,high,"Greptile correctly identified that `extend()` is being called with a single `GridFeatureConv2DBlocksAndIntraCommunication` object instead of a list. The `extend()` method requires an iterable, and passing a single nn.Module object will cause a TypeError at runtime since it will try to iterate over the module's internal structure rather than adding it as a single element. The code should use `append()` or wrap the object in a list `[...]` for `extend()`.",2026-01-21T22:20:14.434893+00:00
nvidia/physicsnemo,1339,Figconvnet refactor,https://github.com/NVIDIA/physicsnemo/pull/1339,,"**logic:** Potential bug: calling conv1 without the out_point_features argument, but out_point_features was just assigned on line 701-703.

```suggestion
            out = self.conv1(in_point_features, out_point_features)
```

 Should this be `self.conv1(in_point_features, out_point_features)` to pass the downsampled points as the second argument?",https://github.com/NVIDIA/physicsnemo/pull/1339#discussion_r2714450786,,2026-01-21T21:43:09+00:00,logic,medium,"Looking at the code, when `out_point_feature_type == ""downsample""`, `out_point_features` is computed via `voxel_down_sample()` on line 701-703, but then `self.conv1(in_point_features)` is called without passing this computed `out_point_features`. This means the downsampled output points are computed but never used in the convolution, which would cause incorrect behavior since conv1 would use 'same' logic instead of the intended downsampled points. This is a subtle logic bug that's easy to miss.",2026-01-21T22:20:14.434921+00:00
nvidia-nemo/emerging-optimizers,89,"added hyperball update step, created a mixin to have different weight update",https://github.com/NVIDIA-NeMo/Emerging-Optimizers/pull/89,,"**logic:** When `R = 0` (parameter initialized to zeros), the update will always remain zero since `alpha=-lr * R = 0`. This prevents learning entirely if parameters start at zero.

Consider adding a guard:
```suggestion
        elif weight_update_method == ""hyperball"":
            # Norm-preserving update: W_{t+1} = R * normalize(W_t - lr * R * normalize(update))
            # Compute R = ||W_t||_F (Frobenius norm)
            R = p.norm()
            
            # Fall back to SGD if parameter norm is too small
            if R < eps:
                p.add_(update, alpha=-lr)
                return

            # Normalize the update (Frobenius norm over all elements)
            normalized_update = update / (update.norm() + eps)

            # Compute W_t - lr * R * normalize(update)
            p.add_(normalized_update, alpha=-lr * R)

            # Normalize the result and scale back by R: p = R * (p / ||p||)
            p.mul_(R / (p.norm() + eps))
```",https://github.com/NVIDIA-NeMo/Emerging-Optimizers/pull/89#discussion_r2714258106,,2026-01-21T20:38:14+00:00,logic,medium,"Greptile correctly identifies that when R (the Frobenius norm of p) is zero, the hyperball update becomes a no-op since `alpha=-lr * R = 0`, and the final multiplication `p.mul_(R / (p.norm() + eps))` will also zero out any update. This is a non-obvious edge case that would silently prevent learning for zero-initialized parameters, and the suggested fallback to SGD is a reasonable fix.",2026-01-21T22:20:14.434943+00:00
posthog/posthog,45643,fix(comments/discussions): have email link go to correct page,https://github.com/PostHog/posthog/pull/45643,,"**logic:** slug missing `#panel=discussion` hash. should match the pattern in `utils.ts:discussionsSlug()` to auto-open discussion panel

```suggestion
                    slug: `/replay/${props.recordingId}#panel=discussion`,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/session-recordings/player/commenting/playerFrameCommentOverlayLogic.ts
Line: 134:134

Comment:
**logic:** slug missing `#panel=discussion` hash. should match the pattern in `utils.ts:discussionsSlug()` to auto-open discussion panel

```suggestion
                    slug: `/replay/${props.recordingId}#panel=discussion`,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45643#discussion_r2714700925,,2026-01-21T23:26:13+00:00,logic,medium,"The PR title says 'have email link go to correct page', and Greptile correctly identifies that the slug is missing the `#panel=discussion` hash fragment that would auto-open the discussion panel when users click the email link. This is a real logic bug that defeats the purpose of the fix - the email link would go to the replay page but wouldn't automatically show the discussion panel as intended.",2026-01-22T00:49:29.086205+00:00
posthog/posthog,45643,fix(comments/discussions): have email link go to correct page,https://github.com/PostHog/posthog/pull/45643,,"**logic:** slug missing `#panel=discussion` hash. should match the pattern in `utils.ts:discussionsSlug()` to auto-open discussion panel

```suggestion
                    slug: `/replay/${props.recordingId}#panel=discussion`,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/session-recordings/player/commenting/playerFrameCommentOverlayLogic.ts
Line: 179:179

Comment:
**logic:** slug missing `#panel=discussion` hash. should match the pattern in `utils.ts:discussionsSlug()` to auto-open discussion panel

```suggestion
                    slug: `/replay/${props.recordingId}#panel=discussion`,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45643#discussion_r2714700947,,2026-01-21T23:26:13+00:00,logic,medium,"The PR title explicitly states the goal is to fix email links to go to the correct page. Greptile correctly identified that the slug is missing the `#panel=discussion` hash fragment, which according to the comment is needed to auto-open the discussion panel - this would directly undermine the PR's intended fix. The pattern should match `discussionsSlug()` utility function to ensure consistent behavior.",2026-01-22T00:49:29.086282+00:00
posthog/posthog,45643,fix(comments/discussions): have email link go to correct page,https://github.com/PostHog/posthog/pull/45643,,"**logic:** fallback href missing `#panel=discussion` hash. email links won't auto-open discussion panel when slug not provided

```suggestion
                href = f""{settings.SITE_URL}/replay/{comment.item_id}#panel=discussion""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/tasks/email.py
Line: 754:754

Comment:
**logic:** fallback href missing `#panel=discussion` hash. email links won't auto-open discussion panel when slug not provided

```suggestion
                href = f""{settings.SITE_URL}/replay/{comment.item_id}#panel=discussion""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45643#discussion_r2714700972,,2026-01-21T23:26:14+00:00,logic,medium,"Greptile correctly identifies that the new fallback URLs for Replay and Notebook scopes are missing the `#panel=discussion` hash fragment that would auto-open the discussion panel. This is a subtle UX bug where email links would take users to the correct page but not automatically show the discussion they were mentioned in, making the mention notification less useful.",2026-01-22T00:49:29.086340+00:00
posthog/posthog,45647,feat(insights): allow goal line color to be changed,https://github.com/PostHog/posthog/pull/45647,,"**logic:** The color picker UI correctly updates `borderColor`, but the new LineGraph implementation in `frontend/src/queries/nodes/DataVisualization/Components/Charts/LineGraph.tsx` (lines 214-272) doesn't use the `borderColor` property when rendering goal line annotations. The old LineGraph at `frontend/src/scenes/insights/views/LineGraph/LineGraph.tsx` (line 707) correctly uses `borderColor`, but if the new LineGraph is being used, color changes won't appear visually. The new LineGraph needs to add `borderColor` to its annotation configuration.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/components/GoalLinesList.tsx
Line: 31:46

Comment:
**logic:** The color picker UI correctly updates `borderColor`, but the new LineGraph implementation in `frontend/src/queries/nodes/DataVisualization/Components/Charts/LineGraph.tsx` (lines 214-272) doesn't use the `borderColor` property when rendering goal line annotations. The old LineGraph at `frontend/src/scenes/insights/views/LineGraph/LineGraph.tsx` (line 707) correctly uses `borderColor`, but if the new LineGraph is being used, color changes won't appear visually. The new LineGraph needs to add `borderColor` to its annotation configuration.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45647#discussion_r2714671574,This makes sense! We have two implementations - for weird legacy reasons. It'd be interesting if you could update both to support this :),2026-01-21T23:11:51+00:00,logic,medium,"Greptile correctly identified that while the UI component updates the borderColor property, the new LineGraph implementation doesn't use it - meaning color changes would only work in the old graph but not the new one. The developer confirmed this is a valid catch by saying 'This makes sense!' and asking for both implementations to be updated.",2026-01-22T00:49:29.086394+00:00
infisical/infisical,5230,docs: add rate limits section to API reference,https://github.com/Infisical/infisical/pull/5230,,"**logic:** The documented rate limit values don't match the actual implementation. According to the backend code:

- Database schema defaults (`backend/src/db/schemas/rate-limit.ts`): `readRateLimit: 600`, `secretsRateLimit: 60`, `writeRateLimit: 200`
- Self-hosted defaults (`backend/src/server/config/rateLimiter.ts`): `readLimit: 60`, `secretsLimit: 60`, `writeLimit: 200`
- Cloud plan defaults (`backend/src/ee/services/license/license-fns.ts`): `readLimit: 60`, `secretsLimit: 40`, `writeLimit: 200`

The statement ""300 for read and secret operations"" doesn't align with any of these values. Please verify which defaults should be documented (cloud vs self-hosted vs database schema) and update accordingly.",https://github.com/Infisical/infisical/pull/5230#discussion_r2714826464,,2026-01-22T00:14:23+00:00,data_integrity,medium,"Greptile correctly identified that the documented rate limits (300 for read/secret, 200 for write) don't match any of the actual implementation values in the codebase. This is a documentation accuracy issue that could mislead users about actual API behavior - the values cited (600, 60, 40, 200 across different configs) don't match what's being documented. While a docs issue, incorrect rate limit documentation can cause real operational problems for API consumers.",2026-01-22T00:49:29.086467+00:00
nvidia/transformerengine,2477,"Add support for SWA (left, right) with FusedAttention ",https://github.com/NVIDIA/TransformerEngine/pull/2477,,"**syntax:** wrong variable - should be `use_flash_attention_2 = False`

this disables all FlashAttention backends when only FlashAttention 2 should be disabled for sliding window with cross-attention

```suggestion
            use_flash_attention_2 = False
```",https://github.com/NVIDIA/TransformerEngine/pull/2477#discussion_r2714819485,,2026-01-22T00:12:11+00:00,logic,medium,"Greptile correctly identified a typo bug - the code sets `use_flash_attention = False` but the variable used throughout the function is `use_flash_attention_2`. This typo means the intended disabling logic won't work, potentially allowing FlashAttention 2 to be used in unsupported configurations. The same bug appears twice in the diff (lines 911 and 935).",2026-01-22T00:49:29.086519+00:00
nvidia/transformerengine,2477,"Add support for SWA (left, right) with FusedAttention ",https://github.com/NVIDIA/TransformerEngine/pull/2477,,"**syntax:** wrong variable - should be `use_flash_attention_2 = False`

this disables all FlashAttention backends when only FlashAttention 2 should be disabled for ALiBi with cross-attention

```suggestion
                use_flash_attention_2 = False
```",https://github.com/NVIDIA/TransformerEngine/pull/2477#discussion_r2714819533,,2026-01-22T00:12:12+00:00,logic,medium,"Greptile correctly identified a typo bug where `use_flash_attention = False` is used instead of `use_flash_attention_2 = False`. This appears in two places in the diff (lines 907 and 936) and would set an undefined/wrong variable, failing to properly disable FlashAttention 2 when intended. This is a subtle typo that could easily be missed by reviewers.",2026-01-22T00:49:29.086605+00:00
opentech1/openchat,546,perf: speed up Railway builds by excluding unnecessary files,https://github.com/tryosschat/openchat/pull/546,4,"**logic:** `shiki` is listed as a production dependency in `apps/web/package.json:54`, so excluding it will break the runtime application if it's used. Verify if `shiki` is only needed at build time or if it's required at runtime.

**Certainty: 4/5** - The package is definitively in production dependencies, but I cannot find runtime usage in the codebase (might be dynamically imported or used in compiled output).",https://github.com/tryosschat/openchat/pull/546#discussion_r2714778915,,2026-01-22T00:01:18+00:00,runtime,medium,"Greptile correctly identifies that shiki is listed as a production dependency but is being excluded via .railpackignore. This could break the runtime application if shiki is used for syntax highlighting at runtime (common for code display). While Greptile notes uncertainty about runtime usage, flagging the contradiction between production dependency status and exclusion is valuable and non-obvious.",2026-01-22T00:49:29.086646+00:00
quiltdata/quilt,4700,Bump lodash and @graphql-codegen/cli in /catalog,https://github.com/quiltdata/quilt/pull/4700,,"**logic:** `@graphql-codegen/cli` was upgraded from v1.21.8 to v6.1.1 (skipping v2-v5), but all related plugins remain at v1:
- `@graphql-codegen/near-operation-file-preset`: ^1.18.6
- `@graphql-codegen/typed-document-node`: ^1.18.10
- `@graphql-codegen/typescript`: ^1.23.0
- `@graphql-codegen/typescript-operations`: ^1.18.4
- `@graphql-codegen/urql-introspection`: ^1.3.1

This version mismatch may cause compatibility issues when running `npm run gql:generate`. The v6 CLI introduced breaking changes including dropping `@graphql-tools/prisma-loader` and adding `allowPartialOutputs` flag.",https://github.com/quiltdata/quilt/pull/4700#discussion_r2714733114,,2026-01-21T23:41:48+00:00,runtime,high,"Greptile correctly identified that @graphql-codegen/cli was upgraded from v1 to v6 while all related plugins remain at v1.x versions. This is a real compatibility issue - GraphQL Codegen v6 introduced major breaking changes and typically requires matching major versions for its plugins, which would likely cause the gql:generate script to fail at runtime.",2026-01-22T00:49:29.086691+00:00
nvidia/earth2studio,639,Data sources pygrib migration,https://github.com/NVIDIA/earth2studio/pull/639,,"**logic:** This datetime modification won't persist since `dt` is a local variable. The original `times[i]` in the list won't be updated.

```suggestion
        for i, dt in enumerate(times):
            if dt.tzinfo is None:
                # Enforce UTC timezone
                times[i] = dt.replace(tzinfo=timezone.utc)
                dt = times[i]
```",https://github.com/NVIDIA/earth2studio/pull/639#discussion_r2714658849,,2026-01-21T23:05:35+00:00,logic,medium,"Greptile correctly identified that the loop variable `dt` is reassigned with `dt = dt.replace(tzinfo=timezone.utc)`, but this doesn't modify the original `times` list. The old code used `times[i] = times[i].replace(tzinfo=timezone.utc)` which correctly updated the list. This means timezone-naive datetimes won't actually get their timezone set, potentially causing comparison issues with timezone-aware datetimes later in the code. This is a subtle bug that could easily be missed during refactoring from index-based to iterator-based looping.",2026-01-22T00:49:29.086736+00:00
nvidia-nemo/emerging-optimizers,89,"added hyperball update step, created a mixin to have different weight update",https://github.com/NVIDIA-NeMo/Emerging-Optimizers/pull/89,,"**logic:** `dim=0` normalizes only along first dimension, not over all elements as the comment says. For Frobenius norm over entire tensor, omit `dim` parameter

```suggestion
            normalized_update = F.normalize(update.flatten(), p=2, eps=eps).view_as(update)
```",https://github.com/NVIDIA-NeMo/Emerging-Optimizers/pull/89#discussion_r2714646551,,2026-01-21T22:59:44+00:00,logic,medium,"Greptile correctly identified that `F.normalize(update, p=2, dim=0, eps=eps)` normalizes along dimension 0 only, not over all elements as the comment states ('Frobenius norm over all elements'). For a true Frobenius norm normalization, the tensor should be flattened first or dim should be omitted. This mismatch between code and documented intent would cause incorrect behavior for the hyperball weight update algorithm.",2026-01-22T00:49:29.086778+00:00
posthog/posthog,45656,feat(product tours): enable linking existing feature flag,https://github.com/PostHog/posthog/pull/45656,,"**logic:** Missing ownership validation for `linked_flag_id`. A user could potentially link to a feature flag from a different team. Add a `validate()` method similar to the one in `/posthog/api/survey.py:406-431` that verifies the feature flag belongs to the same team and validates the variant if `linkedFlagVariant` is specified in conditions.

```suggestion
    linked_flag_id = serializers.IntegerField(required=False, write_only=True, allow_null=True)

    def validate(self, data):
        linked_flag_id = data.get(""linked_flag_id"")
        linked_flag = None
        if linked_flag_id:
            try:
                from posthog.models import FeatureFlag
                linked_flag = FeatureFlag.objects.get(pk=linked_flag_id, team_id=self.context[""team_id""])
            except FeatureFlag.DoesNotExist:
                raise serializers.ValidationError(""Feature Flag with this ID does not exist"")

        # Validate linkedFlagVariant if provided
        conditions = (data.get(""content"") or {}).get(""conditions"") or {}
        linked_flag_variant = conditions.get(""linkedFlagVariant"")
        if linked_flag_variant and linked_flag and linked_flag_variant != ""any"":
            # Get available variants from the linked feature flag
            available_variants = [variant[""key""] for variant in (linked_flag.filters.get(""multivariate"") or {}).get(""variants"", [])]
            if linked_flag_variant not in available_variants:
                if available_variants:
                    raise serializers.ValidationError(
                        f""Feature flag variant '{linked_flag_variant}' does not exist. Available variants: {', '.join(available_variants)}""
                    )
                else:
                    raise serializers.ValidationError(
                        f""Feature flag variant '{linked_flag_variant}' specified but the linked feature flag has no variants""
                    )
        elif linked_flag_variant and not linked_flag_id:
            raise serializers.ValidationError(""linkedFlagVariant can only be used when a linked_flag_id is specified"")

        return data


**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/product_tours/backend/api/product_tour.py
Line: 139:139

Comment:
**logic:** Missing ownership validation for `linked_flag_id`. A user could potentially link to a feature flag from a different team. Add a `validate()` method similar to the one in `/posthog/api/survey.py:406-431` that verifies the feature flag belongs to the same team and validates the variant if `linkedFlagVariant` is specified in conditions.

```suggestion
    linked_flag_id = serializers.IntegerField(required=False, write_only=True, allow_null=True)

    def validate(self, data):
        linked_flag_id = data.get(""linked_flag_id"")
        linked_flag = None
        if linked_flag_id:
            try:
                from posthog.models import FeatureFlag
                linked_flag = FeatureFlag.objects.get(pk=linked_flag_id, team_id=self.context[""team_id""])
            except FeatureFlag.DoesNotExist:
                raise serializers.ValidationError(""Feature Flag with this ID does not exist"")

        # Validate linkedFlagVariant if provided
        conditions = (data.get(""content"") or {}).get(""conditions"") or {}
        linked_flag_variant = conditions.get(""linkedFlagVariant"")
        if linked_flag_variant and linked_flag and linked_flag_variant != ""any"":
            # Get available variants from the linked feature flag
            available_variants = [variant[""key""] for variant in (linked_flag.filters.get(""multivariate"") or {}).get(""variants"", [])]
            if linked_flag_variant not in available_variants:
                if available_variants:
                    raise serializers.ValidationError(
                        f""Feature flag variant '{linked_flag_variant}' does not exist. Available variants: {', '.join(available_variants)}""
                    )
                else:
                    raise serializers.ValidationError(
                        f""Feature flag variant '{linked_flag_variant}' specified but the linked feature flag has no variants""
                    )
        elif linked_flag_variant and not linked_flag_id:
            raise serializers.ValidationError(""linkedFlagVariant can only be used when a linked_flag_id is specified"")

        return data


**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45656#discussion_r2715080660,,2026-01-22T02:34:10+00:00,security,high,"The code adds a `linked_flag_id` field that allows users to link a feature flag to a product tour, but there's no validation that the feature flag belongs to the same team. This is a cross-tenant data access vulnerability - a user could link to any feature flag ID from a different team. The diff shows only the field definition without any ownership validation, and similar validation exists in the codebase for surveys (referenced at `/posthog/api/survey.py:406-431`).",2026-01-22T03:00:21.805455+00:00
posthog/posthog,45654,feat(test): add diagnostic logging for Storybook loader timeouts,https://github.com/PostHog/posthog/pull/45654,,"**logic:** `filter({ hasText: /./ })` doesn't check visibility - it checks if elements contain text. This means the visible count will incorrectly exclude loaders without text content (like spinners or skeleton loaders). Use `.filter({ visible: true })` instead.

```suggestion
        const visibleLoaderCount = await page.locator(loaderSelectors).filter({ visible: true }).count()
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: common/storybook/.storybook/test-runner.ts
Line: 196:196

Comment:
**logic:** `filter({ hasText: /./ })` doesn't check visibility - it checks if elements contain text. This means the visible count will incorrectly exclude loaders without text content (like spinners or skeleton loaders). Use `.filter({ visible: true })` instead.

```suggestion
        const visibleLoaderCount = await page.locator(loaderSelectors).filter({ visible: true }).count()
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45654#discussion_r2714937914,Fixed in 4b7a29b40e - replaced `filter({ hasText: /./ })` with `filter({ visible: true })` to correctly check visibility instead of text content.,2026-01-22T01:12:58+00:00,logic,medium,"Looking at the diff, line 196 shows `.filter({ visible: true })` which matches Greptile's suggestion - but this is because the fix was already applied. The developer confirmed in their reply that Greptile caught a real bug where `filter({ hasText: /./ })` was incorrectly used instead of `filter({ visible: true })`, which would have excluded text-less loaders like spinners from the visibility count. This is a non-obvious logical error that the developer validated and fixed.",2026-01-22T03:00:21.805627+00:00
browserbase/stagehand,1593,update the extract tool in v2 agent,https://github.com/browserbase/stagehand/pull/1593,,"**logic:** `z.enum()` requires at least 1 enum value, but `schema.enum` could be empty

```suggestion
      if (schema.enum && schema.enum.length > 0) return z.enum(schema.enum as [string, ...string[]]);
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: lib/agent/tools/extract.ts
Line: 31:31

Comment:
**logic:** `z.enum()` requires at least 1 enum value, but `schema.enum` could be empty

```suggestion
      if (schema.enum && schema.enum.length > 0) return z.enum(schema.enum as [string, ...string[]]);
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/browserbase/stagehand/pull/1593#discussion_r2715030849,,2026-01-22T02:02:15+00:00,runtime,medium,"Greptile correctly identified that line 31 checks `if (schema.enum)` but doesn't verify the array has elements. Zod's `z.enum()` requires at least one value, so passing an empty array would cause a runtime error. The fix adds `schema.enum.length > 0` check which was actually implemented in the final code, confirming this was a valid bug that was fixed.",2026-01-22T03:00:21.805667+00:00
browserbase/stagehand,1592,update extract,https://github.com/browserbase/stagehand/pull/1592,,"**syntax:** removed `logger` parameter breaks call site at `packages/core/lib/v3/agent/tools/index.ts:90` which still passes `options?.logger`

```suggestion
export const extractTool = (v3: V3, executionModel?: string, logger?: (message: LogLine) => void) =>
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/core/lib/v3/agent/tools/extract.ts
Line: 46:46

Comment:
**syntax:** removed `logger` parameter breaks call site at `packages/core/lib/v3/agent/tools/index.ts:90` which still passes `options?.logger`

```suggestion
export const extractTool = (v3: V3, executionModel?: string, logger?: (message: LogLine) => void) =>
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/browserbase/stagehand/pull/1592#discussion_r2715015289,,2026-01-22T01:53:13+00:00,runtime,medium,"The diff shows the `logger` parameter was removed from the `extractTool` function signature. Greptile correctly identifies this will break the call site at `packages/core/lib/v3/agent/tools/index.ts:90` which still passes `options?.logger` as a third argument. This is a valid breaking change that would cause issues at the call site, though TypeScript might catch it depending on the configuration. The analysis is specific and actionable with a concrete fix suggestion.",2026-01-22T03:00:21.805724+00:00
browserbase/stagehand,1592,update extract,https://github.com/browserbase/stagehand/pull/1592,,"**logic:** if `schema.enum` is empty array, `z.enum()` requires at least one element and will throw runtime error

```suggestion
      if (schema.enum && schema.enum.length > 0) return z.enum(schema.enum as [string, ...string[]]);
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/core/lib/v3/agent/tools/extract.ts
Line: 31:31

Comment:
**logic:** if `schema.enum` is empty array, `z.enum()` requires at least one element and will throw runtime error

```suggestion
      if (schema.enum && schema.enum.length > 0) return z.enum(schema.enum as [string, ...string[]]);
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/browserbase/stagehand/pull/1592#discussion_r2715015304,,2026-01-22T01:53:14+00:00,runtime,medium,"Greptile correctly identified that line 31 in the original code (`if (schema.enum) return z.enum(schema.enum as [string, ...string[]]);`) would cause a runtime error if `schema.enum` is an empty array, since `z.enum()` requires at least one element. The suggested fix adds `schema.enum.length > 0` check to prevent this crash. This is a non-obvious edge case that could cause production failures.",2026-01-22T03:00:21.805754+00:00
nvidia/fuser,5855,Reference implementation for allgather + linear ring-based overlap,https://github.com/NVIDIA/Fuser/pull/5855,,"**logic:** Reference output computed with sharded weight instead of full weight. `weight_shard` has shape `[4*h/d, h]` but should be using `weight_ref` with shape `[4*h, h]` to compute the correct expected output for all-gathered results.

```suggestion
    out_ref = torch.nn.functional.linear(inp_ref.cuda(), weight_ref.cuda())
```",https://github.com/NVIDIA/Fuser/pull/5855#discussion_r2715138383,,2026-01-22T03:08:27+00:00,logic,medium,"Greptile correctly identified that the reference output calculation is incorrect. The test computes `out_ref = torch.nn.functional.linear(inp_ref.cuda(), weight_shard)` but the column-parallel linear operation with all-gather should produce output using the full weight matrix. Since inp_shard is gathered to full inp_ref in the reference implementation, the expected output should use weight_ref.cuda() (full weight), not weight_shard (sharded weight). This would cause the test's assert_close to compare against an incorrect reference value, potentially masking bugs in the actual implementation.",2026-01-22T04:32:54.584573+00:00
raycast/extensions,24764,Update `Simple Dictionary` extension - set default language in Preferences,https://github.com/raycast/extensions/pull/24764,,"**logic:** calling `showFailureToast()` during render violates React rules - side effects must be in `useEffect`, and conditional returns after hooks break Rules of Hooks

```suggestion
  useEffect(() => {
    if (!language) {
      showFailureToast(""Please select a language"");
      return;
    }

    d = new Dictionary(language, word);
```",https://github.com/raycast/extensions/pull/24764#discussion_r2715734593,,2026-01-22T07:57:53+00:00,logic,medium,"Greptile correctly identifies two React violations: (1) showFailureToast() is called during render which is a side effect that should be in useEffect, and (2) the conditional return on line 40-51 happens BEFORE the useEffect hook on line 53, violating the Rules of Hooks since hooks must be called unconditionally in the same order every render. This would cause React errors or unpredictable behavior when language is undefined.",2026-01-22T08:21:00.341647+00:00
democratizedspace/dspace,3017,"Fix docs links, add Testing Guide, and enforce markdown link validation",https://github.com/democratizedspace/dspace/pull/3017,,"**logic:** mapping `v3` to `HEAD` assumes the local repo is on the `v3` branch, but `HEAD` may point to `main` or another branch depending on the current checkout

```suggestion
  const refToCheck = ref === 'v3' ? 'origin/v3' : (['main', 'master'].includes(ref) ? 'HEAD' : ref);
```",https://github.com/democratizedspace/dspace/pull/3017#discussion_r2715658354,,2026-01-22T07:30:24+00:00,logic,medium,"Greptile correctly identifies that mapping 'v3' to 'HEAD' is flawed - if the local checkout is on 'main' branch, HEAD won't contain v3's files, causing false positives for valid v3 branch links. The suggestion to use 'origin/v3' is a valid fix that would check against the actual remote branch state.",2026-01-22T08:21:00.341742+00:00
democratizedspace/dspace,3016,"Add docs link integrity checks and Testing Guide, fix UI lifecycle links",https://github.com/democratizedspace/dspace/pull/3016,,"**logic:** `fetchStatus()` doesn't handle network errors, DNS failures, or timeouts. A failed fetch will throw an unhandled error and crash the test. Should wrap in try/catch to return error status or handle gracefully.",https://github.com/democratizedspace/dspace/pull/3016#discussion_r2715656376,,2026-01-22T07:29:38+00:00,runtime,medium,"The `fetchStatus()` function uses `fetch()` without any try/catch handling. If a network error, DNS failure, or timeout occurs, the function will throw an unhandled exception that propagates up and crashes the test with an unhelpful error message rather than properly reporting the broken link. This is a valid bug that affects test reliability.",2026-01-22T08:21:00.341785+00:00
posthog/posthog,45659,chore(frontend): Add new capabilities for LemonInputSelect,https://github.com/PostHog/posthog/pull/45659,,"**logic:** `setInputValue` applies `inputTransform` (line 291-294), so when entering edit mode in single-select, the value will be transformed. If the stored value doesn't match the transformed format, this creates inconsistency - e.g., stored value ""Test Value"" becomes ""test-value"" when focused if `inputTransform` lowercases and replaces spaces.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/lemon-ui/LemonInputSelect/LemonInputSelect.tsx
Line: 426:427

Comment:
**logic:** `setInputValue` applies `inputTransform` (line 291-294), so when entering edit mode in single-select, the value will be transformed. If the stored value doesn't match the transformed format, this creates inconsistency - e.g., stored value ""Test Value"" becomes ""test-value"" when focused if `inputTransform` lowercases and replaces spaces.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45659#discussion_r2715909122,,2026-01-22T08:53:08+00:00,logic,medium,"Greptile correctly identifies that in _onFocus (line 424-427), when mode is 'single' and a value exists, it calls setInputValue(getStringKey(values[0])). The setInputValue function (lines 288-294) applies inputTransform if provided, meaning the stored value gets transformed when entering edit mode. This creates an inconsistency where a value like 'Test Value' would become 'test-value' upon focus, which is unexpected behavior and could confuse users or cause data integrity issues.",2026-01-22T10:19:10.903455+00:00
ever-co/ever-gauzy,9359,fix: upgrade cross-env to v10 and improve env replacement,https://github.com/ever-co/ever-gauzy/pull/9359,,"missing command after `&&` - this script will fail with a syntax error

```suggestion
		""build:package:ui-config"": ""cross-env NODE_ENV=development NODE_OPTIONS=--max-old-space-size=12288 yarn nx build ui-config --configuration=development"",
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: package.json
Line: 132:132

Comment:
missing command after `&&` - this script will fail with a syntax error

```suggestion
		""build:package:ui-config"": ""cross-env NODE_ENV=development NODE_OPTIONS=--max-old-space-size=12288 yarn nx build ui-config --configuration=development"",
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-gauzy/pull/9359#discussion_r2716143666,,2026-01-22T09:52:10+00:00,runtime,medium,"Greptile correctly identified that line 132 has 'cross-env NODE_ENV=development NODE_OPTIONS=--max-old-space-size=12288 && yarn nx build ui-config' with an errant '&&' between the cross-env command and the yarn command. This is incorrect syntax - cross-env sets environment variables for the command that follows it (without &&), so this script would fail because there's nothing before '&&'. Other similar lines in the diff don't have this '&&', confirming this is an inconsistency/bug.",2026-01-22T10:19:10.903548+00:00
mellanox/hw-mgmt,2262,Update changelog and release notes for V.7.0040.4248,https://github.com/Mellanox/hw-mgmt/pull/2262,,**logic:** Verify that this feature is actually included in this release. No commits in the branch history mention GB3000 or mp2891 changes. The only changes between V.7.0040.4247 and this release are the two xdpe1a2g7b patches (commits 21a8d75 and 17d2054). Was this feature merged separately or is it planned for a different release?,https://github.com/Mellanox/hw-mgmt/pull/2262#discussion_r2716603394,,2026-01-22T11:57:42+00:00,data_integrity,medium,"Greptile correctly identified a potential documentation inconsistency - the release notes claim a GB3000/mp2891 feature but only xdpe1a2g7b patches are mentioned in the kernel patches section. This mismatch between claimed features and actual changes could indicate either incorrect release notes or missing commits, which is a legitimate concern for release documentation accuracy.",2026-01-22T12:26:40.689674+00:00
useautumn/autumn,613,fix: 🐛 attach on dashboard respects success url,https://github.com/useautumn/autumn/pull/613,,"**logic:** the logic doesn't check `mergedParams.successUrl` before using `org.success_url`, so explicit `successUrl` parameters passed to the function are ignored

```suggestion
				successUrl:
					mergedParams.successUrl ||
					(org?.success_url && !isOrgLoading && !orgError
						? org.success_url
						: env === AppEnv.Sandbox
							? `${import.meta.env.VITE_FRONTEND_URL}${redirectUrl}`
							: undefined),
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: vite/src/components/forms/attach-product/use-attach-body-builder.ts
Line: 99:107

Comment:
**logic:** the logic doesn't check `mergedParams.successUrl` before using `org.success_url`, so explicit `successUrl` parameters passed to the function are ignored

```suggestion
				successUrl:
					mergedParams.successUrl ||
					(org?.success_url && !isOrgLoading && !orgError
						? org.success_url
						: env === AppEnv.Sandbox
							? `${import.meta.env.VITE_FRONTEND_URL}${redirectUrl}`
							: undefined),
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/useautumn/autumn/pull/613#discussion_r2716521773,,2026-01-22T11:32:38+00:00,logic,medium,"Greptile correctly identified that the PR adds a `successUrl` parameter to the interface but never uses `mergedParams.successUrl` in the logic - instead it goes straight to checking `org?.success_url`. This means any explicit `successUrl` passed to the function will be completely ignored, which contradicts the PR's stated purpose of 'attach on dashboard respects success url'. This is a real functional bug that's easy to miss.",2026-01-22T12:26:40.689739+00:00
posthog/posthog,45693,feat(ph-ai): edit survey,https://github.com/PostHog/posthog/pull/45693,,"**logic:** These two lines are leftover from the removed `test_arun_impl_handles_llm_failure` test and don't belong in `test_edit_survey_stop_and_archive`. This test is for editing surveys, not analyzing them, so checking for `analysis_failed` error doesn't make sense here.

```suggestion
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/surveys/backend/test_max_tools.py
Line: 1086:1087

Comment:
**logic:** These two lines are leftover from the removed `test_arun_impl_handles_llm_failure` test and don't belong in `test_edit_survey_stop_and_archive`. This test is for editing surveys, not analyzing them, so checking for `analysis_failed` error doesn't make sense here.

```suggestion
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45693#discussion_r2717076137,,2026-01-22T14:09:00+00:00,logic,low,"Greptile correctly identified that the last two lines (checking for 'analysis_failed' error) are leftover from a deleted test `test_arun_impl_handles_llm_failure` that was removed in this diff. These lines make no sense in `test_edit_survey_stop_and_archive` which tests survey editing, not analysis. This is a real code artifact that would cause test confusion and potentially mask real failures.",2026-01-22T14:27:08.924616+00:00
posthog/posthog,45687,feat(data-imports): Add warehouse pipeline load consumer service (pipelineV3),https://github.com/PostHog/posthog/pull/45687,,"**logic:** Silent failure on Redis unavailability leads to potential data duplication. When Redis is unavailable, the function returns `False`, causing all batches to be reprocessed. This violates the idempotency guarantee and could lead to duplicate data in Delta Lake.

```suggestion
@contextmanager
def _get_redis_client():
    """"""Get a Redis client for the data warehouse Redis instance.""""""
    if not settings.DATA_WAREHOUSE_REDIS_HOST or not settings.DATA_WAREHOUSE_REDIS_PORT:
        raise Exception(
            ""Missing env vars for warehouse pipelines: DATA_WAREHOUSE_REDIS_HOST or DATA_WAREHOUSE_REDIS_PORT""
        )

    redis_client = get_client(f""redis://{settings.DATA_WAREHOUSE_REDIS_HOST}:{settings.DATA_WAREHOUSE_REDIS_PORT}/"")
    
    try:
        redis_client.ping()
    except Exception as e:
        logger.exception(""redis_connection_failed"")
        capture_exception(e)
        raise Exception(""Redis unavailable - cannot guarantee idempotency"") from e

    try:
        yield redis_client
    finally:
        pass
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/data_imports/pipelines/pipeline_v3/load/idempotency.py
Line: 16:31

Comment:
**logic:** Silent failure on Redis unavailability leads to potential data duplication. When Redis is unavailable, the function returns `False`, causing all batches to be reprocessed. This violates the idempotency guarantee and could lead to duplicate data in Delta Lake.

```suggestion
@contextmanager
def _get_redis_client():
    """"""Get a Redis client for the data warehouse Redis instance.""""""
    if not settings.DATA_WAREHOUSE_REDIS_HOST or not settings.DATA_WAREHOUSE_REDIS_PORT:
        raise Exception(
            ""Missing env vars for warehouse pipelines: DATA_WAREHOUSE_REDIS_HOST or DATA_WAREHOUSE_REDIS_PORT""
        )

    redis_client = get_client(f""redis://{settings.DATA_WAREHOUSE_REDIS_HOST}:{settings.DATA_WAREHOUSE_REDIS_PORT}/"")
    
    try:
        redis_client.ping()
    except Exception as e:
        logger.exception(""redis_connection_failed"")
        capture_exception(e)
        raise Exception(""Redis unavailable - cannot guarantee idempotency"") from e

    try:
        yield redis_client
    finally:
        pass
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45687#discussion_r2716812174,,2026-01-22T12:58:24+00:00,data_integrity,high,"Greptile correctly identifies that when Redis connection fails, the exception is caught and swallowed (lines 30-31), yielding `redis_client = None`. Then in `is_batch_already_processed()` (line 46), when `redis_client is None`, it returns `False`, which means batches will be reprocessed even if they were already processed - violating the idempotency guarantee and causing data duplication. The TODO comment on line 46 even acknowledges this design decision is questionable.",2026-01-22T14:27:08.924702+00:00
posthog/posthog,45687,feat(data-imports): Add warehouse pipeline load consumer service (pipelineV3),https://github.com/PostHog/posthog/pull/45687,,"**logic:** First batch overwrites table only when `not is_resume`, but if a previous run failed after writing some batches and Redis lost the idempotency keys (after TTL expiry), reprocessing would skip overwriting and append to corrupted data.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/data_imports/pipelines/pipeline_v3/load/processor.py
Line: 209:211

Comment:
**logic:** First batch overwrites table only when `not is_resume`, but if a previous run failed after writing some batches and Redis lost the idempotency keys (after TTL expiry), reprocessing would skip overwriting and append to corrupted data.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45687#discussion_r2716812676,,2026-01-22T12:58:34+00:00,data_integrity,high,"Greptile correctly identifies a real edge case: if a previous run failed after writing batches and Redis idempotency keys expire (TTL), reprocessing batch_index=0 with is_resume=False would overwrite, but if is_resume=True persists incorrectly or batch_index > 0 processes first after key expiry, it would append to potentially corrupted state. This is a subtle data integrity issue in the idempotency/recovery logic that could lead to corrupted data warehouse tables.",2026-01-22T14:27:08.924808+00:00
twentyhq/twenty,17310,Refactor workspace migration runner exception handling,https://github.com/twentyhq/twenty/pull/17310,,"**logic:** `actions.reverse()` mutates the original array. If the rollback fails partway through, retrying could execute actions in wrong order.

```suggestion
      const invertedActions = [...actions].reverse();
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/workspace-manager/workspace-migration/workspace-migration-runner/services/workspace-migration-runner.service.ts
Line: 235:235

Comment:
**logic:** `actions.reverse()` mutates the original array. If the rollback fails partway through, retrying could execute actions in wrong order.

```suggestion
      const invertedActions = [...actions].reverse();
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17310#discussion_r2716843628,,2026-01-22T13:07:19+00:00,logic,medium,"Greptile correctly identified that `actions.reverse()` mutates the original array. The diff shows the fix was indeed applied (line 235 shows `const invertedActions = [...actions].reverse();`), confirming this was a real bug. If a retry occurred after a partial rollback failure, the mutated array would be in the wrong order, potentially causing data integrity issues during workspace migrations.",2026-01-22T14:27:08.924842+00:00
mellanox/libxlio,554,[skip CI][CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,**logic:** Missing the actual step that triggers the main CI job with `do_chaos=true`. The chaos launcher should have a step that calls `build job: env.chaos_ci_job` before checking results. Currently this only checks results but never triggers the chaos test run,https://github.com/Mellanox/libxlio/pull/554#discussion_r2717050202,,2026-01-22T14:01:30+00:00,logic,high,"Greptile correctly identifies that this CI job only has a 'Check Results and Send Email' step that downloads and analyzes results from a hardcoded build URL (2688), but never actually triggers the chaos test run itself. The job references `env.chaos_ci_job` but never uses it to launch a build. This is a critical logic bug that would cause the chaos testing pipeline to never actually run tests.",2026-01-22T14:27:08.924871+00:00
mellanox/libxlio,554,[skip CI][CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,"**logic:** Hardcoded build number `2688` should use a variable like `${env.BUILD_NUMBER}` or be retrieved dynamically from the triggered CI job.

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>",https://github.com/Mellanox/libxlio/pull/554#discussion_r2717064065,,2026-01-22T14:05:27+00:00,logic,high,"Greptile correctly identified that the URL contains a hardcoded build number '2688' which would always fetch artifacts from that specific build instead of the current build. This is clearly a bug that would cause the chaos test to always check results from an old build rather than the current one, making the entire test meaningless. This is non-obvious because it's buried in a long URL string and could easily be overlooked.",2026-01-22T14:27:08.924927+00:00
axelarnetwork/axelarscan-api,328,"Automate deployments, use secrets for Indexer credentials",https://github.com/axelarnetwork/axelarscan-api/pull/328,,"**logic:** `AWS_REGION` defaults to `us-east-2` if the environment variable is not set. However, Lambda is deployed to multiple regions via Terraform. If Lambda runs in a different region (e.g., eu-west-1), the Secrets Manager SDK will attempt to fetch secrets from the wrong region, causing GetSecretValue to fail.

Since Terraform passes `AWS_REGION` as an environment variable (see Terraform `main.tf` line 102-103), remove the hardcoded fallback to ensure the Lambda's actual region is always used.

```suggestion
        const client = new SecretsManagerClient({
          region: process.env.AWS_REGION,
        });
```

If `AWS_REGION` is truly missing in some cases, that's a separate Terraform configuration issue that should be fixed at the source.",https://github.com/axelarnetwork/axelarscan-api/pull/328#discussion_r2716996694,,2026-01-22T13:48:18+00:00,logic,high,"Greptile correctly identifies a multi-region deployment bug: when Lambda runs in eu-west-1 but AWS_REGION isn't set, the hardcoded 'us-east-2' fallback would cause secrets retrieval to fail since secrets are stored regionally. This is a real operational bug that's non-obvious and requires understanding the deployment architecture.",2026-01-22T14:27:08.924951+00:00
axelarnetwork/axelarscan-api,328,"Automate deployments, use secrets for Indexer credentials",https://github.com/axelarnetwork/axelarscan-api/pull/328,,"**logic:** `AWS_REGION` defaults to `us-east-2`, but Lambda is deployed to multiple regions via Terraform. If Lambda runs in a different region (e.g., `eu-west-1`), the Secrets Manager SDK will attempt to fetch secrets from the wrong region, causing `GetSecretValue` to fail.

Since Terraform passes `AWS_REGION` as an environment variable, remove the hardcoded fallback to ensure the Lambda's actual region is always used. If `AWS_REGION` is not set, that's a Terraform configuration issue that should be fixed at the source.

```suggestion
        const client = new SecretsManagerClient({
          region: process.env.AWS_REGION,
        });
```",https://github.com/axelarnetwork/axelarscan-api/pull/328#discussion_r2717026670,,2026-01-22T13:55:43+00:00,logic,high,"Greptile correctly identifies that hardcoding 'us-east-2' as a fallback region is problematic for multi-region Lambda deployments. If AWS_REGION is not set (which shouldn't happen in Lambda but could in edge cases) or if there's any configuration issue, the SDK would attempt to fetch secrets from us-east-2 instead of the Lambda's actual region, causing silent failures. This is a subtle deployment bug that could cause production issues in non-us-east-2 regions.",2026-01-22T14:27:08.924981+00:00
mellanox/libxlio,554,[skip CI][CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,"**logic:** Regex `(\w+)` only captures word characters, missing step names with spaces or special characters like ""Compiler (clang-9.0.1)"", ""Test (Worker thread mode)"", ""Valgrind (R2C)"". These steps will be excluded from expected failures even if configured in `chaos_config.yaml`.

```suggestion
      def expected_failures = (readFile('.ci/chaos/chaos_config.yaml') =~ /- step: (.+)/).collect { it[1].trim() } as Set
```",https://github.com/Mellanox/libxlio/pull/554#discussion_r2717230129,,2026-01-22T14:48:40+00:00,logic,medium,"Greptile correctly identifies that the original regex `(\w+)` would only match word characters (alphanumeric and underscore), which would fail to capture step names containing spaces, parentheses, or other special characters like 'Compiler (clang-9.0.1)' or 'Test (Worker thread mode)'. The suggested fix using `(.+)` would correctly capture the entire step name. This is a non-obvious bug that would cause silent failures in the chaos testing logic by excluding legitimate expected failures from comparison.",2026-01-22T16:29:59.027677+00:00
mellanox/libxlio,554,[skip CI][CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,"**logic:** Awk pattern `substr($0, index($0,$3))==step` may not correctly match multi-word step names. The pattern extracts from `$3` onward but if step names contain spaces like ""Compiler (clang-9.0.1)"", this comparison may fail to match correctly.

```suggestion
    patches=$(awk -v step=""$step_name"" '/- file:/ {file=$3} /- step:/ { sub(/^[[:space:]]*- step:[[:space:]]*/, """"); if ($0 == step) print file }' ""${WORKSPACE}/.ci/chaos/chaos_config.yaml"")
```

 Have you tested this with step names containing parentheses or multiple words to verify the awk pattern matches correctly?",https://github.com/Mellanox/libxlio/pull/554#discussion_r2717230189,,2026-01-22T14:48:40+00:00,logic,medium,"Greptile correctly identifies that the awk pattern `substr($0, index($0,$3))==step` will fail for step names with spaces or special characters like 'Compiler (clang-9.0.1)' because `$3` only captures the first word after 'step:'. The suggested fix using `sub()` to strip the prefix and compare the full remaining line is a valid solution for this edge case that could cause chaos patches to silently fail to apply.",2026-01-22T16:29:59.027744+00:00
mellanox/libxlio,554,[skip CI][CI] issue: HPCINFRA-4019 Add CI chaos testing,https://github.com/Mellanox/libxlio/pull/554,,**logic:** Hardcoded build number `2689` should be dynamic. This will always download artifacts from the same old build instead of the actual chaos test run. Need to retrieve the build number from the triggered job or use `${env.BUILD_NUMBER}`,https://github.com/Mellanox/libxlio/pull/554#discussion_r2717356951,,2026-01-22T15:17:37+00:00,logic,high,"Greptile correctly identified that the URL on line 22 has a hardcoded build number '2689' instead of using a dynamic variable like ${env.BUILD_NUMBER}. This means every chaos test run will always download artifacts from build 2689 rather than the current build's artifacts, completely breaking the intended functionality of checking the actual chaos test results.",2026-01-22T16:29:59.027790+00:00
elephant-xyz/counties-trasform-scripts,666,Update nassau transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666,,"**logic:** `first_name` and `last_name` can be null after validation, but schema requires non-null strings. The removed validation logic prevented creating person objects with null names, but now persons are always created even when names are invalid.

```suggestion
      const person = {
        source_http_request: {
          method: ""GET"",
          url: seed.source_http_request.url
        },
        request_identifier: parcelIdentifier || seed.parcel_id || """",
        birth_date: null,
        first_name: firstName || """",
        last_name: lastName || """",
        middle_name: middleName,
        prefix_name: parsed.prefix ? validatePrefix(parsed.prefix) : null,
        suffix_name: parsed.suffix ? validateSuffix(parsed.suffix) : null,
        us_citizenship_status: null,
        veteran_status: null
      };
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nassau/scripts/data_extractor.js
Line: 2254:2268

Comment:
**logic:** `first_name` and `last_name` can be null after validation, but schema requires non-null strings. The removed validation logic prevented creating person objects with null names, but now persons are always created even when names are invalid.

```suggestion
      const person = {
        source_http_request: {
          method: ""GET"",
          url: seed.source_http_request.url
        },
        request_identifier: parcelIdentifier || seed.parcel_id || """",
        birth_date: null,
        first_name: firstName || """",
        last_name: lastName || """",
        middle_name: middleName,
        prefix_name: parsed.prefix ? validatePrefix(parsed.prefix) : null,
        suffix_name: parsed.suffix ? validateSuffix(parsed.suffix) : null,
        us_citizenship_status: null,
        veteran_status: null
      };
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666#discussion_r2717311781,,2026-01-22T15:06:17+00:00,data_integrity,high,"Greptile correctly identifies that the refactored `validatePersonName` function now returns the original value (including null) instead of validating and rejecting invalid names. The old code had guards like `if (firstName && lastName && typeof firstName === 'string'...)` that prevented creating person objects with null names, but these were removed. Now persons are created with potentially null `first_name`/`last_name` fields, which would violate a schema requiring non-null strings and cause data integrity issues.",2026-01-22T16:29:59.027810+00:00
elephant-xyz/counties-trasform-scripts,666,Update nassau transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666,,"**logic:** `validatePersonName` no longer validates `first_name`, `last_name`, or `middle_name` against the name pattern, only logs warnings and returns the value unchanged (even if null/empty). This allows invalid names to pass through unchecked.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nassau/scripts/data_extractor.js
Line: 1499:1514

Comment:
**logic:** `validatePersonName` no longer validates `first_name`, `last_name`, or `middle_name` against the name pattern, only logs warnings and returns the value unchanged (even if null/empty). This allows invalid names to pass through unchecked.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666#discussion_r2717311815,,2026-01-22T15:06:18+00:00,logic,medium,"Greptile correctly identifies that the new `validatePersonName` function explicitly skips pattern validation for 'first_name', 'last_name', and 'middle_name' (line 1508-1511), only logging warnings for null/non-string values while returning invalid data unchanged. Combined with removal of downstream validation that previously rejected invalid names (lines 2287-2290 removed), this allows malformed or empty names to be written to output files, breaking data integrity guarantees.",2026-01-22T16:29:59.027855+00:00
elephant-xyz/counties-trasform-scripts,666,Update nassau transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666,,"**logic:** simplified `formatName` may produce names that don't match schema pattern. The previous version enforced `^[A-Z][a-z]*([ \-',.][A-Za-z][a-z]*)*$` but new version does basic title-casing which could create invalid formats like ""J.R."" becoming ""J R"" or ""O'Brien"" potentially having issues with special character handling.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nassau/scripts/data_extractor.js
Line: 1516:1522

Comment:
**logic:** simplified `formatName` may produce names that don't match schema pattern. The previous version enforced `^[A-Z][a-z]*([ \-',.][A-Za-z][a-z]*)*$` but new version does basic title-casing which could create invalid formats like ""J.R."" becoming ""J R"" or ""O'Brien"" potentially having issues with special character handling.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666#discussion_r2717311849,,2026-01-22T15:06:18+00:00,data_integrity,medium,"Greptile correctly identifies that the new simplified formatName function uses basic title-casing (`.replace(/\b([a-z])/g, (_, ch) => ch.toUpperCase())`) without enforcing the schema pattern. The old version had explicit validation against the pattern and would return null for invalid formats, while the new version could produce names like 'J R' from 'J.R.' due to the sanitization step that replaces '. ' with ' '. This could cause schema validation failures downstream.",2026-01-22T16:29:59.027882+00:00
elephant-xyz/counties-trasform-scripts,666,Update nassau transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666,,"**logic:** removed filtering of legal designations (L/E, JT/RS, JTWROS, etc.) and parenthetical content like (GUARDIAN), (TRUSTEE). This could result in creating person/company objects with invalid names containing these suffixes.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nassau/scripts/data_extractor.js
Line: 1584:1598

Comment:
**logic:** removed filtering of legal designations (L/E, JT/RS, JTWROS, etc.) and parenthetical content like (GUARDIAN), (TRUSTEE). This could result in creating person/company objects with invalid names containing these suffixes.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666#discussion_r2717311924,,2026-01-22T15:06:19+00:00,data_integrity,medium,"Greptile correctly identifies that the diff removes important name sanitization logic that stripped legal designations (L/E, JT/RS, JTWROS, etc.) and parenthetical content like (GUARDIAN), (TRUSTEE) from owner names. The removed code (lines 1607-1620 in the old version) cleaned these suffixes before creating person/company objects. Without this filtering, names like 'JOHN DOE L/E' or 'JANE SMITH (TRUSTEE)' will be stored as-is, creating invalid person records with legal terminology embedded in the names.",2026-01-22T16:29:59.027951+00:00
elephant-xyz/counties-trasform-scripts,666,Update nassau transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666,,"**logic:** removed check for company names before splitting by `&`, so company names like ""E3 LAND & MINERALS LLC"" will be incorrectly split into ""E3 LAND"" and ""MINERALS LLC"" as separate owners.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nassau/scripts/data_extractor.js
Line: 1584:1590

Comment:
**logic:** removed check for company names before splitting by `&`, so company names like ""E3 LAND & MINERALS LLC"" will be incorrectly split into ""E3 LAND"" and ""MINERALS LLC"" as separate owners.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666#discussion_r2717311977,,2026-01-22T15:06:20+00:00,logic,medium,"Greptile correctly identified that the code removed the check `if (companyIndicators.test(cleanName))` which previously prevented splitting company names containing '&' (like 'E3 LAND & MINERALS LLC'). The new code unconditionally splits on '&', which would incorrectly create two separate owners from a single company name. This is a real data integrity bug that would be non-obvious to catch in review.",2026-01-22T16:29:59.027983+00:00
elephant-xyz/counties-trasform-scripts,666,Update nassau transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666,,"**logic:** removed government entity keywords (STATE, COUNTY, CITY, GOVERNMENT, FEDERAL, PUBLIC, MUNICIPAL, DISTRICT, AUTHORITY, COMMISSION, AGENCY, DEPARTMENT, BOARD, LANDS) from company detection, so government-owned parcels will be misclassified as person names.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nassau/scripts/data_extractor.js
Line: 1584:1584

Comment:
**logic:** removed government entity keywords (STATE, COUNTY, CITY, GOVERNMENT, FEDERAL, PUBLIC, MUNICIPAL, DISTRICT, AUTHORITY, COMMISSION, AGENCY, DEPARTMENT, BOARD, LANDS) from company detection, so government-owned parcels will be misclassified as person names.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666#discussion_r2717312005,,2026-01-22T15:06:21+00:00,logic,medium,"Greptile correctly identified that the companyIndicators regex was changed from including government-related keywords (STATE, COUNTY, CITY, GOVERNMENT, FEDERAL, PUBLIC, MUNICIPAL, DISTRICT, AUTHORITY, COMMISSION, AGENCY, DEPARTMENT, BOARD, LANDS) to excluding them. This means government-owned properties like 'CITY OF NASSAU' or 'STATE OF NEW YORK' will now be incorrectly parsed as person names instead of being recognized as companies/organizations, causing data integrity issues.",2026-01-22T16:29:59.028010+00:00
elephant-xyz/counties-trasform-scripts,666,Update nassau transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666,,"**logic:** removed validation for single-letter names and special parsing logic for 2-token uppercase names. This could result in parsing ""MARY A"" as a complete name when it's likely ""FIRST MIDDLE"" without a last name.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nassau/scripts/ownerMapping.js
Line: 197:214

Comment:
**logic:** removed validation for single-letter names and special parsing logic for 2-token uppercase names. This could result in parsing ""MARY A"" as a complete name when it's likely ""FIRST MIDDLE"" without a last name.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666#discussion_r2717312177,,2026-01-22T15:06:24+00:00,logic,medium,"Greptile correctly identifies that the removed validation logic (lines 203-225 in old code) prevented parsing 2-token uppercase names like 'MARY A' as valid person names when they're likely incomplete (FIRST MIDDLE without last name). The new simplified code at lines 197-203 would incorrectly parse 'MARY A' as last='MARY', first='A', producing invalid person records with single-letter first names.",2026-01-22T16:29:59.028039+00:00
elephant-xyz/counties-trasform-scripts,666,Update nassau transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666,,"**logic:** changed current owner extraction to use `.text()` instead of parsing HTML with `<br>` splits. This could miss multiple owners on the same line that were separated by `<br>` tags.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nassau/scripts/ownerMapping.js
Line: 247:260

Comment:
**logic:** changed current owner extraction to use `.text()` instead of parsing HTML with `<br>` splits. This could miss multiple owners on the same line that were separated by `<br>` tags.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/666#discussion_r2717312215,,2026-01-22T15:06:24+00:00,logic,medium,"Greptile correctly identified that the old code split HTML by `<br>` tags to extract multiple owner lines separately, while the new code uses `.text()` which collapses all text into a single string. This means if multiple owners are listed with `<br>` separators in the HTML, they will now be concatenated into one string rather than being processed as separate owners, potentially causing data loss or incorrect owner parsing.",2026-01-22T16:29:59.028064+00:00
elephant-xyz/counties-trasform-scripts,692,Update osceola transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/692,,"**syntax:** Typo: 'MANSFORD/GAMBLE' should likely be 'MANSARD/GAMBREL'

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: osceola/scripts/structureMapping.js
Line: 41:41

Comment:
**syntax:** Typo: 'MANSFORD/GAMBLE' should likely be 'MANSARD/GAMBREL'

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/692#discussion_r2717304849,,2026-01-22T15:04:31+00:00,data_integrity,medium,"Greptile correctly identified a typo in the mapping key 'MANSFORD/GAMBLE' which should be 'MANSARD/GAMBREL' - these are standard architectural terms for roof designs. This would cause data transformation failures when the source data contains the correct spelling, leading to incorrect null values being returned instead of the proper mapping.",2026-01-22T16:29:59.028088+00:00
elephant-xyz/counties-trasform-scripts,690,Update highlands transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/690,,"**logic:** This logic always sets exterior2 to null regardless of the exterior2Raw value. The condition checks if exterior2Raw contains 'none' but both branches return null.

```suggestion
  const exterior2 = /none/i.test(exterior2Raw) ? null : mapExteriorWallMaterial(exterior2Raw);
```

 Should exterior2 map to a specific material when exterior2Raw doesn't contain 'none'?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: highlands/scripts/structureMapping.js
Line: 147:148

Comment:
**logic:** This logic always sets exterior2 to null regardless of the exterior2Raw value. The condition checks if exterior2Raw contains 'none' but both branches return null.

```suggestion
  const exterior2 = /none/i.test(exterior2Raw) ? null : mapExteriorWallMaterial(exterior2Raw);
```

 Should exterior2 map to a specific material when exterior2Raw doesn't contain 'none'?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/690#discussion_r2717304460,,2026-01-22T15:04:26+00:00,logic,medium,"Greptile correctly identified that line 148 `const exterior2 = /none/i.test(exterior2Raw) ? null : null;` always evaluates to null regardless of the condition result - both the truthy and falsy branches return null. This is clearly a logic bug where the developer likely meant to map the value when it's not 'none', causing data loss for exterior wall secondary materials.",2026-01-22T16:29:59.028118+00:00
elephant-xyz/counties-trasform-scripts,690,Update highlands transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/690,,"**logic:** The ampersand handling assumes `&` indicates joint ownership but treats the entire remainder after the first token as a single last name, which may not work correctly for names like 'John & Jane Smith'. How should joint ownership cases like 'John Smith & Jane Doe' be handled?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: highlands/scripts/ownerMapping.js
Line: 193:211

Comment:
**logic:** The ampersand handling assumes `&` indicates joint ownership but treats the entire remainder after the first token as a single last name, which may not work correctly for names like 'John & Jane Smith'. How should joint ownership cases like 'John Smith & Jane Doe' be handled?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/690#discussion_r2717304600,,2026-01-22T15:04:28+00:00,logic,medium,"Greptile correctly identifies a logic flaw in the ampersand handling code. When processing 'John & Jane Smith', the code replaces '&' with space to get 'John Jane Smith', then takes parts[0]='John' as first_name and parts.slice(1).join(' ')='Jane Smith' as last_name - treating 'Jane Smith' as a single last name rather than recognizing Jane as a separate person or middle name. This would produce incorrect owner records for joint ownership scenarios which are common in property records.",2026-01-22T16:29:59.028148+00:00
elephant-xyz/counties-trasform-scripts,682,Update hendry transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/682,,"**logic:** Undefined variables assigned but never returned or used

```suggestion
  if (cooling_system_type === ""CentralAir"") {
    return {
      cooling_system_type,
      heating_system_type,
      hvac_system_configuration: ""SplitSystem"",
      hvac_equipment_component: ""CondenserAndAirHandler"",
      hvac_condensing_unit_present: ""Yes""
    };
  }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hendry/scripts/utilityMapping.js
Line: 93:97

Comment:
**logic:** Undefined variables assigned but never returned or used

```suggestion
  if (cooling_system_type === ""CentralAir"") {
    return {
      cooling_system_type,
      heating_system_type,
      hvac_system_configuration: ""SplitSystem"",
      hvac_equipment_component: ""CondenserAndAirHandler"",
      hvac_condensing_unit_present: ""Yes""
    };
  }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/682#discussion_r2717302811,,2026-01-22T15:04:04+00:00,logic,medium,"Greptile correctly identified that in the `inferHVAC` function, `hvac_system_configuration`, `hvac_equipment_component`, and `hvac_condensing_unit_present` are assigned without `let`/`const`/`var` declarations (making them implicit globals) and are never returned or used. The function only returns `cooling_system_type` and `heating_system_type`, so the HVAC configuration data is lost. This is a real logic bug that causes data loss and would be easy to miss.",2026-01-22T16:29:59.028176+00:00
elephant-xyz/counties-trasform-scripts,682,Update hendry transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/682,,"**syntax:** `street_number_candidate` not declared with const/let/var

```suggestion
    const street_number_candidate = parts[0];
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hendry/scripts/data_extractor.js
Line: 1624:1624

Comment:
**syntax:** `street_number_candidate` not declared with const/let/var

```suggestion
    const street_number_candidate = parts[0];
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/682#discussion_r2717303916,,2026-01-22T15:04:18+00:00,runtime,medium,"Greptile correctly identified that `street_number_candidate` on line 1624 is used without being declared with `const`, `let`, or `var`. In strict mode this would cause a ReferenceError, and in non-strict mode it creates an implicit global variable - both are bugs. This is a non-obvious issue that could easily be missed during review.",2026-01-22T16:29:59.028205+00:00
elephant-xyz/counties-trasform-scripts,682,Update hendry transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/682,,"**syntax:** `suffix_candidate` not declared with const/let/var

```suggestion
    const suffix_candidate = parts[parts.length - 1];
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hendry/scripts/data_extractor.js
Line: 1631:1631

Comment:
**syntax:** `suffix_candidate` not declared with const/let/var

```suggestion
    const suffix_candidate = parts[parts.length - 1];
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/682#discussion_r2717303970,,2026-01-22T15:04:19+00:00,runtime,medium,"Greptile correctly identified that `suffix_candidate` is used without being declared with const/let/var on line 1631. In non-strict mode this creates a global variable, and in strict mode this would throw a ReferenceError. This is a real bug that could cause unexpected behavior or crashes, and is easy to miss during review as it's a subtle syntax issue.",2026-01-22T16:29:59.028226+00:00
elephant-xyz/counties-trasform-scripts,702,Update lafayette transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/702,,"**logic:** URL should reference Lafayette County domain, not Bradford County

```suggestion
    url: propertySeed?.source_http_request?.url || ""https://www.lafayettepa.com/gis"",
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: lafayette/scripts/data_extractor.js
Line: 993:993

Comment:
**logic:** URL should reference Lafayette County domain, not Bradford County

```suggestion
    url: propertySeed?.source_http_request?.url || ""https://www.lafayettepa.com/gis"",
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/702#discussion_r2717302419,,2026-01-22T15:03:59+00:00,data_integrity,medium,"Greptile correctly identified that the code at line 993 uses 'https://www.bradfordappraiser.com/gis' as a fallback URL, but this is a Lafayette County transform script, so it should reference 'https://www.lafayettepa.com/gis'. This is a copy-paste error that would cause incorrect source attribution in the output data. This is non-obvious as it requires understanding the context (the file path indicates Lafayette County scripts) and noticing the hardcoded Bradford County URL.",2026-01-22T16:29:59.028248+00:00
elephant-xyz/counties-trasform-scripts,702,Update lafayette transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/702,,"**logic:** URL should reference Lafayette County domain, not Bradford County

```suggestion
      propertySeed?.entry_http_request?.url ||
      ""https://www.lafayettepa.com/gis"",
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: lafayette/scripts/data_extractor.js
Line: 1002:1003

Comment:
**logic:** URL should reference Lafayette County domain, not Bradford County

```suggestion
      propertySeed?.entry_http_request?.url ||
      ""https://www.lafayettepa.com/gis"",
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/702#discussion_r2717302465,,2026-01-22T15:04:00+00:00,logic,medium,"The PR is titled 'Update lafayette transform scripts' and the file is in the lafayette directory, but line 1003 incorrectly references 'https://www.bradfordappraiser.com/gis' instead of 'https://www.lafayettepa.com/gis'. This is a copy-paste error that would cause incorrect source attribution for Lafayette County data, and is non-obvious since it's buried in a fallback URL path.",2026-01-22T16:29:59.028282+00:00
elephant-xyz/counties-trasform-scripts,709,Update citrus transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/709,,"**logic:** `parseInt` with radix 10 can return `NaN` if the string contains only non-numeric characters after replacement, but the ternary operator doesn't check for `NaN`. This means `NaN` values could be assigned instead of `null`.

```suggestion
    lot_length_feet: (() => {
      if (!frontage) return null;
      const parsed = parseInt(String(frontage).replace(/[^0-9.]/g, """"), 10);
      return isNaN(parsed) ? null : parsed;
    })(),
    lot_width_feet: (() => {
      if (!depth) return null;
      const parsed = parseInt(String(depth).replace(/[^0-9.]/g, """"), 10);
      return isNaN(parsed) ? null : parsed;
    })(),
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: citrus/scripts/data_extractor.js
Line: 1373:1378

Comment:
**logic:** `parseInt` with radix 10 can return `NaN` if the string contains only non-numeric characters after replacement, but the ternary operator doesn't check for `NaN`. This means `NaN` values could be assigned instead of `null`.

```suggestion
    lot_length_feet: (() => {
      if (!frontage) return null;
      const parsed = parseInt(String(frontage).replace(/[^0-9.]/g, """"), 10);
      return isNaN(parsed) ? null : parsed;
    })(),
    lot_width_feet: (() => {
      if (!depth) return null;
      const parsed = parseInt(String(depth).replace(/[^0-9.]/g, """"), 10);
      return isNaN(parsed) ? null : parsed;
    })(),
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/709#discussion_r2717301035,,2026-01-22T15:03:42+00:00,data_integrity,medium,"Greptile correctly identifies that the new simplified code removes the NaN check that existed in the original version. The old code had `return parsed >= 1 ? parsed : null` which handled NaN (since NaN >= 1 is false), but the new code `frontage ? parseInt(...) : null` only checks if frontage is truthy, not if the parsed result is valid. If frontage contains only non-numeric characters like 'N/A', parseInt would return NaN which would be stored instead of null, causing data integrity issues.",2026-01-22T16:29:59.028305+00:00
elephant-xyz/counties-trasform-scripts,663,Update columbia transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/663,,"**logic:** inconsistency with `ownerMapping.js` - this no longer strips dots before lookup, but `ownerMapping.js:129` still does. inputs like ""Mr."" won't match here but will in ownerMapping

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: columbia/scripts/data_extractor.js
Line: 915:915

Comment:
**logic:** inconsistency with `ownerMapping.js` - this no longer strips dots before lookup, but `ownerMapping.js:129` still does. inputs like ""Mr."" won't match here but will in ownerMapping

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/663#discussion_r2717298148,,2026-01-22T15:02:59+00:00,logic,medium,"Greptile correctly identified that the refactored `mapPrefixName` and `mapSuffixName` functions removed the `.replace(/\./g, """")` call that stripped dots before lookup. The old code explicitly handled inputs like ""Mr."" by stripping the dot before the lookup, but the new code using `name?.toUpperCase()` will fail to match ""Mr."" against the key ""MR"" in the prefixes object. This is a real behavioral inconsistency that would cause data processing failures.",2026-01-22T16:29:59.028343+00:00
elephant-xyz/counties-trasform-scripts,663,Update columbia transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/663,,"**logic:** inconsistency with `ownerMapping.js` - this no longer strips dots before lookup, but `ownerMapping.js:157` still does. inputs like ""Jr."" won't match here but will in ownerMapping

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: columbia/scripts/data_extractor.js
Line: 925:925

Comment:
**logic:** inconsistency with `ownerMapping.js` - this no longer strips dots before lookup, but `ownerMapping.js:157` still does. inputs like ""Jr."" won't match here but will in ownerMapping

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/663#discussion_r2717298183,,2026-01-22T15:02:59+00:00,logic,medium,"Greptile correctly identified that the old code used `name.replace(/\./g, """").toUpperCase()` to strip dots before lookup, but the new code only does `name?.toUpperCase()`. This means inputs like 'Jr.' or 'Dr.' with dots will no longer match their corresponding entries in the suffixes/prefixes objects ('JR', 'DR'), causing a behavioral regression that could lead to data inconsistency between this file and ownerMapping.js.",2026-01-22T16:29:59.028368+00:00
elephant-xyz/counties-trasform-scripts,703,Update st. lucie transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/703,,"**logic:** newly created `currentOwnerRecord` is not added to `currentOwnerRecordsList`

this prevents mailing address creation at line 2204 when owner is parsed from HTML rather than `owner_data.json`. the deleted code block (previously at ~line 2088-2117) added the record to the list, but now this step is missing.

```suggestion
    if (!currentOwnerRecord && currentOwnerName) {
      currentOwnerRecord = ensureOwnerRecordFromName(currentOwnerName);
      registerPropertyRole(currentOwnerRecord, ""current"");
      // Add to currentOwnerRecordsList to ensure relationships are created
      if (!currentOwnerRecordIds.has(currentOwnerRecord.id)) {
        currentOwnerRecordIds.add(currentOwnerRecord.id);
        currentOwnerRecordsList.push(currentOwnerRecord);
      }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: st. lucie/scripts/data_extractor.js
Line: 2195:2197

Comment:
**logic:** newly created `currentOwnerRecord` is not added to `currentOwnerRecordsList`

this prevents mailing address creation at line 2204 when owner is parsed from HTML rather than `owner_data.json`. the deleted code block (previously at ~line 2088-2117) added the record to the list, but now this step is missing.

```suggestion
    if (!currentOwnerRecord && currentOwnerName) {
      currentOwnerRecord = ensureOwnerRecordFromName(currentOwnerName);
      registerPropertyRole(currentOwnerRecord, ""current"");
      // Add to currentOwnerRecordsList to ensure relationships are created
      if (!currentOwnerRecordIds.has(currentOwnerRecord.id)) {
        currentOwnerRecordIds.add(currentOwnerRecord.id);
        currentOwnerRecordsList.push(currentOwnerRecord);
      }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/703#discussion_r2717295243,,2026-01-22T15:02:18+00:00,logic,medium,"Greptile correctly identifies that the deleted code block (lines 2088-2117 in the old code) was responsible for adding newly created currentOwnerRecord to currentOwnerRecordsList. Without this, when an owner is parsed from HTML and a new record is created, it won't be added to the list, which would prevent downstream operations like mailing address creation that depend on currentOwnerRecordsList containing all owner records.",2026-01-22T16:29:59.028392+00:00
elephant-xyz/counties-trasform-scripts,703,Update st. lucie transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/703,,"**logic:** validation is less strict than before, which could allow non-numeric truthy values

the old code used `typeof value === ""number"" && Number.isFinite(value) && value > 0`, while the new code only checks `value && value > 0`. if `assessedVal`, `justVal`, `buildingVal`, or `landVal` are non-numeric truthy values (e.g., strings, objects), they'll pass through.

```suggestion
        property_assessed_value_amount:
          typeof assessedVal === ""number"" && Number.isFinite(assessedVal) && assessedVal > 0 ? assessedVal : null,
        property_market_value_amount: 
          typeof justVal === ""number"" && Number.isFinite(justVal) && justVal > 0 ? justVal : null,
        property_building_amount:
          typeof buildingVal === ""number"" && Number.isFinite(buildingVal) && buildingVal > 0 ? buildingVal : null,
        property_land_amount: 
          typeof landVal === ""number"" && Number.isFinite(landVal) && landVal > 0 ? landVal : null,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: st. lucie/scripts/data_extractor.js
Line: 2859:2864

Comment:
**logic:** validation is less strict than before, which could allow non-numeric truthy values

the old code used `typeof value === ""number"" && Number.isFinite(value) && value > 0`, while the new code only checks `value && value > 0`. if `assessedVal`, `justVal`, `buildingVal`, or `landVal` are non-numeric truthy values (e.g., strings, objects), they'll pass through.

```suggestion
        property_assessed_value_amount:
          typeof assessedVal === ""number"" && Number.isFinite(assessedVal) && assessedVal > 0 ? assessedVal : null,
        property_market_value_amount: 
          typeof justVal === ""number"" && Number.isFinite(justVal) && justVal > 0 ? justVal : null,
        property_building_amount:
          typeof buildingVal === ""number"" && Number.isFinite(buildingVal) && buildingVal > 0 ? buildingVal : null,
        property_land_amount: 
          typeof landVal === ""number"" && Number.isFinite(landVal) && landVal > 0 ? landVal : null,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/703#discussion_r2717295317,,2026-01-22T15:02:19+00:00,data_integrity,medium,"Greptile correctly identified that the old code used strict type checking (`typeof value === ""number"" && Number.isFinite(value) && value > 0`) while the new code only uses `value && value > 0`. This regression allows non-numeric truthy values (like strings or objects) to pass through validation and be written to the output JSON, potentially causing data integrity issues in downstream processing.",2026-01-22T16:29:59.028440+00:00
elephant-xyz/counties-trasform-scripts,677,Update pasco transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/677,,"**logic:** `livable_floor_area` field has been removed from the property output despite being extracted from the HTML (lines 1759-1769). This represents data loss - living area information is valuable property data that downstream consumers may depend on. If removal is intentional per schema changes, this should be documented in commit message.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: pasco/scripts/data_extractor.js
Line: 1789:1791

Comment:
**logic:** `livable_floor_area` field has been removed from the property output despite being extracted from the HTML (lines 1759-1769). This represents data loss - living area information is valuable property data that downstream consumers may depend on. If removal is intentional per schema changes, this should be documented in commit message.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/677#discussion_r2717294494,,2026-01-22T15:02:07+00:00,data_integrity,medium,"Greptile correctly identified that `livable_floor_area` was removed from the property output object (line 1791 in the diff shows the removal) while the extraction logic for `livableSqft` (lines 1759-1769 referenced) still exists in the code. This is a data loss issue where extracted data is computed but never written to output, potentially breaking downstream consumers that depend on this field.",2026-01-22T16:29:59.028486+00:00
elephant-xyz/counties-trasform-scripts,677,Update pasco transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/677,,"**logic:** Latitude and longitude fields have been removed from the address object. While mailing_address (lines 2157-2158) retains these fields (set to null), the primary address loses them entirely. This removes geocoding data that was previously available and may break consumers that depend on these fields.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: pasco/scripts/data_extractor.js
Line: 2125:2131

Comment:
**logic:** Latitude and longitude fields have been removed from the address object. While mailing_address (lines 2157-2158) retains these fields (set to null), the primary address loses them entirely. This removes geocoding data that was previously available and may break consumers that depend on these fields.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/677#discussion_r2717294564,,2026-01-22T15:02:08+00:00,data_integrity,medium,"Greptile correctly identified that the diff removes latitude and longitude fields from the address object (lines 2115-2122 in old code vs lines 2124-2131 in new code). The old code explicitly included these fields with conditional logic from addrSeed, but the new address object completely omits them, which could break downstream consumers expecting geocoding data in the address output.",2026-01-22T16:29:59.028515+00:00
elephant-xyz/counties-trasform-scripts,671,Update suwannee transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/671,,"**logic:** Missing error handling will crash if files don't exist. Other counties (alachua, dixie, liberty) wrap these in try-catch blocks since these files are optional.

```suggestion
    // Owners/utilities/layout must be built from their JSONs
    let ownerData = {};
    let utilitiesData = {};
    let structuresData = {};
    let layoutData = {};
    try {
      ownerData = readJson(path.join(""owners"", ""owner_data.json""));
    } catch (e) {}
    try {
      utilitiesData = readJson(path.join(""owners"", ""utilities_data.json""));
    } catch (e) {}
    try {
      structuresData = readJson(path.join(""owners"", ""structure_data.json""));
    } catch (e) {}
    try {
      layoutData = readJson(path.join(""owners"", ""layout_data.json""));
    } catch (e) {}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: suwannee/scripts/data_extractor.js
Line: 1579:1582

Comment:
**logic:** Missing error handling will crash if files don't exist. Other counties (alachua, dixie, liberty) wrap these in try-catch blocks since these files are optional.

```suggestion
    // Owners/utilities/layout must be built from their JSONs
    let ownerData = {};
    let utilitiesData = {};
    let structuresData = {};
    let layoutData = {};
    try {
      ownerData = readJson(path.join(""owners"", ""owner_data.json""));
    } catch (e) {}
    try {
      utilitiesData = readJson(path.join(""owners"", ""utilities_data.json""));
    } catch (e) {}
    try {
      structuresData = readJson(path.join(""owners"", ""structure_data.json""));
    } catch (e) {}
    try {
      layoutData = readJson(path.join(""owners"", ""layout_data.json""));
    } catch (e) {}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/671#discussion_r2717292859,,2026-01-22T15:01:46+00:00,runtime,medium,"Greptile correctly identified that the new code directly calls readJson() on optional files without try-catch blocks, which will crash if files don't exist. The diff shows the old code had try-catch blocks with fs.existsSync() checks for these same files, and the new code removes all that safety. This is a real regression that would cause runtime crashes when optional owner/utilities/structures/layout files are missing.",2026-01-22T16:29:59.028542+00:00
elephant-xyz/counties-trasform-scripts,671,Update suwannee transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/671,,"**logic:** Removed conditional check that only wrote owners when sales exist. Now `writeOwners` is always called, but if `ownerData` is empty (due to missing try-catch above), this will create empty owner files.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: suwannee/scripts/data_extractor.js
Line: 1669:1674

Comment:
**logic:** Removed conditional check that only wrote owners when sales exist. Now `writeOwners` is always called, but if `ownerData` is empty (due to missing try-catch above), this will create empty owner files.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/671#discussion_r2717293051,,2026-01-22T15:01:48+00:00,logic,medium,"Greptile correctly identifies two related issues: (1) The try-catch blocks for reading owner/utility/structure/layout JSON files were removed, so if those files don't exist, readJson will throw and crash the script. (2) The conditional check `if (sales.length > 0)` before calling writeOwners was removed, meaning writeOwners is now always called even when ownerData may be empty (or undefined if readJson failed). This creates potential runtime errors and data integrity issues with empty owner files being written.",2026-01-22T16:29:59.028626+00:00
elephant-xyz/counties-trasform-scripts,701,Update franklin transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/701,,"**logic:** File name can now be empty string if both `instAbbr` and `bookPage` are falsy (previously had fallback to ""Deed Document"").

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: franklin/scripts/data_extractor.js
Line: 3176:3176

Comment:
**logic:** File name can now be empty string if both `instAbbr` and `bookPage` are falsy (previously had fallback to ""Deed Document"").

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/701#discussion_r2717292603,,2026-01-22T15:01:43+00:00,data_integrity,medium,"Greptile correctly identified that the diff removes the fallback logic (lines 3212-3217 in the old code) that ensured the file name defaulted to 'Deed Document' when both instAbbr and bookPage were falsy. Now line 3176 can produce an empty string, which could cause issues downstream when processing or displaying files with no name.",2026-01-22T16:29:59.028658+00:00
elephant-xyz/counties-trasform-scripts,678,Update miami dade transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/678,,"**logic:** Replacing the comprehensive `propertyTypeMapping` table with a simple switch statement loses critical data. The old mapping returned objects with `ownership_estate_type`, `build_status`, `structure_form`, and `property_usage_type`, but the new code only returns simple strings like `""SingleFamily""` or `""VacantLand""`.

This breaks lines 807-810 where the code tries to access `.ownership_estate_type`, `.build_status`, `.structure_form`, and `.property_usage_type` from `mappedType`, which will be undefined since `mappedType` is now just a string.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: miami dade/scripts/data_extractor.js
Line: 73:183

Comment:
**logic:** Replacing the comprehensive `propertyTypeMapping` table with a simple switch statement loses critical data. The old mapping returned objects with `ownership_estate_type`, `build_status`, `structure_form`, and `property_usage_type`, but the new code only returns simple strings like `""SingleFamily""` or `""VacantLand""`.

This breaks lines 807-810 where the code tries to access `.ownership_estate_type`, `.build_status`, `.structure_form`, and `.property_usage_type` from `mappedType`, which will be undefined since `mappedType` is now just a string.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/678#discussion_r2717292457,,2026-01-22T15:01:41+00:00,logic,critical,"Greptile correctly identified that the new `mapDorToPropertyType` function returns simple strings like 'SingleFamily' instead of objects, but the code at lines 807-810 (property object creation) still tries to access properties like `.property_type`, `.ownership_estate_type`, `.build_status`, `.structure_form`, and `.property_usage_type` from `mappedType`. This will result in undefined values being written to the property output, breaking the data transformation pipeline.",2026-01-22T16:29:59.028680+00:00
elephant-xyz/counties-trasform-scripts,678,Update miami dade transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/678,,"**logic:** All commercial, industrial, agricultural, and institutional property codes (prefixes 11-99) now return errors instead of being mapped to proper types. The old mapping handled 100+ specific use codes with detailed property classifications - this will cause transformation failures for most non-residential properties.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: miami dade/scripts/data_extractor.js
Line: 110:180

Comment:
**logic:** All commercial, industrial, agricultural, and institutional property codes (prefixes 11-99) now return errors instead of being mapped to proper types. The old mapping handled 100+ specific use codes with detailed property classifications - this will cause transformation failures for most non-residential properties.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/678#discussion_r2717292499,,2026-01-22T15:01:41+00:00,logic,high,"Greptile correctly identified that the refactored `mapDorToPropertyType` function now returns error objects for prefixes 11-49 (commercial, industrial, agricultural codes) instead of mapping them to proper property types. The old code had a comprehensive 100+ entry `propertyTypeMapping` array that handled all these codes, but the new switch statement only handles residential prefixes (00-10) and returns errors for everything else, causing transformation failures for non-residential properties.",2026-01-22T16:29:59.028715+00:00
elephant-xyz/counties-trasform-scripts,678,Update miami dade transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/678,,"**logic:** Removed middle name space removal pattern `/[^A-Za-z\-' ]/g` and replaced with `/[^A-Za-z\-']/g` (no space). This removes all spaces from middle names.

Middle names with spaces (e.g., ""Mary Ann"") will become ""MaryAnn""

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: miami dade/scripts/ownerMapping.js
Line: 135:137

Comment:
**logic:** Removed middle name space removal pattern `/[^A-Za-z\-' ]/g` and replaced with `/[^A-Za-z\-']/g` (no space). This removes all spaces from middle names.

Middle names with spaces (e.g., ""Mary Ann"") will become ""MaryAnn""

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/678#discussion_r2717292539,,2026-01-22T15:01:42+00:00,data_integrity,medium,"Greptile correctly identified that the regex change from `/[^A-Za-z\-' ]/g` to `/[^A-Za-z\-']/g` removes the space character from the allowed characters, meaning middle names with spaces like 'Mary Ann' will be concatenated to 'MaryAnn'. This is a subtle data integrity issue that could easily be missed during review.",2026-01-22T16:29:59.028744+00:00
elephant-xyz/counties-trasform-scripts,706,Update hernando transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/706,,"**syntax:** Syntax error: space between `unnorm.` and `longitude` will cause runtime error

```suggestion
    longitude: unnorm.longitude ||null,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: hernando/scripts/data_extractor.js
Line: 759:759

Comment:
**syntax:** Syntax error: space between `unnorm.` and `longitude` will cause runtime error

```suggestion
    longitude: unnorm.longitude ||null,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/706#discussion_r2717291293,,2026-01-22T15:01:23+00:00,runtime,high,"Looking at line 759, the code shows `longitude: unnorm. longitude ||null,` with an errant space between `unnorm.` and `longitude`. This is a genuine syntax error that would cause a runtime crash when JavaScript tries to parse this invalid property access. While simple, this is easy to miss in a large diff with many changes.",2026-01-22T16:29:59.028768+00:00
elephant-xyz/counties-trasform-scripts,668,Update lee transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/668,,"**syntax:** ES6 module syntax (`import`) incompatible with CommonJS used in other files and counties. This will cause runtime errors since Node.js requires either `""type"": ""module""` in `package.json` or `.mjs` extension.

```suggestion
const fs = require('fs');
const path = require('path');
const cheerio = require('cheerio');
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: lee/scripts/structureMapping.js
Line: 4:6

Comment:
**syntax:** ES6 module syntax (`import`) incompatible with CommonJS used in other files and counties. This will cause runtime errors since Node.js requires either `""type"": ""module""` in `package.json` or `.mjs` extension.

```suggestion
const fs = require('fs');
const path = require('path');
const cheerio = require('cheerio');
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/668#discussion_r2717290847,,2026-01-22T15:01:16+00:00,runtime,high,"The PR changes CommonJS require() syntax to ES6 import syntax. Without 'type': 'module' in package.json or .mjs extension, Node.js will throw 'SyntaxError: Cannot use import statement outside a module' at runtime. This is a real compatibility issue that could easily be missed during code review since the code looks syntactically correct in isolation.",2026-01-22T16:29:59.028799+00:00
elephant-xyz/counties-trasform-scripts,673,Update bradford transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/673,,"**logic:** simplified formatting loses important edge cases - no longer handles trailing periods, abbreviations (e.g., `J.R.`), or validates output format

```suggestion
function formatNameToPattern(name) {
  if (!name) return null;

  // Clean and normalize whitespace
  let cleaned = name.trim().replace(/\s+/g, ' ');

  // Remove trailing periods
  cleaned = cleaned.replace(/\.+$/, '');

  // Handle abbreviations: ensure letters after periods, hyphens, apostrophes are uppercase
  // Pattern: letter + special char + letter should be: Upper + special + Upper
  cleaned = cleaned.replace(/([A-Za-z])([.\-',])([A-Za-z])/g, (match, before, sep, after) => {
    return before.charAt(0).toUpperCase() + sep + after.charAt(0).toUpperCase();
  });

  // Split by spaces and format each word part
  const result = cleaned.split(' ').map(part => {
    // For parts with special characters (abbreviations), handle carefully
    if (/[.\-',]/.test(part)) {
      // Split by special characters and capitalize each segment
      return part.split(/([.\-',])/).map((segment, idx) => {
        // If it's a separator, keep it
        if (/[.\-',]/.test(segment)) return segment;
        // If it's a letter segment, capitalize first letter
        if (segment.length > 0) {
          return segment.charAt(0).toUpperCase() + segment.slice(1).toLowerCase();
        }
        return segment;
      }).join('');
    } else {
      // Normal word: capitalize first letter, lowercase rest
      return part.charAt(0).toUpperCase() + part.slice(1).toLowerCase();
    }
  }).join(' ');

  // Validate result matches required pattern: must start with uppercase letter
  // and only contain letters, spaces, hyphens, apostrophes, commas, periods
  if (!result || result.length === 0 || !/^[A-Z][a-zA-Z\s\-',.]*$/.test(result)) {
    return null;
  }

  return result;
}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: bradford/scripts/data_extractor.js
Line: 64:70

Comment:
**logic:** simplified formatting loses important edge cases - no longer handles trailing periods, abbreviations (e.g., `J.R.`), or validates output format

```suggestion
function formatNameToPattern(name) {
  if (!name) return null;

  // Clean and normalize whitespace
  let cleaned = name.trim().replace(/\s+/g, ' ');

  // Remove trailing periods
  cleaned = cleaned.replace(/\.+$/, '');

  // Handle abbreviations: ensure letters after periods, hyphens, apostrophes are uppercase
  // Pattern: letter + special char + letter should be: Upper + special + Upper
  cleaned = cleaned.replace(/([A-Za-z])([.\-',])([A-Za-z])/g, (match, before, sep, after) => {
    return before.charAt(0).toUpperCase() + sep + after.charAt(0).toUpperCase();
  });

  // Split by spaces and format each word part
  const result = cleaned.split(' ').map(part => {
    // For parts with special characters (abbreviations), handle carefully
    if (/[.\-',]/.test(part)) {
      // Split by special characters and capitalize each segment
      return part.split(/([.\-',])/).map((segment, idx) => {
        // If it's a separator, keep it
        if (/[.\-',]/.test(segment)) return segment;
        // If it's a letter segment, capitalize first letter
        if (segment.length > 0) {
          return segment.charAt(0).toUpperCase() + segment.slice(1).toLowerCase();
        }
        return segment;
      }).join('');
    } else {
      // Normal word: capitalize first letter, lowercase rest
      return part.charAt(0).toUpperCase() + part.slice(1).toLowerCase();
    }
  }).join(' ');

  // Validate result matches required pattern: must start with uppercase letter
  // and only contain letters, spaces, hyphens, apostrophes, commas, periods
  if (!result || result.length === 0 || !/^[A-Z][a-zA-Z\s\-',.]*$/.test(result)) {
    return null;
  }

  return result;
}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/673#discussion_r2717289971,,2026-01-22T15:01:04+00:00,logic,medium,"Greptile correctly identified that the simplified `formatNameToPattern` function removes important functionality: 1) no longer removes trailing periods, 2) no longer handles abbreviations like 'J.R.' properly (the new code would convert 'J.R.' to 'J.r.' instead of 'J.R.'), and 3) removes output validation. This is a real regression that could cause incorrect name formatting in production data.",2026-01-22T16:29:59.028827+00:00
elephant-xyz/counties-trasform-scripts,670,Update seminole transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/670,,"**logic:** These files are now required but may not exist, which will cause runtime errors if they're missing

```suggestion
  const unaddr = fs.existsSync(""unnormalized_address.json"")
    ? readJSON(""unnormalized_address.json"")
    : {};
  const propSeed = fs.existsSync(""property_seed.json"")
    ? readJSON(""property_seed.json"")
    : {};
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: seminole/scripts/data_extractor.js
Line: 2907:2908

Comment:
**logic:** These files are now required but may not exist, which will cause runtime errors if they're missing

```suggestion
  const unaddr = fs.existsSync(""unnormalized_address.json"")
    ? readJSON(""unnormalized_address.json"")
    : {};
  const propSeed = fs.existsSync(""property_seed.json"")
    ? readJSON(""property_seed.json"")
    : {};
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/670#discussion_r2717290074,,2026-01-22T15:01:06+00:00,runtime,high,"The diff shows the code changed from using `fs.existsSync()` checks with fallback to empty objects, to directly calling `readJSON()` without checking if files exist. If these files don't exist, `readJSON()` will throw an error since `fs.readFileSync` fails on missing files. This is a real regression that will cause runtime crashes in cases where these optional files aren't present.",2026-01-22T16:29:59.028898+00:00
elephant-xyz/counties-trasform-scripts,691,Update duval transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/691,,"**logic:** `titleCaseNamePart` now returns null for empty inputs, but the simplified logic removed the validation that checked if first/last names exist before creating person records. If `owner.first_name` or `owner.last_name` are null/empty after `titleCaseNamePart`, invalid person records with null required fields will be written.

```suggestion
          const firstName = titleCaseNamePart(owner.first_name);
          const lastName = titleCaseNamePart(owner.last_name);
          if (!firstName || !lastName) return;
          const payload = {
            first_name: firstName,
            middle_name: titleCaseNamePart(owner.middle_name),
            last_name: lastName,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: duval/scripts/data_extractor.js
Line: 2306:2310

Comment:
**logic:** `titleCaseNamePart` now returns null for empty inputs, but the simplified logic removed the validation that checked if first/last names exist before creating person records. If `owner.first_name` or `owner.last_name` are null/empty after `titleCaseNamePart`, invalid person records with null required fields will be written.

```suggestion
          const firstName = titleCaseNamePart(owner.first_name);
          const lastName = titleCaseNamePart(owner.last_name);
          if (!firstName || !lastName) return;
          const payload = {
            first_name: firstName,
            middle_name: titleCaseNamePart(owner.middle_name),
            last_name: lastName,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/691#discussion_r2717289787,,2026-01-22T15:01:02+00:00,data_integrity,medium,"Greptile correctly identified that the old code (lines 2304-2308 in the removed section) validated that firstName and lastName existed before creating person records, but the new code directly uses titleCaseNamePart results in the payload without validation. This could write invalid person records with null first_name or last_name fields when the owner data is missing or empty.",2026-01-22T16:29:59.028926+00:00
elephant-xyz/counties-trasform-scripts,680,Update st. johns transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/680,,"**syntax:** cannot reassign `inputCounty` declared with `const` on line 1931

```suggestion
  let inputCounty = (unnorm.county_jurisdiction || """").trim();
  if (!inputCounty) {
    inputCounty = (unnorm.county_name || """").trim();
  }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: st. johns/scripts/data_extractor.js
Line: 1931:1934

Comment:
**syntax:** cannot reassign `inputCounty` declared with `const` on line 1931

```suggestion
  let inputCounty = (unnorm.county_jurisdiction || """").trim();
  if (!inputCounty) {
    inputCounty = (unnorm.county_name || """").trim();
  }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/680#discussion_r2717288898,,2026-01-22T15:00:48+00:00,runtime,critical,"Greptile correctly identified a real JavaScript syntax error at lines 1931-1934. The code declares `inputCounty` with `const` and then tries to reassign it in the if block, which will throw a TypeError at runtime. This would crash the script when attempting to write an address.",2026-01-22T16:29:59.028974+00:00
elephant-xyz/counties-trasform-scripts,687,Update marion transform scripts,https://github.com/elephant-xyz/Counties-trasform-scripts/pull/687,,"**logic:** This will throw `TypeError: Cannot read properties of null` when `utilsData` or `structuresData` is null. These variables can be null when the corresponding files don't exist (see lines 1042-1043).

```suggestion
  const util = utilsData ? utilsData[key] : null;
  const struct = structuresData ? structuresData[key] : null;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: marion/scripts/data_extractor.js
Line: 1073:1074

Comment:
**logic:** This will throw `TypeError: Cannot read properties of null` when `utilsData` or `structuresData` is null. These variables can be null when the corresponding files don't exist (see lines 1042-1043).

```suggestion
  const util = utilsData ? utilsData[key] : null;
  const struct = structuresData ? structuresData[key] : null;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/elephant-xyz/Counties-trasform-scripts/pull/687#discussion_r2717287753,,2026-01-22T15:00:31+00:00,runtime,high,"The diff clearly shows the removal of null checks for `utilsData` and `structuresData`. Looking at the diff, the original code had `utilsData ? utilsData[key] : null` which safely handled null cases. The change removes these guards, making the code vulnerable to TypeError when these variables are null (which can happen when corresponding files don't exist). This is a real regression bug that would cause crashes and is non-obvious since it requires understanding the data flow from earlier file loading logic.",2026-01-22T16:29:59.028999+00:00
posthog/posthog,45726,chore: Clear all existing temporary tokens,https://github.com/PostHog/posthog/pull/45726,,"**syntax:** retrieve User model using apps parameter

The User model should be retrieved using apps.get_model within the function instead of importing from django.db.models at the top. This ensures the migration uses the correct historical state of the model. See other migrations like 0906 or 0929 for the correct pattern.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/migrations/0983_clear_temporary_tokens.py
Line: 9:9

Comment:
**syntax:** retrieve User model using apps parameter

The User model should be retrieved using apps.get_model within the function instead of importing from django.db.models at the top. This ensures the migration uses the correct historical state of the model. See other migrations like 0906 or 0929 for the correct pattern.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45726#discussion_r2717971537,,2026-01-22T17:47:37+00:00,data_integrity,medium,"Greptile correctly identifies that the migration imports User directly from posthog.models.user instead of using apps.get_model('posthog', 'User'). This is a well-known Django migration anti-pattern that can cause issues if the model schema changes between when the migration was written and when it runs, potentially leading to migration failures or incorrect behavior. This is non-obvious to developers unfamiliar with Django migration best practices.",2026-01-22T18:20:55.086650+00:00
posthog/posthog,45724,SUPPORT SUPERDAY - Update link to web vitals settings page,https://github.com/PostHog/posthog/pull/45724,,"**logic:** hardcoding `https://us.posthog.com` breaks self-hosted and EU cloud instances - use `uiHost` like line 76 does

```suggestion
                            <Link to={`${uiHost}${urls.settings('project', 'web-vitals-autocapture')}`}>settings page</Link> to start capturing web vitals.
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/toolbar/web-vitals/WebVitalsToolbarMenu.tsx
Line: 34:34

Comment:
**logic:** hardcoding `https://us.posthog.com` breaks self-hosted and EU cloud instances - use `uiHost` like line 76 does

```suggestion
                            <Link to={`${uiHost}${urls.settings('project', 'web-vitals-autocapture')}`}>settings page</Link> to start capturing web vitals.
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45724#discussion_r2717902287,,2026-01-22T17:27:50+00:00,logic,medium,"Greptile correctly identified that hardcoding 'us.posthog.com' would break functionality for self-hosted instances and EU cloud users. The suggestion to use `uiHost` is validated by the existing pattern elsewhere in the codebase (referenced at line 76), making this a real bug that would cause incorrect behavior for a significant portion of users.",2026-01-22T18:20:55.086730+00:00
posthog/posthog,45719,fix: gemini timeout in milliseconds not seconds,https://github.com/PostHog/posthog/pull/45719,,"**syntax:** The test in `products/llm_analytics/backend/providers/test/test_gemini.py:16` expects `TIMEOUT == 300` and needs to be updated to `300_000`

```suggestion
    TIMEOUT: int = 300_000
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/llm_analytics/backend/llm/providers/gemini.py
Line: 34:34

Comment:
**syntax:** The test in `products/llm_analytics/backend/providers/test/test_gemini.py:16` expects `TIMEOUT == 300` and needs to be updated to `300_000`

```suggestion
    TIMEOUT: int = 300_000
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45719#discussion_r2717816460,Of course I then forget to update the test 🤦 ,2026-01-22T17:07:13+00:00,logic,low,"Greptile correctly identified that a test file needs to be updated to match the new timeout value change from 300 to 300_000. The developer confirmed this was a valid catch they had overlooked with '🤦'. While this is technically test maintenance rather than a production bug, catching test-code inconsistencies before merge prevents CI failures and confusion.",2026-01-22T18:20:55.086787+00:00
infisical/infisical,5238,feat(pki): add default TTL setting to certificate profiles,https://github.com/Infisical/infisical/pull/5238,,"**logic:** Empty string fallback could cause issues downstream. If both `certificateRequest.validity.ttl` and `profile.defaultTtlDays` are missing, this passes an empty string to the CA service.

```suggestion
        ttl: certificateRequest.validity.ttl || (profile.defaultTtlDays ? `${profile.defaultTtlDays}d` : undefined),
```",https://github.com/Infisical/infisical/pull/5238#discussion_r2718048455,,2026-01-22T18:11:26+00:00,logic,medium,Greptile correctly identifies that passing an empty string to the CA service when both TTL sources are missing could cause unexpected behavior downstream. The suggestion to use `undefined` instead is more appropriate as it clearly signals 'no value' rather than an ambiguous empty string that might be interpreted differently by the signing service.,2026-01-22T18:20:55.086848+00:00
nvidia/fuser,5863,Add kernel based alltoallv and cuda backend for MoE dispatch and combine,https://github.com/NVIDIA/Fuser/pull/5863,,"**logic:** checking wrong backend constant - should check `backend` parameter, not hardcoded `kNccl`

```suggestion
  if (!communicator_->isBackendAvailable(backend)) {
    GTEST_SKIP() << ""Backend "" << backend << "" not available."";
  }
```",https://github.com/NVIDIA/Fuser/pull/5863#discussion_r2717943730,,2026-01-22T17:39:17+00:00,logic,medium,"Greptile correctly identified that the code retrieves `backend` from `GetParam()` but then checks availability of the hardcoded `CommunicatorBackend::kNccl` instead of the actual `backend` parameter. This means when the test runs with `kCuda` backend, it would incorrectly check if `kNccl` is available rather than `kCuda`, causing incorrect test skip behavior. The test is parameterized over both backends, making this a real bug.",2026-01-22T18:20:55.086890+00:00
nvidia-nemo/curator,1415,Ray Pool Executor,https://github.com/NVIDIA-NeMo/Curator/pull/1415,,"**logic:** inconsistent maturity claim - `RayActorPoolExecutor` is marked as ""Stable"" but still in `experimental` directory

The executor is located in `nemo_curator.backends.experimental.ray_actor_pool`, contradicting the ""Stable"" maturity claim in the comparison table. Has RayActorPoolExecutor been moved out of experimental, or should the maturity status be updated to reflect its experimental nature?",https://github.com/NVIDIA-NeMo/Curator/pull/1415#discussion_r2717967018,,2026-01-22T17:46:08+00:00,logic,medium,"Greptile correctly identified a documentation inconsistency - the comparison table claims RayActorPoolExecutor is 'Stable' but the code example shows the import path as `nemo_curator.backends.ray_actor_pool` (not experimental), while RayDataExecutor remains in experimental. Looking at the diff, RayActorPoolExecutor was actually moved OUT of experimental (import changed from `nemo_curator.backends.experimental.ray_actor_pool` to `nemo_curator.backends.ray_actor_pool`), so the 'Stable' claim is consistent with the new import path. This appears to be a valid documentation update, not an error - Greptile may have misread the new import path.",2026-01-22T18:20:55.086929+00:00
mellanox/libxlio,558,[CI] issue: HPCINFRA-4182 Add Coverity analysis for examples dir,https://github.com/Mellanox/libxlio/pull/558,,"**logic:** The linker flag `-libverbs` should be `-lxlio -libverbs` or placed after the source file. The example code includes `<mellanox/xlio_extra.h>` (line 50 of the C file) which requires linking against libxlio.

```suggestion
    gcc -I${cov_dir}/install/include -L${cov_dir}/install/lib -o ${cov_dir}/xlio_ultra_api_ping_pong ${WORKSPACE}/examples/xlio_ultra_api_ping_pong.c -lxlio -libverbs >> ${cov_dir}/cov.log 2>&1""
```",https://github.com/Mellanox/libxlio/pull/558#discussion_r2717799615,,2026-01-22T17:02:37+00:00,logic,medium,"Greptile correctly identifies that the example code includes xlio headers and likely uses xlio functions, meaning it should be linked against libxlio (not just libibverbs). The suggestion to add -L flag for library path and -lxlio is correct for a proper build. Additionally, the original has a typo: '-libverbs' should be '-libibverbs' (with double 'ib').",2026-01-22T18:20:55.086978+00:00
browseros-ai/browseros-server,272,feat: onboarding fixes ,https://github.com/browseros-ai/BrowserOS-agent/pull/272,,"**logic:** `publishConfig` is incomplete — missing `main`, `module`, and `types` overrides

The `publishConfig` only overrides the `exports` field, but the top-level `main`, `module`, and `types` fields still point to `./src/index.ts`. Since the `files` array only includes `[""dist""]`, the published package won't contain `src/`, causing legacy tools and TypeScript to fail with missing file errors.

```suggestion
  ""publishConfig"": {
    ""main"": ""./dist/index.cjs"",
    ""module"": ""./dist/index.js"",
    ""types"": ""./dist/index.d.ts"",
    ""exports"": {
      ""."": {
        ""types"": ""./dist/index.d.ts"",
        ""import"": ""./dist/index.js"",
        ""require"": ""./dist/index.cjs""
      },
      ""./api.json"": ""./dist/api.json""
    }
  },
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/agent-sdk/package.json
Line: 43:52

Comment:
**logic:** `publishConfig` is incomplete — missing `main`, `module`, and `types` overrides

The `publishConfig` only overrides the `exports` field, but the top-level `main`, `module`, and `types` fields still point to `./src/index.ts`. Since the `files` array only includes `[""dist""]`, the published package won't contain `src/`, causing legacy tools and TypeScript to fail with missing file errors.

```suggestion
  ""publishConfig"": {
    ""main"": ""./dist/index.cjs"",
    ""module"": ""./dist/index.js"",
    ""types"": ""./dist/index.d.ts"",
    ""exports"": {
      ""."": {
        ""types"": ""./dist/index.d.ts"",
        ""import"": ""./dist/index.js"",
        ""require"": ""./dist/index.cjs""
      },
      ""./api.json"": ""./dist/api.json""
    }
  },
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/browseros-ai/BrowserOS-agent/pull/272#discussion_r2717754105,,2026-01-22T16:50:48+00:00,logic,high,"Greptile correctly identifies that the `files` array only includes `dist/` but the top-level `main`, `module`, and `types` fields point to `./src/index.ts`. While `publishConfig.exports` is set correctly, npm's `publishConfig` doesn't override the root `main`/`module`/`types` fields, so legacy tools that don't use the exports map will try to resolve `./src/index.ts` which won't exist in the published package. This would cause the published package to be broken for many consumers.",2026-01-22T18:20:55.087054+00:00
browseros-ai/browseros-agent,272,feat: onboarding fixes ,https://github.com/browseros-ai/BrowserOS-agent/pull/272,,"**logic:** `publishConfig` is incomplete — missing `main`, `module`, and `types` overrides

The `publishConfig` only overrides the `exports` field, but the top-level `main`, `module`, and `types` fields still point to `./src/index.ts`. Since the `files` array only includes `[""dist""]`, the published package won't contain `src/`, causing legacy tools and TypeScript to fail with missing file errors.

```suggestion
  ""publishConfig"": {
    ""main"": ""./dist/index.cjs"",
    ""module"": ""./dist/index.js"",
    ""types"": ""./dist/index.d.ts"",
    ""exports"": {
      ""."": {
        ""types"": ""./dist/index.d.ts"",
        ""import"": ""./dist/index.js"",
        ""require"": ""./dist/index.cjs""
      },
      ""./api.json"": ""./dist/api.json""
    }
  },
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/agent-sdk/package.json
Line: 43:52

Comment:
**logic:** `publishConfig` is incomplete — missing `main`, `module`, and `types` overrides

The `publishConfig` only overrides the `exports` field, but the top-level `main`, `module`, and `types` fields still point to `./src/index.ts`. Since the `files` array only includes `[""dist""]`, the published package won't contain `src/`, causing legacy tools and TypeScript to fail with missing file errors.

```suggestion
  ""publishConfig"": {
    ""main"": ""./dist/index.cjs"",
    ""module"": ""./dist/index.js"",
    ""types"": ""./dist/index.d.ts"",
    ""exports"": {
      ""."": {
        ""types"": ""./dist/index.d.ts"",
        ""import"": ""./dist/index.js"",
        ""require"": ""./dist/index.cjs""
      },
      ""./api.json"": ""./dist/api.json""
    }
  },
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/browseros-ai/BrowserOS-agent/pull/272#discussion_r2717754105,,2026-01-22T16:50:48+00:00,logic,high,"Greptile correctly identified that the `files` array only includes `dist`, but the top-level `main`, `module`, and `types` fields point to `./src/index.ts`. While `publishConfig.exports` is set correctly, npm's `publishConfig` doesn't override `main`, `module`, and `types` fields - only certain fields like `registry` and `access` are supported. This would cause the published package to have broken references since src/ won't be included. This is a non-obvious npm publishing bug that would break consumers of the package.",2026-01-22T18:20:55.087150+00:00
openucx/ucc,1245,UCC/CORE: Added local rank from topo if not provided by user,https://github.com/openucx/ucc/pull/1245,,"**logic:** `ctx->topo` is NULL when `params->oob.n_oob_eps <= 1`. The inner block (lines 733-761) initializes `ctx->topo` on line 746, but it only executes if `params->oob.n_oob_eps > 1`. When this condition is false, line 770 will call `ucc_topo_init()` with a NULL `ctx->topo`, causing a crash.

```suggestion
        if (ctx->topo) {
            status = ucc_topo_init(set, ctx->topo, &topo);
```",https://github.com/openucx/ucc/pull/1245#discussion_r2717622121,,2026-01-22T16:18:20+00:00,null,high,"Greptile correctly identifies that when `params->oob.n_oob_eps <= 1`, the inner block that initializes `ctx->topo` (line 746) is skipped, but `ucc_topo_init(set, ctx->topo, &topo)` on line 770 is still called with an uninitialized/NULL `ctx->topo`, which would cause a crash. This is a non-obvious control flow bug that a typical reviewer could miss.",2026-01-22T18:20:55.087241+00:00
openucx/ucc,1245,UCC/CORE: Added local rank from topo if not provided by user,https://github.com/openucx/ucc/pull/1245,,"**logic:** returning `1` when the node subgroup doesn't exist is inconsistent - should return `0` for single-rank scenarios

```suggestion
        return 0;
```

 is the local rank intended to be 1-indexed or 0-indexed in the context where this function is called?",https://github.com/openucx/ucc/pull/1245#discussion_r2717622163,,2026-01-22T16:18:21+00:00,logic,medium,"Greptile correctly identifies an inconsistency - when the node subgroup exists, `sbgp->group_rank` returns a 0-indexed rank, but when it doesn't exist (single-rank scenario), returning 1 is inconsistent with 0-indexed semantics. A single-rank process should have local rank 0, not 1. This could cause off-by-one errors in code that uses this function.",2026-01-22T18:20:55.087283+00:00
openucx/ucc,1245,UCC/CORE: Added local rank from topo if not provided by user,https://github.com/openucx/ucc/pull/1245,,"**logic:** When `params->oob.n_oob_eps <= 1`, the block on lines 733-761 doesn't execute, leaving `ctx->topo` uninitialized (NULL). Passing NULL `ctx->topo` to `ucc_topo_init()` causes `topo->topo = NULL` to be set (ucc_topo.c:171). When `ucc_topo_get_sbgp()` is called in `ucc_topo_node_local_rank()`, it may invoke `ucc_sbgp_create()`, which uses the `UCC_TOPO_IS_BOUND` macro that dereferences `topo->topo->sock_bound` or `topo->topo->numa_bound`, causing a NULL pointer dereference.

```suggestion
        status = ucc_topo_init(set, ctx->topo ? ctx->topo : NULL, &topo);
```

Better fix: Either ensure `ctx->topo` is initialized before this point, or handle the NULL case in `ucc_topo_init()` / `ucc_sbgp_create()`.",https://github.com/openucx/ucc/pull/1245#discussion_r2717845816,,2026-01-22T17:14:14+00:00,null,high,"Greptile correctly identifies that when `params->oob.n_oob_eps <= 1`, the code block initializing `ctx->topo` (lines 733-761) is skipped, but the code then unconditionally calls `ucc_topo_init(set, ctx->topo, &topo)` at line 770 with an uninitialized `ctx->topo`. This can lead to NULL pointer dereference when the topo subsystem attempts to access members through `ctx->topo`. The suggested fix is trivial (already passes NULL) but the real issue - potential NULL dereference in downstream code - is a valid bug.",2026-01-22T18:20:55.087306+00:00
openucx/ucc,1245,UCC/CORE: Added local rank from topo if not provided by user,https://github.com/openucx/ucc/pull/1245,,"**logic:** Returning `1` when the node subgroup doesn't exist is inconsistent. All other ranks would return their 0-indexed `sbgp->group_rank`. For single-rank scenarios, this should return `0` to maintain consistency with 0-indexed ranking.

```suggestion
    if (sbgp->status == UCC_SBGP_NOT_EXISTS) {
        return 0;
    }
```

 Is the node local rank intended to be 0-indexed or 1-indexed when passed to UCX?",https://github.com/openucx/ucc/pull/1245#discussion_r2717845924,,2026-01-22T17:14:14+00:00,logic,medium,"Greptile correctly identifies an inconsistency: when the node subgroup exists, the function returns `sbgp->group_rank` which is 0-indexed, but when it doesn't exist (single-rank scenario), it returns 1 instead of 0. This breaks the expected 0-indexed semantics and could cause off-by-one errors in callers expecting consistent 0-indexed ranks.",2026-01-22T18:20:55.087345+00:00
posthog/posthog,45758,feat(web-analytics): serve favicons from static CDN and add to domain selector,https://github.com/PostHog/posthog/pull/45758,,"**logic:** `window.JS_URL` can be undefined according to the type declaration in `globals.d.ts`

if undefined, this produces URLs like `undefined/static/favicons/${domain}`, which will fail. add a fallback or handle the undefined case

```suggestion
export const faviconUrl = (domain: string): string => `${window.JS_URL || ''}/static/favicons/${domain}.png`
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/web-analytics/common.ts
Line: 557:557

Comment:
**logic:** `window.JS_URL` can be undefined according to the type declaration in `globals.d.ts`

if undefined, this produces URLs like `undefined/static/favicons/${domain}`, which will fail. add a fallback or handle the undefined case

```suggestion
export const faviconUrl = (domain: string): string => `${window.JS_URL || ''}/static/favicons/${domain}.png`
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45758#discussion_r2719116632,,2026-01-23T00:11:35+00:00,runtime,medium,"Greptile correctly identifies that `window.JS_URL` can be undefined according to type declarations, which would result in malformed URLs like 'undefined/static/favicons/domain' that would fail to load favicons. This is a non-obvious edge case that could cause broken images in production environments where JS_URL isn't set.",2026-01-23T00:47:41.715495+00:00
onyx-dot-app/onyx,7680,THINKING PACKETS :BIG_GRIN:,https://github.com/onyx-dot-app/onyx/pull/7680,,"verification assertions don't match implementation

All these assertions expect `config[""options""]` but the implementation stores thinking/reasoning config at `config[""provider""][provider][""models""][model_name][""config""]`. These assertions will all fail.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/sandbox/internal/verify_opencode_config.py
Line: 198:201

Comment:
verification assertions don't match implementation

All these assertions expect `config[""options""]` but the implementation stores thinking/reasoning config at `config[""provider""][provider][""models""][model_name][""config""]`. These assertions will all fail.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7680#discussion_r2718981986,,2026-01-22T23:13:20+00:00,logic,medium,"Greptile correctly identified that the assertions at the end of the file are accessing `config[""options""]` but based on the implementation in DirectoryManager.setup_opencode_config (not shown but referenced), the thinking/reasoning configuration is stored in a nested provider/model structure, not at the top-level options key. This is a real bug that would cause all these verification assertions to fail with KeyError when the script is run.",2026-01-23T00:47:41.715582+00:00
onyx-dot-app/onyx,7680,THINKING PACKETS :BIG_GRIN:,https://github.com/onyx-dot-app/onyx/pull/7680,,"`select.select()` doesn't work with pipes on Windows - only with sockets. When running on Windows, line 298 will raise an exception, caught at line 320, and return empty string. The Windows fallback path (lines 317-318) is never reached.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/sandbox/internal/agent_client.py
Line: 298:318

Comment:
`select.select()` doesn't work with pipes on Windows - only with sockets. When running on Windows, line 298 will raise an exception, caught at line 320, and return empty string. The Windows fallback path (lines 317-318) is never reached.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7680#discussion_r2719091092,,2026-01-22T23:59:59+00:00,runtime,medium,"Greptile correctly identifies that select.select() doesn't work with pipes on Windows (only sockets), which would cause an OSError at line 298 before reaching the fcntl ImportError fallback at lines 317-318. The Windows fallback code path is unreachable because the exception occurs earlier in the try block. This is a non-obvious cross-platform compatibility bug that would cause stderr reading to silently fail on Windows.",2026-01-23T00:47:41.715641+00:00
helixdb/helix-db,821,"impr(cli): improving cli dx, docker builds check for cli availability before building",https://github.com/HelixDB/helix-db/pull/821,,"Port checking before build may cause race condition. The build process (line 151-152) regenerates `docker-compose.yml` with the original requested port via `generate_docker_files()`. If the port becomes unavailable between the check (line 141) and the actual docker start (line 166), or if the build overwrites the port-adjusted compose file, the deployment could fail.",https://github.com/HelixDB/helix-db/pull/821#discussion_r2718930824,,2026-01-22T22:56:41+00:00,concurrency,medium,"Greptile correctly identifies a race condition: port availability is checked at line 141, then build runs at line 151-152 which likely regenerates docker-compose with the original port (via generate_docker_files()), and only afterward (lines 155-163) is the compose file regenerated with the new port if changed. This means the build step could overwrite the port adjustment, and even if not, the port could become unavailable between check and actual docker start at line 166. The time window between check and use creates a TOCTOU vulnerability.",2026-01-23T00:47:41.715695+00:00
getarcaneapp/arcane,1561,feat: arcane specific labels for icons and custom urls,https://github.com/getarcaneapp/arcane/pull/1561,,"**logic:** Potential XSS risk: `javascript:` URLs bypass this check and could execute arbitrary code

```suggestion
	function toHref(raw: string): string {
		const trimmed = raw.trim();
		if (!trimmed) return '#';
		// Block javascript: and data: URLs
		if (/^(javascript|data|vbscript):/i.test(trimmed)) return '#';
		if (/^[a-zA-Z][a-zA-Z0-9+.-]*:/.test(trimmed)) return trimmed;
		return `http://${trimmed}`;
	}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/routes/(app)/projects/[projectId]/+page.svelte
Line: 278:282

Comment:
**logic:** Potential XSS risk: `javascript:` URLs bypass this check and could execute arbitrary code

```suggestion
	function toHref(raw: string): string {
		const trimmed = raw.trim();
		if (!trimmed) return '#';
		// Block javascript: and data: URLs
		if (/^(javascript|data|vbscript):/i.test(trimmed)) return '#';
		if (/^[a-zA-Z][a-zA-Z0-9+.-]*:/.test(trimmed)) return trimmed;
		return `http://${trimmed}`;
	}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/getarcaneapp/arcane/pull/1561#discussion_r2718863255,,2026-01-22T22:27:42+00:00,security,high,"The `toHref` function correctly identifies that strings with any protocol scheme (like `javascript:`) are passed through unchanged via the regex `/^[a-zA-Z][a-zA-Z0-9+.-]*:/`. This URL is then used in an `<a href={toHref(url)}>` tag, allowing potential XSS attacks if project URLs are user-controlled. This is a real, non-obvious security vulnerability that could lead to arbitrary JavaScript execution.",2026-01-23T00:47:41.715728+00:00
onyx-dot-app/onyx,7686,feat: IDEAL OUTPUT PANEL,https://github.com/onyx-dot-app/onyx/pull/7686,,"security vulnerability: removed path traversal security check

the removed code validated that `file_path` doesn't escape `sandbox_path` using `.resolve()`. without this check, a malicious user could use `../` in the path parameter to access files outside their sandbox

```suggestion
        sandbox_path = Path(SANDBOX_BASE_PATH) / str(sandbox.session_id)
        file_path = sandbox_path / path

        # Security check: ensure path doesn't escape sandbox
        try:
            file_path = file_path.resolve()
            sandbox_path_resolved = sandbox_path.resolve()
            if not str(file_path).startswith(str(sandbox_path_resolved)):
                raise ValueError(""Access denied - path traversal"")
        except ValueError:
            raise
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/session/manager.py
Line: 1011:1012

Comment:
security vulnerability: removed path traversal security check

the removed code validated that `file_path` doesn't escape `sandbox_path` using `.resolve()`. without this check, a malicious user could use `../` in the path parameter to access files outside their sandbox

```suggestion
        sandbox_path = Path(SANDBOX_BASE_PATH) / str(sandbox.session_id)
        file_path = sandbox_path / path

        # Security check: ensure path doesn't escape sandbox
        try:
            file_path = file_path.resolve()
            sandbox_path_resolved = sandbox_path.resolve()
            if not str(file_path).startswith(str(sandbox_path_resolved)):
                raise ValueError(""Access denied - path traversal"")
        except ValueError:
            raise
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7686#discussion_r2719273971,,2026-01-23T01:44:46+00:00,security,critical,"The diff clearly shows the removal of a path traversal security check that prevented users from accessing files outside their sandbox using '../' sequences. Without this validation, a malicious user could potentially read arbitrary files from the server filesystem, which is a critical security vulnerability. This is a non-obvious catch since the removal might have seemed intentional during refactoring.",2026-01-23T02:56:29.230983+00:00
onyx-dot-app/onyx,7685,Kubernetes deployment,https://github.com/onyx-dot-app/onyx/pull/7685,,"path sanitization only removes `..` from anywhere in the string, but doesn't prevent path traversal attacks using sequences like `/workspace/outputs/../../../etc/passwd`

```suggestion
        clean_path = str(Path(path.lstrip(""/"")).resolve().relative_to(Path(""."")))
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/sandbox/kubernetes/manager.py
Line: 728:729

Comment:
path sanitization only removes `..` from anywhere in the string, but doesn't prevent path traversal attacks using sequences like `/workspace/outputs/../../../etc/passwd`

```suggestion
        clean_path = str(Path(path.lstrip(""/"")).resolve().relative_to(Path(""."")))
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7685#discussion_r2719276936,,2026-01-23T01:46:39+00:00,security,critical,"The path sanitization using `.replace("".."", """")` on line 729 is ineffective because it only removes the literal string "".."" once, not recursively. An attacker could use paths like ""....//"" which after replacement becomes ""../"" or encoded variants. This allows path traversal to read arbitrary files from the sandbox container (e.g., `/etc/passwd`). This is a real security vulnerability in file access operations that could expose sensitive data.",2026-01-23T02:56:29.231092+00:00
onyx-dot-app/onyx,7685,Kubernetes deployment,https://github.com/onyx-dot-app/onyx/pull/7685,,"same path traversal vulnerability as in `list_directory` - the `.replace("".."", """")` approach is insufficient

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/sandbox/kubernetes/manager.py
Line: 829:829

Comment:
same path traversal vulnerability as in `list_directory` - the `.replace("".."", """")` approach is insufficient

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7685#discussion_r2719276981,,2026-01-23T01:46:41+00:00,security,critical,"The path sanitization using `.replace("".."", """")` is indeed insufficient for preventing path traversal attacks. An attacker could use patterns like `....//` which after replacing `..` becomes `../`, or URL-encoded variants like `%2e%2e/`. This vulnerability could allow reading arbitrary files outside the intended outputs directory, which is a serious security issue in a sandbox environment.",2026-01-23T02:56:29.231165+00:00
onyx-dot-app/onyx,7685,Kubernetes deployment,https://github.com/onyx-dot-app/onyx/pull/7685,,"uses `sandbox-file-sync` service account which has S3 IRSA permissions - verify the main `sandbox` container doesn't inherit these credentials after the init container completes

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/sandbox/kubernetes/manager.py
Line: 311:311

Comment:
uses `sandbox-file-sync` service account which has S3 IRSA permissions - verify the main `sandbox` container doesn't inherit these credentials after the init container completes

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7685#discussion_r2719277015,,2026-01-23T01:46:42+00:00,security,high,"Greptile correctly identifies a security concern - the pod spec at line 311 sets `service_account_name=self._file_sync_service_account` for the entire pod, meaning both the init container AND the main sandbox container will have S3 IRSA credentials. This violates the principle of least privilege since the main sandbox container (which runs user code) shouldn't have S3 write access that was only meant for the file-sync init container. This is a non-obvious security issue that could allow sandbox escapes to access/modify S3 data.",2026-01-23T02:56:29.231205+00:00
greptileai/react-mirror,409,[compiler][wip] Support `finally` clauses,https://github.com/greptileai/react-mirror/pull/409,,"**logic:** finalizer block not added to `activeTryBlocks`, so JSX in finally won't be validated

```suggestion
    if (block.terminal.kind === 'try') {
      // Add the try block itself to activeTryBlocks
      activeTryBlocks.push(block.terminal.block);
      // Also add handler if present
      if (block.terminal.handler !== null) {
        activeTryBlocks.push(block.terminal.handler);
      }
      // Also add finalizer if present
      if (block.terminal.finalizer !== null) {
        activeTryBlocks.push(block.terminal.finalizer);
      }
    }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: compiler/packages/babel-plugin-react-compiler/src/Validation/ValidateNoJSXInTryStatement.ts
Line: 57:64

Comment:
**logic:** finalizer block not added to `activeTryBlocks`, so JSX in finally won't be validated

```suggestion
    if (block.terminal.kind === 'try') {
      // Add the try block itself to activeTryBlocks
      activeTryBlocks.push(block.terminal.block);
      // Also add handler if present
      if (block.terminal.handler !== null) {
        activeTryBlocks.push(block.terminal.handler);
      }
      // Also add finalizer if present
      if (block.terminal.finalizer !== null) {
        activeTryBlocks.push(block.terminal.finalizer);
      }
    }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/409#discussion_r2719275513,,2026-01-23T01:45:44+00:00,logic,medium,"The PR title explicitly mentions 'Support `finally` clauses', yet the code only adds handler blocks to activeTryBlocks without adding finalizer blocks. This means JSX in finally blocks won't be validated, which directly contradicts the purpose of this PR. Greptile correctly identified this omission in the validation logic.",2026-01-23T02:56:29.231232+00:00
nvidia/fuser,5865,[IR Refactor] Phase 1: IrContainer/IrStorage Composition Pattern,https://github.com/NVIDIA/Fuser/pull/5865,,"**logic:** `parent_` is `nullptr` when copy constructor is invoked, but `IrStorage::copy()` calls `to->parent()` (line 40) which triggers `NVF_ERROR` when `parent_` is `nullptr` (storage.h:225-226).

```suggestion
IrStorage::IrStorage(const IrStorage& other) {
  FUSER_PERF_SCOPE(""IrStorage copy"");
  parent_ = other.parent_;  // Initialize parent before calling copy
  IrStorage::copy(&other, this);
}
```",https://github.com/NVIDIA/Fuser/pull/5865#discussion_r2719360698,,2026-01-23T02:35:12+00:00,runtime,high,"The comment correctly identifies that IrStorage's copy constructor calls IrStorage::copy() which uses to->parent() on line 40, but parent_ is never initialized before this call. Looking at the code, parent() likely has an NVF_ERROR check when parent_ is nullptr (as referenced in storage.h:225-226), meaning the copy constructor would crash. This is a non-obvious initialization order bug that would cause runtime errors when copying IrStorage objects.",2026-01-23T02:56:29.231301+00:00
nvidia/nvflare,4019,[2.7] AddFedAvgMemEfficient,https://github.com/NVIDIA/NVFlare/pull/4019,,"Integer tensors are not weighted correctly - adds raw values without normalization, breaking FedAvg semantics. With 2 clients sending `[1]` and `[3]` with equal weights, produces `[4]` instead of expected average `[2]`.

```suggestion
                    else:
                        # Integer/bool tensors: scale then add (cast to float, aggregate, round back)
                        target_param.add_((result.params[k].float() * normalized_weight).to(target_param.dtype))
```",https://github.com/NVIDIA/NVFlare/pull/4019#discussion_r2719201210,,2026-01-23T01:01:14+00:00,logic,high,"Greptile correctly identifies that integer tensors skip normalization (lines 155-157), breaking FedAvg weighted averaging. The code explicitly adds integer tensor values without the `alpha=normalized_weight` parameter, so two clients sending [1] and [3] with equal weights would produce [4] instead of the correct weighted average [2]. This is a subtle but significant logic bug in a federated averaging algorithm.",2026-01-23T02:56:29.231325+00:00
onyx-dot-app/onyx,7691,feat(opensearch): Implement complete retrieval filtering,https://github.com/onyx-dot-app/onyx/pull/7691,,"`list.remove()` will raise `ValueError` if `PUBLIC_DOC_PAT` is not in the list, which happens when `access.is_public` is `False`

```suggestion
def _generate_filtered_access_control_list(access: DocumentAccess) -> list[str]:
    """"""Filters the access control list to remove PUBLIC_DOC_PAT.

    In the OpenSearch schema this is represented by PUBLIC_FIELD_NAME.
    """"""
    filtered_access_control_list = [acl for acl in access.to_acl() if acl != PUBLIC_DOC_PAT]
    return filtered_access_control_list
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/document_index/opensearch/opensearch_document_index.py
Line: 73:80

Comment:
`list.remove()` will raise `ValueError` if `PUBLIC_DOC_PAT` is not in the list, which happens when `access.is_public` is `False`

```suggestion
def _generate_filtered_access_control_list(access: DocumentAccess) -> list[str]:
    """"""Filters the access control list to remove PUBLIC_DOC_PAT.

    In the OpenSearch schema this is represented by PUBLIC_FIELD_NAME.
    """"""
    filtered_access_control_list = [acl for acl in access.to_acl() if acl != PUBLIC_DOC_PAT]
    return filtered_access_control_list
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7691#discussion_r2719418316,,2026-01-23T03:09:54+00:00,runtime,high,"The code calls `filtered_access_control_list.remove(PUBLIC_DOC_PAT)` unconditionally, but when `access.is_public` is False, PUBLIC_DOC_PAT won't be in the list returned by `to_acl()`, causing a ValueError crash. This is a real runtime bug that would occur whenever processing non-public documents, and the suggested list comprehension fix is correct and actionable.",2026-01-23T04:30:08.249706+00:00
nvidia/nvflare,4020,[2.7] Enhance weighted agg helper,https://github.com/NVIDIA/NVFlare/pull/4020,,"in-place modification of caller's input tensor can cause unexpected side effects if caller retains reference to `v`

```suggestion
                                self.total[k] = v.mul(weight)
```",https://github.com/NVIDIA/NVFlare/pull/4020#discussion_r2719392572,,2026-01-23T02:54:55+00:00,data_integrity,high,"The code uses `v.mul_(weight)` which is an in-place PyTorch operation that modifies the original tensor. If the caller retains a reference to the data passed to `add()`, their data will be unexpectedly modified. The code comment claims this is safe for 'InTime aggregation' but this assumption may not hold for all use cases, making this a subtle but real bug that could cause data corruption.",2026-01-23T04:30:08.249803+00:00
valian-ca/firebase-functions,15,feat: add @valian/pino-logger,https://github.com/valian-ca/firebase-functions/pull/15,,"**logic:** unconditional import at module load time conflicts with `firebase-functions` being marked as optional peer dependency - module will fail to load if `firebase-functions` isn't installed, even in dev environments where it's never used

consider either: making it a required peer dependency, or using dynamic import when in production mode",https://github.com/valian-ca/firebase-functions/pull/15#discussion_r2719404442,,2026-01-23T03:00:56+00:00,runtime,medium,"Greptile correctly identifies that `firebase-functions` is imported unconditionally at the top of the module, but based on the logic, it's only used in production (non-local) environments. If firebase-functions is an optional peer dependency, the module will crash on import in dev environments where firebase-functions isn't installed, even though it would never actually be used there. This is a real runtime bug that's non-obvious and requires understanding the relationship between the import, the peer dependency configuration, and the conditional usage.",2026-01-23T04:30:08.249841+00:00
posthog/posthog,45764,fix(data-modeling): don't pause matview temporal schedules with empty results,https://github.com/PostHog/posthog/pull/45764,,"**logic:** Removing `a_pause_saved_query_schedule` from the generic error handler changes behavior for all unexpected errors. Previously, any unexpected error would pause the schedule. Now only timeouts pause schedules. This means unexpected errors might continue retrying indefinitely.

```suggestion
        else:
            saved_query.latest_error = f""Query failed to materialize: {error_message}""
            saved_query.sync_frequency_interval = None
            await logger.aerror(""Failed to materialize model with unexpected error: %s"", str(e))
            await database_sync_to_async(saved_query.save)()
            await a_pause_saved_query_schedule(saved_query)
            await mark_job_as_failed(job, error_message, logger)
```

 Was the removal of schedule pausing for unexpected errors intentional, or should unexpected errors still pause schedules?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/data_modeling/run_workflow.py
Line: 640:645

Comment:
**logic:** Removing `a_pause_saved_query_schedule` from the generic error handler changes behavior for all unexpected errors. Previously, any unexpected error would pause the schedule. Now only timeouts pause schedules. This means unexpected errors might continue retrying indefinitely.

```suggestion
        else:
            saved_query.latest_error = f""Query failed to materialize: {error_message}""
            saved_query.sync_frequency_interval = None
            await logger.aerror(""Failed to materialize model with unexpected error: %s"", str(e))
            await database_sync_to_async(saved_query.save)()
            await a_pause_saved_query_schedule(saved_query)
            await mark_job_as_failed(job, error_message, logger)
```

 Was the removal of schedule pausing for unexpected errors intentional, or should unexpected errors still pause schedules?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45764#discussion_r2719750331,,2026-01-23T06:10:54+00:00,logic,medium,"Greptile correctly identified that the removal of `await a_pause_saved_query_schedule(saved_query)` from the generic error handler (the `else` branch) changes behavior for all unexpected errors. In the old code, unexpected errors would pause the schedule, but in the new code only timeouts pause schedules (and QueryReturnedNoResultsException explicitly doesn't pause). This could lead to unexpected errors continuously retrying without stopping, which may not be intentional given the PR title focuses specifically on 'empty results'.",2026-01-23T06:22:08.614156+00:00
posthog/posthog,45763,feat(workflows): add wait for event action,https://github.com/PostHog/posthog/pull/45763,,"**logic:** `ConditionalBranchHandler` expects actions with `config.condition` or `config.conditions`, but `wait_for_event` has `config.event_filters`. This will cause runtime errors when the handler tries to access `action.config.condition` which doesn't exist for this action type.

The handler's type signature at line 18 of `conditional_branch.ts` explicitly excludes `wait_for_event`:
```typescript
ActionHandlerOptions<Extract<HogFlowAction, { type: 'conditional_branch' | 'wait_until_condition' }>>
```

You need to either:
1. Update `ConditionalBranchHandler.execute()` to handle `wait_for_event` actions by checking for `config.event_filters` and transforming it appropriately, or
2. Create a dedicated handler for `wait_for_event` that adapts the data structure before delegating to the conditional logic

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nodejs/src/cdp/services/hogflows/hogflow-executor.service.ts
Line: 96:96

Comment:
**logic:** `ConditionalBranchHandler` expects actions with `config.condition` or `config.conditions`, but `wait_for_event` has `config.event_filters`. This will cause runtime errors when the handler tries to access `action.config.condition` which doesn't exist for this action type.

The handler's type signature at line 18 of `conditional_branch.ts` explicitly excludes `wait_for_event`:
```typescript
ActionHandlerOptions<Extract<HogFlowAction, { type: 'conditional_branch' | 'wait_until_condition' }>>
```

You need to either:
1. Update `ConditionalBranchHandler.execute()` to handle `wait_for_event` actions by checking for `config.event_filters` and transforming it appropriately, or
2. Create a dedicated handler for `wait_for_event` that adapts the data structure before delegating to the conditional logic

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45763#discussion_r2719641753,,2026-01-23T05:15:41+00:00,type_error,high,"Greptile correctly identifies that `wait_for_event` uses `config.event_filters` based on the PR title context, while `ConditionalBranchHandler` is designed for actions with `config.condition`/`config.conditions`. The type signature explicitly only handles `conditional_branch` and `wait_until_condition` types. This type mismatch would cause runtime errors when the handler tries to access non-existent properties.",2026-01-23T06:22:08.614310+00:00
onyx-dot-app/onyx,7695,Kube deploy v2,https://github.com/onyx-dot-app/onyx/pull/7695,,"Remove debugging sleep - this will block tests for 1000 seconds (16+ minutes)

```suggestion
```

**Context Used:** Rule from `dashboard` - Remove temporary debugging code before merging to production, especially tenant-specific debugging l... ([source](https://app.greptile.com/review/custom-context?memory=b39fa59b-2568-4dd3-9576-83b46251a7b8))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/sandbox/kubernetes/test_kubernetes_sandbox_provision.py
Line: 321:321

Comment:
Remove debugging sleep - this will block tests for 1000 seconds (16+ minutes)

```suggestion
```

**Context Used:** Rule from `dashboard` - Remove temporary debugging code before merging to production, especially tenant-specific debugging l... ([source](https://app.greptile.com/review/custom-context?memory=b39fa59b-2568-4dd3-9576-83b46251a7b8))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7695#discussion_r2719602006,,2026-01-23T04:55:20+00:00,performance,high,"Greptile correctly identified a `time.sleep(1000)` call on line 321 that would block the test for over 16 minutes. This is clearly leftover debugging code that was accidentally left in the PR. While this is in a test file, it would cause CI/CD pipelines to hang or timeout, making it a significant issue that could easily be missed during review.",2026-01-23T06:22:08.614396+00:00
onyx-dot-app/onyx,7695,Kube deploy v2,https://github.com/onyx-dot-app/onyx/pull/7695,,"`entrypoint.sh` file is not included in this PR - the Docker build will fail

The Dockerfile copies and references this file, but it doesn't exist in the repository or the changeset.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/sandbox/kubernetes/docker/Dockerfile
Line: 59:60

Comment:
`entrypoint.sh` file is not included in this PR - the Docker build will fail

The Dockerfile copies and references this file, but it doesn't exist in the repository or the changeset.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7695#discussion_r2719602029,,2026-01-23T04:55:21+00:00,runtime,high,"The Dockerfile references `COPY entrypoint.sh /entrypoint.sh` but based on Greptile's analysis, this file is not included in the PR changeset. This would cause the Docker build to fail immediately with a 'file not found' error. This is a non-obvious catch since it requires checking what files were included in the PR against what the Dockerfile expects.",2026-01-23T06:22:08.614430+00:00
nvidia/nvflare,4020,[2.7] Enhance weighted agg helper,https://github.com/NVIDIA/NVFlare/pull/4020,,"stores reference to caller's NumPy array instead of creating copy. If `weigh_by_local_iter=False` and caller passes NumPy array, modifications to caller's array after `add()` will affect accumulated values

```suggestion
                        else:
                            self.total[k] = v.copy() if hasattr(v, 'copy') else v
```",https://github.com/NVIDIA/NVFlare/pull/4020#discussion_r2719615204,,2026-01-23T05:02:23+00:00,data_integrity,medium,"Greptile correctly identified that when `weigh_by_local_iter=False` and a NumPy array is passed (non-PyTorch tensor without in-place ops), the code stores `v` directly without copying. The else branch at line 68 does `self.total[k] = v` for the first contribution, creating a reference alias. If the caller later modifies their array, it corrupts the accumulated values. The suggested fix using `v.copy() if hasattr(v, 'copy') else v` properly addresses this aliasing bug.",2026-01-23T06:22:08.614520+00:00
nvidia/nvflare,4020,[2.7] Enhance weighted agg helper,https://github.com/NVIDIA/NVFlare/pull/4020,,"stores reference to caller's NumPy array, not a copy. When `weigh_by_local_iter=False`, if caller passes NumPy array, subsequent modifications to the caller's array will affect the accumulated values in `self.total[k]`

```suggestion
                            self.total[k] = v.copy() if hasattr(v, 'copy') else v
```",https://github.com/NVIDIA/NVFlare/pull/4020#discussion_r2719635906,,2026-01-23T05:12:34+00:00,data_integrity,medium,"Looking at line 80 in the diff, when `weigh_by_local_iter=False` and the tensor is not a PyTorch tensor, the code does `self.total[k] = v` which stores a direct reference to the caller's array. This means if the caller modifies their array later, it will corrupt the accumulated values in the aggregator. The suggested fix to use `v.copy()` for NumPy arrays is correct and prevents this aliasing bug.",2026-01-23T06:22:08.614545+00:00
democratizedspace/dspace,3051,Add NPC select options to quest creation,https://github.com/democratizedspace/dspace/pull/3051,,"**style:** Test `QuestFormDefaultSubmission.test.js:94` expects `savedQuest.npc` to be `DEFAULT_NPC_NAME` ('Mission Control'), but this change sets the default to `/assets/npc/dChat.jpg`. The test will fail. Either update the test to expect the new avatar path, or consider whether the old test assertion was incorrect (since `quest.npc` is used as an `<img src>` in `QuestChat.svelte:100`).",https://github.com/democratizedspace/dspace/pull/3051#discussion_r2719591638,,2026-01-23T04:49:26+00:00,logic,medium,"Greptile correctly identifies that the default NPC value is changing from DEFAULT_NPC_NAME to npcOptions[0]?.avatar, which will break the existing test expecting DEFAULT_NPC_NAME ('Mission Control'). The comment also insightfully points out that since npc is used as an img src, the old test assertion may have been incorrect anyway. This catches a test failure that would occur and highlights a semantic inconsistency in the codebase.",2026-01-23T06:22:08.614567+00:00
democratizedspace/dspace,3045,Support custom quest dependencies and custom content in quest editor,https://github.com/democratizedspace/dspace/pull/3045,,"**logic:** Premature image release: `releaseRewardImages()` is called immediately after fetching items (line 39), before the items are used. This will decrement ref counts for all reward images right after loading them, potentially causing URLs to be revoked before they're actually displayed. Remove this call since `onDestroy` already handles cleanup.",https://github.com/democratizedspace/dspace/pull/3045#discussion_r2719588645,,2026-01-23T04:48:03+00:00,resource_leak,medium,"Greptile correctly identifies that `releaseRewardImages()` is called at line 39 right after fetching new items but before the new `rewardItems` array is assigned (line 40-52). This releases the OLD items' images (which is correct), but the comment is slightly misleading - it's actually trying to clean up previous items. However, there's still a bug: the `releaseRewardImages()` call at line 39 operates on the current `rewardItems` which could be empty on first load, making it a no-op, but if `onMount` somehow runs twice (or reactivity triggers reloads), it would release images before they're used. The concern about premature release is valid for edge cases.",2026-01-23T06:22:08.614593+00:00
posthog/posthog,45771,feat(experiments): new action to pause experiments,https://github.com/PostHog/posthog/pull/45771,,"**logic:** pause/resume buttons will show for completed experiments if `experiment.feature_flag` exists, since this block is wrapped in `isExperimentRunning` (line 465) which returns `true` for any experiment with a `start_date`, including completed ones

```suggestion
                        {experiment.feature_flag && !experiment.end_date &&
                            (experiment.feature_flag.active ? (
                                <ButtonPrimitive
                                    variant=""danger""
                                    menuItem
                                    data-attr=""pause-experiment""
                                    onClick={() => openPauseExperimentModal()}
                                >
                                    <IconPause /> Pause experiment
                                </ButtonPrimitive>
                            ) : (
                                <ButtonPrimitive
                                    menuItem
                                    data-attr=""resume-experiment""
                                    onClick={() => resumeExperiment()}
                                >
                                    <IconPlay /> Resume experiment
                                </ButtonPrimitive>
                            ))}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/experiments/ExperimentView/components.tsx
Line: 524:542

Comment:
**logic:** pause/resume buttons will show for completed experiments if `experiment.feature_flag` exists, since this block is wrapped in `isExperimentRunning` (line 465) which returns `true` for any experiment with a `start_date`, including completed ones

```suggestion
                        {experiment.feature_flag && !experiment.end_date &&
                            (experiment.feature_flag.active ? (
                                <ButtonPrimitive
                                    variant=""danger""
                                    menuItem
                                    data-attr=""pause-experiment""
                                    onClick={() => openPauseExperimentModal()}
                                >
                                    <IconPause /> Pause experiment
                                </ButtonPrimitive>
                            ) : (
                                <ButtonPrimitive
                                    menuItem
                                    data-attr=""resume-experiment""
                                    onClick={() => resumeExperiment()}
                                >
                                    <IconPlay /> Resume experiment
                                </ButtonPrimitive>
                            ))}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45771#discussion_r2720533667,,2026-01-23T10:00:00+00:00,logic,medium,"Greptile correctly identifies that the pause/resume buttons will appear for completed experiments. The new code block only checks `experiment.feature_flag` but doesn't check `!experiment.end_date`, while the adjacent 'Stop' button does include that check. This would allow users to pause/resume experiments that have already ended, which is logically incorrect behavior. The fix to add `&& !experiment.end_date` is correct and consistent with the Stop button's condition.",2026-01-23T10:17:05.030044+00:00
gptme/gptme,1142,feat(cli): add visible prompt type-ahead using patch_stdout,https://github.com/gptme/gptme/pull/1142,,"**logic:** Thread-safety issue: `prompt_queue` is a regular Python list accessed from both the background thread (here) and the main thread in `chat.py` (lines 183, 280, 288) without synchronization. The `self._lock` only protects access within this method, but doesn't protect against concurrent access from the main thread. This can cause race conditions like:
- Main thread pops while background thread appends
- Corrupted queue state
- Lost or duplicate messages

Use `queue.Queue` or ensure all `prompt_queue` access in `chat.py` also uses the same lock.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/util/async_input.py
Line: 113:124

Comment:
**logic:** Thread-safety issue: `prompt_queue` is a regular Python list accessed from both the background thread (here) and the main thread in `chat.py` (lines 183, 280, 288) without synchronization. The `self._lock` only protects access within this method, but doesn't protect against concurrent access from the main thread. This can cause race conditions like:
- Main thread pops while background thread appends
- Corrupted queue state
- Lost or duplicate messages

Use `queue.Queue` or ensure all `prompt_queue` access in `chat.py` also uses the same lock.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1142#discussion_r2720377811,"✅ Fixed in commit 5a218e1f2

Replaced list with  which is thread-safe for producer-consumer patterns. The collector now owns its queue internally and provides thread-safe accessor methods: `get_message()`, `has_messages()`, `clear_queue()`.",2026-01-23T09:19:09+00:00,concurrency,medium,"The developer confirmed and fixed this issue by replacing the list with queue.Queue. While the final diff shows the fixed code using queue.Queue, Greptile correctly identified a thread-safety problem in the original code where a list was being accessed from multiple threads without proper synchronization, which could cause race conditions and data corruption.",2026-01-23T10:17:05.030194+00:00
cedarjs/cedar,1024,chore(test-project): AboutPage Redwood -> Cedar,https://github.com/cedarjs/cedar/pull/1024,,"missing assertion - the `await` and `expect()` were removed, so this line no longer verifies that the text exists on the page

```suggestion
  await expect(
    page.getByText(
      'This site was created to demonstrate my mastery of Cedar: Look on my works, ye',
    ),
  ).toBeVisible()
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: tasks/smoke-tests/shared/common.ts
Line: 41:43

Comment:
missing assertion - the `await` and `expect()` were removed, so this line no longer verifies that the text exists on the page

```suggestion
  await expect(
    page.getByText(
      'This site was created to demonstrate my mastery of Cedar: Look on my works, ye',
    ),
  ).toBeVisible()
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/cedarjs/cedar/pull/1024#discussion_r2720370038,,2026-01-23T09:17:05+00:00,logic,medium,"Greptile correctly identified that the original code had `await page.getByText(...)` which would wait for the element, but the change removed both the `await` and any `expect()` assertion, leaving a dead line that doesn't verify anything. This would cause the smoke test to pass even if the expected text is missing from the page, defeating the purpose of the test.",2026-01-23T10:17:05.030268+00:00
posthog/posthog,45785,feat(hogql): postgres style casts,https://github.com/PostHog/posthog/pull/45785,,"**logic:** field name mismatch - C++ uses `type_name` but Python AST expects `to_type` (see `posthog/hogql/ast.py:36`). This will cause deserialization to fail when using cpp-json backend.

```suggestion
  VISIT(ColumnExprPgCast) {
    Json expr_json = visitAsJSON(ctx->columnExpr());
    string type_name = to_lower_copy(visitAsString(ctx->identifier()));

    Json json = Json::object();
    json[""node""] = ""PGCast"";
    if (!is_internal) addPositionInfo(json, ctx);
    json[""expr""] = expr_json;
    json[""to_type""] = type_name;
    return json;
  }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: common/hogql_parser/parser_json.cpp
Line: 1478:1489

Comment:
**logic:** field name mismatch - C++ uses `type_name` but Python AST expects `to_type` (see `posthog/hogql/ast.py:36`). This will cause deserialization to fail when using cpp-json backend.

```suggestion
  VISIT(ColumnExprPgCast) {
    Json expr_json = visitAsJSON(ctx->columnExpr());
    string type_name = to_lower_copy(visitAsString(ctx->identifier()));

    Json json = Json::object();
    json[""node""] = ""PGCast"";
    if (!is_internal) addPositionInfo(json, ctx);
    json[""expr""] = expr_json;
    json[""to_type""] = type_name;
    return json;
  }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45785#discussion_r2720850309,,2026-01-23T11:35:38+00:00,data_integrity,high,This is a cross-language serialization bug where the C++ parser uses 'type_name' as the JSON field name but the Python AST class likely expects 'to_type' for deserialization. This mismatch would cause the PGCast feature to fail when using the cpp-json backend. The bug is non-obvious because it requires knowledge of both the C++ and Python codebases and their interaction.,2026-01-23T12:22:59.934677+00:00
posthog/posthog,45775,refactor(comments): move comment model to directory structure,https://github.com/PostHog/posthog/pull/45775,,"**logic:** self-mentions will produce events when they shouldn't. test `test_self_mentions_do_not_produce_events` expects self-mentions to be filtered, but there's no filtering logic here.

```suggestion
        for mentioned_user in mentioned_users:
            # Skip self-mentions
            if mentioned_user.id == commenter.id:
                continue
                
            produce_internal_event(
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/models/comment/utils.py
Line: 96:122

Comment:
**logic:** self-mentions will produce events when they shouldn't. test `test_self_mentions_do_not_produce_events` expects self-mentions to be filtered, but there's no filtering logic here.

```suggestion
        for mentioned_user in mentioned_users:
            # Skip self-mentions
            if mentioned_user.id == commenter.id:
                continue
                
            produce_internal_event(
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45775#discussion_r2720641698,,2026-01-23T10:30:34+00:00,logic,medium,"Greptile correctly identifies that the code has no filtering for self-mentions. The function iterates through all mentioned_users and produces events for each one, including when the commenter mentions themselves. If a test named 'test_self_mentions_do_not_produce_events' exists and expects this behavior to be filtered, this is clearly a missing logic bug that would cause unwanted notification events.",2026-01-23T12:22:59.934791+00:00
twentyhq/twenty,17394,Encrypt/decrypt app secret variables,https://github.com/twentyhq/twenty/pull/17394,,"when updating existing variables in `upsertManyApplicationVariableEntities`, only the metadata fields are updated, but not the variable value itself. This means if the variable already exists, its value won't change even when a new value is provided.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/core-modules/applicationVariable/application-variable.service.ts
Line: 111:128

Comment:
when updating existing variables in `upsertManyApplicationVariableEntities`, only the metadata fields are updated, but not the variable value itself. This means if the variable already exists, its value won't change even when a new value is provided.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17394#discussion_r2720750653,,2026-01-23T11:03:10+00:00,logic,high,"Greptile correctly identified that in the upsertManyApplicationVariableEntities method, when an existing variable is found (lines 117-124), the update only changes 'description' and 'isSecret' fields, but NOT the 'value' field. The encryptedValue is computed but never used in the update path, meaning existing variables won't have their values updated - a clear upsert logic bug.",2026-01-23T12:22:59.934857+00:00
twentyhq/twenty,17394,Encrypt/decrypt app secret variables,https://github.com/twentyhq/twenty/pull/17394,,"silently returns the original value when encryption fails. If `APP_SECRET` is missing or invalid, this could lead to secrets being stored unencrypted without the caller knowing. Consider throwing an error or logging at a higher level than debug.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/core-modules/secret-encryption/secret-encryption.service.ts
Line: 28:38

Comment:
silently returns the original value when encryption fails. If `APP_SECRET` is missing or invalid, this could lead to secrets being stored unencrypted without the caller knowing. Consider throwing an error or logging at a higher level than debug.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17394#discussion_r2720750705,,2026-01-23T11:03:11+00:00,security,high,"Greptile correctly identifies that the encrypt method silently falls back to returning the original unencrypted value when encryption fails (lines 31-35). This is a security vulnerability because secrets could be stored in plaintext without any notification to the caller, especially if APP_SECRET is misconfigured. The debug-level logging makes this failure easy to miss in production.",2026-01-23T12:22:59.934909+00:00
twentyhq/twenty,17400,Files v2 - FILES field update/create in common API,https://github.com/twentyhq/twenty/pull/17400,,"Duplicate check only validates `addFiles` but the same `fileId` could appear in both `addFiles` and `updateFiles` across different records. Check for duplicates across all file operations (addFiles, updateFiles, removeFiles).

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/api/common/common-args-processors/data-arg-processor/services/files-field-sync.service.ts
Line: 94:102

Comment:
Duplicate check only validates `addFiles` but the same `fileId` could appear in both `addFiles` and `updateFiles` across different records. Check for duplicates across all file operations (addFiles, updateFiles, removeFiles).

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17400#discussion_r2721148126,,2026-01-23T13:13:49+00:00,data_integrity,medium,"Greptile correctly identifies that lines 94-102 only check for duplicates within `toAddFileIds`, but a fileId could appear in both `addFiles` and `updateFiles` (or other combinations) across different records without being caught. This could lead to data integrity issues where the same file is processed multiple times in conflicting operations (e.g., both added and removed simultaneously).",2026-01-23T14:20:20.645517+00:00
twentyhq/twenty,17400,Files v2 - FILES field update/create in common API,https://github.com/twentyhq/twenty/pull/17400,,"File sync operations run after the database transaction completes. If file operations fail (e.g., file move fails), the database will have inconsistent state with references to files that weren't moved. Consider wrapping this in a transaction or implementing rollback logic.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/api/common/common-query-runners/common-create-many-query-runner/common-create-many-query-runner.service.ts
Line: 109:115

Comment:
File sync operations run after the database transaction completes. If file operations fail (e.g., file move fails), the database will have inconsistent state with references to files that weren't moved. Consider wrapping this in a transaction or implementing rollback logic.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17400#discussion_r2721148267,True ! Need to discuss it,2026-01-23T13:13:51+00:00,data_integrity,medium,"Greptile correctly identified that `processFilesFieldSyncOperationsIfNeeded` runs after the database upsert operations complete (line 107-113), meaning if file sync fails, the database will contain references to files that weren't properly synced. The developer acknowledged this is a valid concern with 'True ! Need to discuss it', confirming the issue.",2026-01-23T14:20:20.645591+00:00
nvidia/earth2studio,645,In-Situ Cyclone Tracking pipeline,https://github.com/NVIDIA/earth2studio/pull/645,,"`TempestExtremes` and `AsyncTempestExtremes` are exported in `__all__` but never imported

```suggestion
from earth2studio.models.dx.tempest_extremes import (
    AsyncTempestExtremes,
    TempestExtremes,
)
from earth2studio.models.dx.tc_tracking import (
    TCTrackerVitart,
    TCTrackerWuDuan,
)
from earth2studio.models.dx.wind_gust import WindgustAFNO  # noqa

__all__ = [
    ""AsyncTempestExtremes"",
    ""ClimateNet"",
    ""CorrDiff"",
    ""CorrDiffCMIP6"",
    ""CorrDiffTaiwan"",
    ""PrecipitationAFNO"",
    ""PrecipitationAFNOv2"",
    ""SolarRadiationAFNO1H"",
    ""SolarRadiationAFNO6H"",
    ""TempestExtremes"",
    ""WindgustAFNO"",
]
```",https://github.com/NVIDIA/earth2studio/pull/645#discussion_r2721171453,,2026-01-23T13:20:53+00:00,runtime,medium,"Looking at the diff, the code adds 'TempestExtremes' to __all__ but only shows imports up to line 57. Greptile correctly identifies that if TempestExtremes and AsyncTempestExtremes are listed in __all__ without being imported, attempting to use `from earth2studio.models.dx import TempestExtremes` would fail with an ImportError. This is a real bug that would cause runtime failures and is easy to miss when adding new exports.",2026-01-23T14:20:20.645647+00:00
posthog/posthog,45820,chore(experiments): Move default stats method to Team model,https://github.com/PostHog/posthog/pull/45820,,"**logic:** potential issue with null handling - `default_experiment_stats_method` can be `None` for existing teams before backfill runs

```suggestion
            default_method = team.default_experiment_stats_method or ""bayesian""
            stats_config[""method""] = default_method
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/clickhouse/views/experiments.py
Line: 301:302

Comment:
**logic:** potential issue with null handling - `default_experiment_stats_method` can be `None` for existing teams before backfill runs

```suggestion
            default_method = team.default_experiment_stats_method or ""bayesian""
            stats_config[""method""] = default_method
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45820#discussion_r2721872267,,2026-01-23T16:11:19+00:00,null,medium,"The code changed from using organization's default to team's default, and the diff shows no null handling was added. For existing teams before any backfill migration runs, the `default_experiment_stats_method` field could be None, which would result in `stats_config['method']` being set to None instead of a valid method. This is a legitimate null-handling issue that could cause downstream problems when the stats method is expected to be a valid string value.",2026-01-23T16:20:36.584671+00:00
posthog/posthog,45820,chore(experiments): Move default stats method to Team model,https://github.com/PostHog/posthog/pull/45820,,"**logic:** potential issue with null handling - `default_experiment_stats_method` can be `None` for existing teams before backfill runs

```suggestion
        default_method = team.default_experiment_stats_method or ""bayesian""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/web_experiment.py
Line: 169:169

Comment:
**logic:** potential issue with null handling - `default_experiment_stats_method` can be `None` for existing teams before backfill runs

```suggestion
        default_method = team.default_experiment_stats_method or ""bayesian""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45820#discussion_r2721872300,,2026-01-23T16:11:19+00:00,null,medium,"The code is moving from organization-level to team-level default_experiment_stats_method. For existing teams that haven't been backfilled yet, this field could be None, which would then propagate into stats_config causing potential issues downstream. This is a migration edge case that a typical reviewer could miss.",2026-01-23T16:20:36.584757+00:00
posthog/posthog,45819,feat(insight-ai): check billing addons,https://github.com/PostHog/posthog/pull/45819,,"**logic:** need to also check `billingLoading` state from `billingLogic` to avoid incorrectly disabling the button while billing data is loading

```suggestion
    const { currentPlatformAddon, billingLoading } = useValues(billingLogic)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/insights/InsightAIAnalysis.tsx
Line: 25:25

Comment:
**logic:** need to also check `billingLoading` state from `billingLogic` to avoid incorrectly disabling the button while billing data is loading

```suggestion
    const { currentPlatformAddon, billingLoading } = useValues(billingLogic)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45819#discussion_r2721846946,,2026-01-23T16:05:40+00:00,logic,medium,"Greptile correctly identifies a race condition where `currentPlatformAddon` will be undefined/null while billing data is loading, causing `hasBoostOrHigher` to be false and incorrectly disabling the button with an upgrade message even for users who have the required addon. This is a real UX bug that affects paid users and is non-obvious since it involves async state timing.",2026-01-23T16:20:36.584793+00:00
posthog/posthog,45819,feat(insight-ai): check billing addons,https://github.com/PostHog/posthog/pull/45819,,"**logic:** should also check `billingLoading` to prevent disabling the button while billing data loads

```suggestion
                        disabledReason={
                            billingLoading
                                ? 'Loading billing information...'
                                : !hasBoostOrHigher
                                  ? 'Upgrade to at least the Boost add-on to use AI analysis'
                                  : insightDataLoading
                                    ? 'Please wait for the insight to finish loading'
                                    : undefined
                        }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/insights/InsightAIAnalysis.tsx
Line: 62:68

Comment:
**logic:** should also check `billingLoading` to prevent disabling the button while billing data loads

```suggestion
                        disabledReason={
                            billingLoading
                                ? 'Loading billing information...'
                                : !hasBoostOrHigher
                                  ? 'Upgrade to at least the Boost add-on to use AI analysis'
                                  : insightDataLoading
                                    ? 'Please wait for the insight to finish loading'
                                    : undefined
                        }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45819#discussion_r2721846997,,2026-01-23T16:05:40+00:00,logic,medium,"Greptile correctly identifies a race condition: when billing data is still loading, `currentPlatformAddon` will be undefined/null, causing `hasBoostOrHigher` to be false and incorrectly showing the upgrade message to users who may already have the addon. This is a real UX bug that would briefly show misleading disabled states during page load.",2026-01-23T16:20:36.584819+00:00
posthog/posthog,45805,feat(llma): add generation-level clustering and summarization,https://github.com/PostHog/posthog/pull/45805,,"**logic:** The `provider` field is missing from `inputs_with_window` construction, which could cause issues since `max_length` lookup depends on it

```suggestion
            provider=inputs.provider,
            model=inputs.model,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/llm_analytics/trace_summarization/workflow.py
Line: 182:182

Comment:
**logic:** The `provider` field is missing from `inputs_with_window` construction, which could cause issues since `max_length` lookup depends on it

```suggestion
            provider=inputs.provider,
            model=inputs.model,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45805#discussion_r2721392070,,2026-01-23T14:19:31+00:00,logic,medium,"Looking at the diff, the `inputs_with_window` construction (around line 182) includes `model=inputs.model` but omits `provider=inputs.provider`. This is a real bug since `inputs_with_window` is passed to activities that may need the provider information, and the original `inputs` object has a `provider` field that should be preserved. The max_length lookup uses `inputs.provider` directly, but any activity receiving `inputs_with_window` would lose the provider context.",2026-01-23T16:20:36.584859+00:00
raycast/extensions,24801,Add Contributor Covenant Code of Conduct,https://github.com/raycast/extensions/pull/24801,,"**syntax:** missing actual contact information - line 63 currently says `CODE_OF_CONDUCT.md` instead of an email address or contact method

```suggestion
reported to the community leaders responsible for enforcement at
[INSERT CONTACT EMAIL OR METHOD HERE].
```",https://github.com/raycast/extensions/pull/24801#discussion_r2721521821,,2026-01-23T14:50:22+00:00,data_integrity,medium,"Greptile correctly identified that line 63 says reports should be sent to 'CODE_OF_CONDUCT.md' which is the filename itself, not a valid contact method. This is a real functional issue - people who need to report incidents would have no way to actually contact community leaders. While not a code bug, this is a clear copy-paste error that renders the enforcement section non-functional.",2026-01-23T16:20:36.584886+00:00
twentyhq/twenty,17403,fix(twenty-server): add SSRF protection to webhook requests,https://github.com/twentyhq/twenty/pull/17403,,"the `error` field is not defined in the `webhookResponseSchema` at `packages/twenty-server/src/engine/core-modules/audit/utils/events/workspace-event/webhook/webhook-response.ts`. The schema uses `z.strictObject` which rejects unknown properties, causing a Zod validation error when `makeTrackEvent` parses the event.

Add `error: z.string().optional()` to the schema's properties object.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/core-modules/webhook/jobs/call-webhook.job.ts
Line: 117:124

Comment:
the `error` field is not defined in the `webhookResponseSchema` at `packages/twenty-server/src/engine/core-modules/audit/utils/events/workspace-event/webhook/webhook-response.ts`. The schema uses `z.strictObject` which rejects unknown properties, causing a Zod validation error when `makeTrackEvent` parses the event.

Add `error: z.string().optional()` to the schema's properties object.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17403#discussion_r2721410633,,2026-01-23T14:23:41+00:00,runtime,medium,"The code adds an `error` field to the audit event payload when SSRF is blocked, but if the webhookResponseSchema uses z.strictObject (which rejects unknown properties), this will cause a Zod validation error at runtime. This is a non-obvious cross-file bug where the new field wasn't added to the schema definition, causing the audit logging to fail silently or throw errors.",2026-01-23T16:20:36.584942+00:00
nvidia/nvflare,4028,Increase link check timeout,https://github.com/NVIDIA/NVFlare/pull/4028,,"Duplicate `--timeout` parameters conflict with each other. Line 38's `--timeout 100` will override line 36's `--timeout=30`, making the intended change ineffective. Remove line 38.

```suggestion
          --timeout=30
          --verbose
```",https://github.com/NVIDIA/NVFlare/pull/4028#discussion_r2721750117,,2026-01-23T15:41:07+00:00,logic,medium,"Greptile correctly identifies that the PR adds `--timeout=30` on line 36 but fails to remove the existing `--timeout 100` on line 38, creating duplicate conflicting parameters where the later value (100) will override the intended new value (30). The PR title states 'Increase link check timeout' but the actual effect is no change since 100 already wins. This is a non-obvious configuration error that defeats the purpose of the PR.",2026-01-23T16:20:36.584981+00:00
alpaca-network/gatewayz-frontend,891,feat(subscriptions): update UI to display tiered subscription credits,https://github.com/Alpaca-Network/gatewayz-frontend/pull/891,,"displays cents as dollars - should divide by 100 first

```suggestion
                  ${(subscriptionAllowance / 100).toFixed(0)}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/components/layout/credits-display.tsx
Line: 196:196

Comment:
displays cents as dollars - should divide by 100 first

```suggestion
                  ${(subscriptionAllowance / 100).toFixed(0)}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/891#discussion_r2721429085,,2026-01-23T14:27:56+00:00,logic,medium,"The code shows that `monthlyAllowance` is calculated by dividing `TIER_CONFIG[tier].monthlyAllowance` by 100 (line 157), indicating values are stored in cents. However, `subscriptionAllowance` from userData is displayed directly on line 196 without the same division, causing the UI to show cents as dollars (e.g., $1000 instead of $10). This is a real bug that would cause incorrect financial amounts to be displayed to users.",2026-01-23T16:20:36.585003+00:00
alpaca-network/gatewayz-frontend,891,feat(subscriptions): update UI to display tiered subscription credits,https://github.com/Alpaca-Network/gatewayz-frontend/pull/891,,"displays cents as dollars - should divide by 100 first

```suggestion
                  +${(purchasedCredits / 100).toFixed(0)}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/components/layout/credits-display.tsx
Line: 202:202

Comment:
displays cents as dollars - should divide by 100 first

```suggestion
                  +${(purchasedCredits / 100).toFixed(0)}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/891#discussion_r2721429194,,2026-01-23T14:27:57+00:00,logic,medium,"The code comment on line 157 explicitly states 'in cents, convert to dollars' and shows division by 100 for monthlyAllowance. The subscriptionAllowance on line 195 is also divided by 100 before display, but purchasedCredits on line 202 is displayed directly without this conversion, creating an inconsistent display where purchased credits would appear 100x larger than intended.",2026-01-23T16:20:36.585028+00:00
infisical/infisical,5257,feat(helm): Added Gateway API and HTTPRoute along with the existing Ingress,https://github.com/Infisical/infisical/pull/5257,,"**CRITICAL BUG**: References `$gateway.hostName` but `values.yaml:131` defines it as `hostname` (lowercase 'n'). This will cause template rendering to fail or use empty values.

```suggestion
      hostname: {{ $gateway.hostname }}
```",https://github.com/Infisical/infisical/pull/5257#discussion_r2722159400,,2026-01-23T17:22:30+00:00,runtime,high,"Greptile correctly identified a case sensitivity mismatch between the template using `$gateway.hostName` (camelCase) and the values.yaml likely defining `hostname` (lowercase). This would cause the template to render with empty/nil values for the hostname field, breaking the Gateway API configuration. This is a non-obvious typo that would cause runtime failures.",2026-01-23T18:21:26.205884+00:00
veda-labs/boring-vault,589,Fix/syrup assets,https://github.com/Veda-Labs/boring-vault/pull/589,,"**syntax:** Variable `i` is out of scope here (loop ended on line 48). This uses an undefined variable and will cause a compilation error.

```suggestion
        RolesAuthority(rolesAuthForDeployer).setUserRole(ryan, 1, false);  
```",https://github.com/Veda-Labs/boring-vault/pull/589#discussion_r2722003275,,2026-01-23T16:40:04+00:00,runtime,high,"Greptile correctly identified that the loop variable `i` is used on line 52 outside its scope (the for loop ends at line 48). The code should use `ryan` instead of `hardwareWallets[i]`. This would cause a compilation error in Solidity, preventing the script from running at all. While compilation errors are caught before deployment, this is a real bug that Greptile correctly identified with a proper fix suggestion.",2026-01-23T18:21:26.206015+00:00
synnaxlabs/synnax,1865,SY-3603: Fix Issues with Moving Arc Programs Between Racks,https://github.com/synnaxlabs/synnax/pull/1865,,"missing rollback for ontology relationship created here - if this operation succeeds but a later operation fails, the relationship won't be cleaned up by the rollback mechanism",https://github.com/synnaxlabs/synnax/pull/1865#discussion_r2722309101,,2026-01-23T18:11:01+00:00,data_integrity,medium,"Greptile correctly identifies that at line 229 (`await client.ontology.addChildren(...)`), if this operation succeeds but a subsequent operation fails, there's no rollback registered to clean up this relationship. The existing code pattern shows rollbacks being added for store operations, but this ontology relationship creation has no corresponding rollback, which could leave orphaned relationships on failure.",2026-01-23T18:21:26.206102+00:00
synnaxlabs/synnax,1865,SY-3603: Fix Issues with Moving Arc Programs Between Racks,https://github.com/synnaxlabs/synnax/pull/1865,,"missing rollback for task creation - if the task creation succeeds but the `addChildren` call on line 229 fails, the task won't be cleaned up. add `rollbacks.push(store.tasks.delete(newTsk.key))` after task creation",https://github.com/synnaxlabs/synnax/pull/1865#discussion_r2722309135,,2026-01-23T18:11:02+00:00,data_integrity,medium,"Greptile correctly identifies that if `client.ontology.addChildren()` on line 229 fails after the task is created on lines 220-228, there's no rollback registered to clean up the orphaned task. The existing rollback pattern (e.g., `rollbacks.push(store.arcs.set(prog))` after arc creation) should also be applied after task creation to maintain consistency. This is a non-obvious consistency issue in a complex multi-step operation.",2026-01-23T18:21:26.206174+00:00
synnaxlabs/synnax,1864,SY-3602: Fix version scripts usage and bump versions to v0.50.0,https://github.com/synnaxlabs/synnax/pull/1864,,"`sed -i ''` syntax is macOS-specific and will fail on Linux. Since CI runs on ubuntu-latest, this script will break. Use portable sed syntax instead:

```suggestion
        sed -i.bak ""s/^version = \"".*\""/version = \""$VERSION\""/"" ""$pyproject"" && rm ""$pyproject.bak""
```",https://github.com/synnaxlabs/synnax/pull/1864#discussion_r2722287142,,2026-01-23T18:04:46+00:00,runtime,medium,"Greptile correctly identifies that `sed -i ''` is macOS-specific syntax. On Linux/GNU sed, the `-i` flag doesn't require an argument, so `sed -i ''` will fail or behave unexpectedly. This is a real cross-platform compatibility bug that would cause the script to break on Linux CI environments, and it's non-obvious because the script would work fine on the developer's Mac.",2026-01-23T18:21:26.206222+00:00
synnaxlabs/synnax,1864,SY-3602: Fix version scripts usage and bump versions to v0.50.0,https://github.com/synnaxlabs/synnax/pull/1864,,"`sed -i ''` syntax is macOS-specific and will fail on Linux

```suggestion
        sed -i.bak ""s/\""version\"": \"".*\""/\""version\"": \""$VERSION\""/"" ""$pkg_json"" && rm ""$pkg_json.bak""
```",https://github.com/synnaxlabs/synnax/pull/1864#discussion_r2722287203,,2026-01-23T18:04:47+00:00,runtime,medium,"Greptile correctly identified that `sed -i ''` is macOS-specific syntax. On Linux/GNU sed, this will fail because GNU sed requires `-i` to have no argument or an argument directly attached (e.g., `-i.bak`). This is a real portability bug that would cause the script to fail on Linux systems, which is non-obvious to developers who only test on macOS.",2026-01-23T18:21:26.206264+00:00
synnaxlabs/synnax,1864,SY-3602: Fix version scripts usage and bump versions to v0.50.0,https://github.com/synnaxlabs/synnax/pull/1864,,"`sed -i ''` syntax is macOS-specific and will fail on Linux

```suggestion
        sed -i.bak ""s/\""version\"": \"".*\""/\""version\"": \""$VERSION\""/"" ""$tauri_conf"" && rm ""$tauri_conf.bak""
```",https://github.com/synnaxlabs/synnax/pull/1864#discussion_r2722287258,,2026-01-23T18:04:48+00:00,runtime,medium,"Greptile correctly identifies that `sed -i ''` is macOS-specific syntax - on Linux/GNU sed, the empty string argument would be interpreted differently, causing the script to fail. This is a real portability bug that would affect CI/CD environments typically running Linux, and it's non-obvious because the script would work fine on macOS development machines but fail in production/CI contexts.",2026-01-23T18:21:26.206288+00:00
nvidia/topograph,200,initial commit for lambda provider simulator,https://github.com/NVIDIA/topograph/pull/200,,"**logic:** Index out-of-bounds panic if `from + c.pageSize > len(c.instanceIDs)`. The loop condition should also check `indx < len(c.instanceIDs)`.

```suggestion
	for indx = from; indx < from+c.pageSize && indx < len(c.instanceIDs); indx++ {
		node, exists := c.model.Nodes[c.instanceIDs[indx]]
```",https://github.com/NVIDIA/topograph/pull/200#discussion_r2722236164,,2026-01-23T17:48:40+00:00,runtime,high,"The loop iterates from `from` to `from + c.pageSize` without checking if `indx` exceeds `len(c.instanceIDs)`. When pagination is used, if `from + pageSize > len(instanceIDs)`, accessing `c.instanceIDs[indx]` will cause an index out-of-bounds panic. This is a real bug that could crash the application during normal pagination operations, and the suggested fix is correct and actionable.",2026-01-23T18:21:26.206314+00:00
alpaca-network/gatewayz-frontend,896,feat(models): add grouped table view with per-provider pricing,https://github.com/Alpaca-Network/gatewayz-frontend/pull/896,,"The best price calculation compares raw string values as floats without normalizing them first. Different gateways return prices in different formats (per-token vs per-million), so comparing `parseFloat(gatewayPricing[gw]?.prompt)` directly can lead to incorrect results.

For example:
- OpenRouter returns `0.00000015` (per-token)
- OneRouter returns `0.15` (per-million)

Both represent the same price, but the comparison would incorrectly choose OpenRouter as ""cheaper"" because `0.00000015 < 0.15`.

```suggestion
  // For display, use the best pricing (lowest input cost)
  const bestGateway = useMemo(() => {
    if (gateways.length === 0) return sourceGateway;
    let best = gateways[0];
    let bestInputPrice = Infinity;
    for (const gw of gateways) {
      // Normalize prices to per-million format for fair comparison
      const normalizedPrice = parseFloat(formatPricingForDisplay(gatewayPricing[gw]?.prompt, gw) || '999999');
      if (normalizedPrice < bestInputPrice) {
        bestInputPrice = normalizedPrice;
        best = gw;
      }
    }
    return best;
  }, [gateways, gatewayPricing, sourceGateway]);
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/app/models/models-client.tsx
Line: 253:265

Comment:
The best price calculation compares raw string values as floats without normalizing them first. Different gateways return prices in different formats (per-token vs per-million), so comparing `parseFloat(gatewayPricing[gw]?.prompt)` directly can lead to incorrect results.

For example:
- OpenRouter returns `0.00000015` (per-token)
- OneRouter returns `0.15` (per-million)

Both represent the same price, but the comparison would incorrectly choose OpenRouter as ""cheaper"" because `0.00000015 < 0.15`.

```suggestion
  // For display, use the best pricing (lowest input cost)
  const bestGateway = useMemo(() => {
    if (gateways.length === 0) return sourceGateway;
    let best = gateways[0];
    let bestInputPrice = Infinity;
    for (const gw of gateways) {
      // Normalize prices to per-million format for fair comparison
      const normalizedPrice = parseFloat(formatPricingForDisplay(gatewayPricing[gw]?.prompt, gw) || '999999');
      if (normalizedPrice < bestInputPrice) {
        bestInputPrice = normalizedPrice;
        best = gw;
      }
    }
    return best;
  }, [gateways, gatewayPricing, sourceGateway]);
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/896#discussion_r2722681041,,2026-01-23T20:11:06+00:00,logic,medium,"The code at line 253-265 directly compares raw price strings as floats without normalization. Looking at line 256, `parseFloat(gatewayPricing[gw]?.prompt || '999999')` is used for comparison, while line 261 uses `formatPricingForDisplay()` which normalizes pricing. If different gateways use different pricing formats (per-token vs per-million), this would incorrectly identify the 'best' price. The suggested fix to use `formatPricingForDisplay()` for comparison is correct and actionable.",2026-01-23T20:17:53.281854+00:00
alpaca-network/gatewayz-frontend,896,feat(models): add grouped table view with per-provider pricing,https://github.com/Alpaca-Network/gatewayz-frontend/pull/896,,"When `bestGateway` is not in `gatewayPricing` (e.g., when it falls back to `sourceGateway`), this code uses `model.pricing` which may not exist or may be from a different gateway. This can display incorrect pricing if the model's primary pricing doesn't match the best gateway's pricing.

Check if `gatewayPricing[bestGateway]` exists before falling back, and handle the case where neither exists to avoid displaying mismatched pricing.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/app/models/models-client.tsx
Line: 267:269

Comment:
When `bestGateway` is not in `gatewayPricing` (e.g., when it falls back to `sourceGateway`), this code uses `model.pricing` which may not exist or may be from a different gateway. This can display incorrect pricing if the model's primary pricing doesn't match the best gateway's pricing.

Check if `gatewayPricing[bestGateway]` exists before falling back, and handle the case where neither exists to avoid displaying mismatched pricing.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/896#discussion_r2722681097,,2026-01-23T20:11:07+00:00,logic,medium,"Greptile correctly identifies that when `bestGateway` falls back to `sourceGateway` (which happens when `gateways.length === 0`), the code then tries to access `gatewayPricing[bestGateway]` which will be undefined since `gatewayPricing` is empty, causing a fallback to `model.pricing` which may represent pricing from a different gateway than `bestGateway`, displaying potentially incorrect/mismatched pricing information to users.",2026-01-23T20:17:53.282017+00:00
traceroot-ai/traceroot,367,[AgentOps][Python SDK] Simplify sdk user experience,https://github.com/traceroot-ai/traceroot/pull/367,,"**logic:** Race condition in concurrent scenarios. If multiple threads call decorated functions simultaneously before initialization, both could see `_client is None` and create multiple `TracerootClient` instances, each calling `trace.set_tracer_provider()`.

Consider using thread-safe initialization:
```suggestion
    global _client
    if _client is None:
        import threading
        with threading.Lock():
            if _client is None:
                _client = TracerootClient()
    return _client
```

Or use double-checked locking with a module-level lock.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: traceroot-py/traceroot/__init__.py
Line: 86:89

Comment:
**logic:** Race condition in concurrent scenarios. If multiple threads call decorated functions simultaneously before initialization, both could see `_client is None` and create multiple `TracerootClient` instances, each calling `trace.set_tracer_provider()`.

Consider using thread-safe initialization:
```suggestion
    global _client
    if _client is None:
        import threading
        with threading.Lock():
            if _client is None:
                _client = TracerootClient()
    return _client
```

Or use double-checked locking with a module-level lock.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/367#discussion_r2722490747,,2026-01-23T19:10:19+00:00,concurrency,medium,"Greptile correctly identifies a real race condition - the check-then-act pattern on _client is not thread-safe, so concurrent threads could both see _client as None and create multiple TracerootClient instances. This is a non-obvious bug in auto-initialization code that could cause duplicate tracer providers to be set up, leading to unexpected behavior in multi-threaded applications.",2026-01-23T20:17:53.282079+00:00
posthog/posthog,45838,fix(workflows): make workflows scenes tab aware,https://github.com/PostHog/posthog/pull/45838,,"**logic:** Form should use the dynamic `logic` instance instead of static `messageTemplateLogic` to maintain consistency with the rest of the component

```suggestion
        <Form logic={logic} formKey=""template"" props={props}>
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/workflows/frontend/TemplateLibrary/MessageTemplate.tsx
Line: 38:38

Comment:
**logic:** Form should use the dynamic `logic` instance instead of static `messageTemplateLogic` to maintain consistency with the rest of the component

```suggestion
        <Form logic={logic} formKey=""template"" props={props}>
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45838#discussion_r2722877920,,2026-01-23T21:18:37+00:00,logic,medium,"Greptile correctly identifies an inconsistency: the component creates a dynamic `logic` instance with props (line 30) and uses it for `useActions` and `useValues`, but the Form component still references the static `messageTemplateLogic` instead of the dynamic `logic` instance. This could cause the Form to connect to a different logic instance than the rest of the component, leading to state synchronization issues.",2026-01-23T22:14:38.748397+00:00
onyx-dot-app/onyx,7709,feat: google ai studio (gemini api keys),https://github.com/onyx-dot-app/onyx/pull/7709,,"Setting `GEMINI_API_KEY` as an environment variable creates potential security and concurrency issues in multi-tenant environments. Environment variables are process-wide and can leak across requests/tenants.

Pass the API key directly to litellm via the `api_key` parameter instead of using environment variables, similar to how other providers handle authentication.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/llm/multi_llm.py
Line: 159:160

Comment:
Setting `GEMINI_API_KEY` as an environment variable creates potential security and concurrency issues in multi-tenant environments. Environment variables are process-wide and can leak across requests/tenants.

Pass the API key directly to litellm via the `api_key` parameter instead of using environment variables, similar to how other providers handle authentication.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7709#discussion_r2722922580,,2026-01-23T21:35:25+00:00,security,high,"Greptile correctly identifies that setting GEMINI_API_KEY as a process-wide environment variable in a multi-tenant application creates a real security vulnerability where API keys from one tenant could leak to another during concurrent requests. This is a non-obvious issue since the same pattern exists for OLLAMA_API_BASE (though API bases are less sensitive than API keys), and the fix is actionable - litellm supports passing api_key directly as a parameter.",2026-01-23T22:14:38.748486+00:00
onyx-dot-app/onyx,7706,feat: onyx discord bot - supervisord and kube deployment,https://github.com/onyx-dot-app/onyx/pull/7706,,"Missing Helm deployment template for Discord bot. The `discordbot` values are defined here but there's no corresponding `deployment/helm/charts/onyx/templates/discordbot.yaml` file to actually deploy the Discord bot pod in Kubernetes. Compare with `slackbot.yaml` template which exists for the Slack bot configuration.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: deployment/helm/charts/onyx/values.yaml
Line: 707:741

Comment:
Missing Helm deployment template for Discord bot. The `discordbot` values are defined here but there's no corresponding `deployment/helm/charts/onyx/templates/discordbot.yaml` file to actually deploy the Discord bot pod in Kubernetes. Compare with `slackbot.yaml` template which exists for the Slack bot configuration.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7706#discussion_r2722758225,,2026-01-23T20:36:49+00:00,logic,high,"The diff shows new discordbot configuration values being added to values.yaml, but without a corresponding deployment template, these values are useless - the Discord bot would never actually be deployed to Kubernetes. This is a real functional bug that would cause the feature to silently not work, and it's non-obvious since the values.yaml changes look complete on their own.",2026-01-23T22:14:38.748522+00:00
onyx-dot-app/onyx,7706,feat: onyx discord bot - supervisord and kube deployment,https://github.com/onyx-dot-app/onyx/pull/7706,,"Missing `API_SERVER_PROTOCOL` and `API_SERVER_HOST` in configMap. These are required for the Discord bot to communicate with the API server (referenced in `backend/onyx/configs/app_configs.py`). The Docker Compose files set these, but they're missing from the Helm configMap section.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: deployment/helm/charts/onyx/values.yaml
Line: 1201:1204

Comment:
Missing `API_SERVER_PROTOCOL` and `API_SERVER_HOST` in configMap. These are required for the Discord bot to communicate with the API server (referenced in `backend/onyx/configs/app_configs.py`). The Docker Compose files set these, but they're missing from the Helm configMap section.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7706#discussion_r2722758289,,2026-01-23T20:36:51+00:00,logic,medium,"Greptile correctly identifies that the Discord bot configuration in the Helm values.yaml is missing API_SERVER_PROTOCOL and API_SERVER_HOST environment variables that are required for the bot to communicate with the API server. The diff shows DISCORD_BOT_TOKEN and DISCORD_BOT_INVOKE_CHAR were added but not the API server connection variables, which would cause the Discord bot deployment to fail to connect to the API. This is a real configuration bug that would cause runtime failures and is easy to miss when reviewing deployment configs.",2026-01-23T22:14:38.748552+00:00
democratizedspace/dspace,3062,Add category detail to inventory item cards,https://github.com/democratizedspace/dspace/pull/3062,,"**logic:** displaying `item.category` without checking if it exists will show ""undefined"" for items missing the category field (e.g., all items in `tools.json`)

```suggestion
                {#if item.category}
                    <p><strong>Category:</strong> {item.category}</p>
                {/if}
```",https://github.com/democratizedspace/dspace/pull/3062#discussion_r2723152838,,2026-01-23T23:14:16+00:00,logic,low,Greptile correctly identifies that displaying item.category without a null check will show 'undefined' in the UI for items that don't have a category field. This is a real bug that would cause poor user experience with literal 'undefined' text appearing. The suggestion to add a conditional check is correct and actionable.,2026-01-24T00:46:23.626706+00:00
nvidia/matx,1122,cuBLASDx support,https://github.com/NVIDIA/MatX/pull/1122,,"**logic:** For RANK &gt; 4, all batch dimensions are flattened into blocks.x only, ignoring blocks.y and blocks.z. This could exceed CUDA grid limits (65535) for large batch sizes.",https://github.com/NVIDIA/MatX/pull/1122#discussion_r2723158847,,2026-01-23T23:18:40+00:00,runtime,medium,"Greptile correctly identifies that for RANK > 4, the code flattens all batch dimensions into blocks.x only (line 336), while blocks.y and blocks.z remain 1. This could easily exceed CUDA's grid dimension limit of 65535 for blocks.x when total_batches is large, causing runtime failures. This is non-obvious because RANK 3-4 cases properly distribute across multiple grid dimensions, but the higher-rank fallback doesn't follow the same pattern.",2026-01-24T00:46:23.626765+00:00
nvidia/matx,1122,cuBLASDx support,https://github.com/NVIDIA/MatX/pull/1122,,"**logic:** Using `CUDA_VERSION` check but referencing `CUDART_VERSION` in the string. If `CUDA_VERSION` is undefined, `CUDART_VERSION` may also be undefined, potentially causing compilation issues.

```suggestion
#if defined(CUDART_VERSION)
        symbol_name += ""_CUDA"";
        symbol_name += std::to_string(CUDART_VERSION);
#else
        symbol_name += ""_CUDAUNKNOWN"";
#endif
```",https://github.com/NVIDIA/MatX/pull/1122#discussion_r2723159013,,2026-01-23T23:18:48+00:00,logic,low,"Greptile correctly identifies an inconsistency where the code checks `#if defined(CUDA_VERSION)` but then uses `CUDART_VERSION` in the string concatenation. While both macros are typically defined together in CUDA environments, this is technically inconsistent and could cause issues in edge cases where one is defined but not the other. The fix to check `CUDART_VERSION` directly is correct and makes the code more robust.",2026-01-24T00:46:23.626802+00:00
nvidia/matx,1122,cuBLASDx support,https://github.com/NVIDIA/MatX/pull/1122,,"**logic:** Output indexing logic assumes `threadIdx.x` maps directly to output elements, but this may not align with the 2D block dimensions suggested by cuBLASDx. The logic doesn't account for `threadIdx.y` or `threadIdx.z`. How should the output indexing work with 2D/3D block dimensions from cuBLASDx?",https://github.com/NVIDIA/MatX/pull/1122#discussion_r2723159027,,2026-01-23T23:18:48+00:00,logic,high,"Greptile correctly identifies a real bug: the output indexing code only uses `threadIdx.x` to compute `output_idx`, but cuBLASDx returns 2D/3D block dimensions via `GetBlockDim()`. This means threads with different `threadIdx.y` or `threadIdx.z` values would compute the same `output_idx`, causing incorrect/duplicate writes and missing elements in the output matrix. This is a non-obvious concurrency bug that would cause incorrect results.",2026-01-24T00:46:23.626841+00:00
ironyh/vuesip,178,feat(pwa-softphone): improve settings menu UX,https://github.com/ironyh/VueSIP/pull/178,,"**logic:** `AudioContext` is created but never closed, causing resource leak.

```suggestion
  try {
    isTestingSpeaker.value = true
    // Create a test tone
    const audioContext = new AudioContext()
    const oscillator = audioContext.createOscillator()
    const gainNode = audioContext.createGain()
    const destination = audioContext.destination

    oscillator.connect(gainNode)
    gainNode.connect(destination)

    oscillator.frequency.value = 440 // A4 note
    gainNode.gain.value = 0.1

    oscillator.start()
    oscillator.stop(audioContext.currentTime + 0.5)

    setTimeout(() => {
      audioContext.close()
      isTestingSpeaker.value = false
    }, 500)
  } catch (error) {
    console.error('Speaker test failed:', error)
    isTestingSpeaker.value = false
  }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: templates/pwa-softphone/src/components/SettingsMenu.vue
Line: 113:134

Comment:
**logic:** `AudioContext` is created but never closed, causing resource leak.

```suggestion
  try {
    isTestingSpeaker.value = true
    // Create a test tone
    const audioContext = new AudioContext()
    const oscillator = audioContext.createOscillator()
    const gainNode = audioContext.createGain()
    const destination = audioContext.destination

    oscillator.connect(gainNode)
    gainNode.connect(destination)

    oscillator.frequency.value = 440 // A4 note
    gainNode.gain.value = 0.1

    oscillator.start()
    oscillator.stop(audioContext.currentTime + 0.5)

    setTimeout(() => {
      audioContext.close()
      isTestingSpeaker.value = false
    }, 500)
  } catch (error) {
    console.error('Speaker test failed:', error)
    isTestingSpeaker.value = false
  }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ironyh/VueSIP/pull/178#discussion_r2723206020,,2026-01-23T23:51:32+00:00,resource_leak,medium,"Greptile correctly identified that the AudioContext created in testSpeaker() is never closed. Looking at the code, while the oscillator is stopped, audioContext.close() is never called, which will leak browser audio resources. Unlike the microphone test which has cleanup in onUnmounted, there's no cleanup for the AudioContext. The fix to call audioContext.close() in the setTimeout is correct and actionable.",2026-01-24T00:46:23.626878+00:00
onyx-dot-app/onyx,7736,fix(chat): Internal search enablement matches source enablement,https://github.com/onyx-dot-app/onyx/pull/7736,,"When toggling the search tool on, this doesn't persist the restored sources. If `previous.length > 0`, it uses `setSelectedSources(previous)` instead of `enableSources(previous)`. This means the sources won't be saved to localStorage.

Same issue when enabling all sources - `setSelectedSources(configuredSources)` doesn't persist.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/refresh-components/popovers/ActionsPopover/index.tsx
Line: 633:647

Comment:
When toggling the search tool on, this doesn't persist the restored sources. If `previous.length > 0`, it uses `setSelectedSources(previous)` instead of `enableSources(previous)`. This means the sources won't be saved to localStorage.

Same issue when enabling all sources - `setSelectedSources(configuredSources)` doesn't persist.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7736#discussion_r2723431214,,2026-01-24T02:12:02+00:00,data_integrity,medium,"Greptile correctly identified that `setSelectedSources(previous)` and `setSelectedSources(configuredSources)` in the `handleToggleTool` function don't persist to localStorage, while other similar operations use `enableSources()` or `enableAllSources()` which do persist. This is a real inconsistency that could cause user preferences to be lost on page refresh - a non-obvious bug that requires understanding the difference between the raw setter and the persistence-aware helper functions.",2026-01-24T02:51:50.544954+00:00
infisical/infisical,5262,feature: add Azure DNS as a provider for ACME,https://github.com/Infisical/infisical/pull/5262,,"**Security Issue: URL Injection in OAuth Token Endpoint**

The `tenantId` is directly interpolated into the Microsoft OAuth URL without validation. A malicious tenant ID could redirect the token request to an attacker-controlled server.

**Attack:** Setting `tenantId` to `evil.com/steal?x=` would create:
```
https://login.microsoftonline.com/evil.com/steal?x=/oauth2/v2.0/token
```
which resolves to `https://evil.com/steal?x=/oauth2/v2.0/token`, sending the client credentials to the attacker.

**Fix:** Validate `tenantId` format (should be a GUID).

```suggestion
    // Validate tenantId is a valid GUID
    if (!/^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(tenantId)) {
      throw new Error('Invalid tenantId format');
    }
    
    const tokenResponse = await request.post<{ access_token: string; expires_in: number }>(
```",https://github.com/Infisical/infisical/pull/5262#discussion_r2723487133,,2026-01-24T02:49:38+00:00,security,critical,This is a real security vulnerability - the tenantId is directly interpolated into the OAuth URL without validation. A malicious tenantId like 'evil.com/steal?x=' would indeed redirect the token request (containing client_id and client_secret) to an attacker-controlled server. This is a non-obvious SSRF/credential theft vulnerability that most reviewers would miss.,2026-01-24T04:26:43.832471+00:00
infisical/infisical,5262,feature: add Azure DNS as a provider for ACME,https://github.com/Infisical/infisical/pull/5262,,"**Security Issue: URL Injection via subscriptionId**

The `subscriptionId` is interpolated into the URL without validation. A malicious subscription ID could redirect to an attacker's server.

Validate that `subscriptionId` is a valid GUID before using it in URL construction.",https://github.com/Infisical/infisical/pull/5262#discussion_r2723487178,,2026-01-24T02:49:39+00:00,security,high,"The code directly interpolates `subscriptionId` into URLs at lines 81 and 144 without any validation. Unlike `hostedZoneId` which has GUID format validation via regex, a malicious `subscriptionId` like `../../../attacker.com/malicious` could potentially redirect requests to unintended servers (SSRF). This is a non-obvious security issue that warrants input validation similar to what exists for other Azure resource IDs in this file.",2026-01-24T04:26:43.832596+00:00
synnaxlabs/synnax,1873,SY-3596: Increase NI sample Rate Limit to 1 MHz,https://github.com/synnaxlabs/synnax/pull/1873,,"Lower bound should be `gt=0` instead of `ge=0` to match TypeScript validation which uses `.positive()` (excludes 0).

```suggestion
    sample_rate: conint(gt=0, le=1000000)
```",https://github.com/synnaxlabs/synnax/pull/1873#discussion_r2723502141,,2026-01-24T02:54:56+00:00,logic,medium,"Greptile correctly identifies an inconsistency - using `ge=0` allows a sample_rate of 0, which is semantically invalid for a hardware sampling rate (you can't sample at 0 Hz). The suggestion to use `gt=0` ensures the validation matches the logical requirement that sample rates must be positive, catching a potential edge case that could cause unexpected behavior in downstream NI hardware operations.",2026-01-24T04:26:43.832643+00:00
raycast/extensions,24818,Update myanimelist-search extension,https://github.com/raycast/extensions/pull/24818,,"**syntax:** The original `ViewType` type (`""list"" | ""list-detailed"" | ""grid""`) was removed and replaced with these new types. However, `ViewType` is still imported by `ViewTypeCtx.tsx` and `useSearch.ts`, which will cause build failures.

```suggestion
export type ViewType = ""list"" | ""list-detailed"" | ""grid"";

export type AnimeData = {
```",https://github.com/raycast/extensions/pull/24818#discussion_r2723636389,,2026-01-24T05:00:41+00:00,type_error,high,"Greptile correctly identified that the `ViewType` type export was removed from this file. If other files (`ViewTypeCtx.tsx` and `useSearch.ts`) still import `ViewType`, this will cause TypeScript compilation/build failures. This is a real breaking change that could easily be missed when focusing on the new type additions.",2026-01-24T06:17:54.304923+00:00
posthog/posthog,45865,chore(ux): remove sidepanel part 3 (Help) ,https://github.com/PostHog/posthog/pull/45865,,"**logic:** wrong URL for changelog (links to roadmap instead of changelog)

```suggestion
                            to=""https://posthog.com/changelog""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/components/HelpMenu/HelpMenu.tsx
Line: 153:153

Comment:
**logic:** wrong URL for changelog (links to roadmap instead of changelog)

```suggestion
                            to=""https://posthog.com/changelog""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45865#discussion_r2723829190,,2026-01-24T07:15:21+00:00,logic,low,"Greptile correctly identified that the Changelog menu item at line 153 links to 'https://posthog.com/roadmap' instead of 'https://posthog.com/changelog'. The label says 'Changelog' and tooltip says 'View our changelog' but it actually navigates to the roadmap page - this is a copy-paste error that would confuse users. While not critical, this is a real bug that would be easy to miss in a large UI component.",2026-01-24T08:16:51.897635+00:00
democratizedspace/dspace,3090,Add reusable EntitySelector and Item/Process pickers; wire into Quest editor,https://github.com/democratizedspace/dspace/pull/3090,,"Binding both `on:click` and `on:touchstart` to the same handler causes double-firing on touch devices (touchstart fires first, then click). Recommend using only `on:click` or add preventDefault to touchstart.

```suggestion
                            on:click={() => handleSelect(entry)}
```",https://github.com/democratizedspace/dspace/pull/3090#discussion_r2723847024,,2026-01-24T07:39:29+00:00,logic,medium,"Greptile correctly identifies that binding both `on:click` and `on:touchstart` to the same handler (handleSelect) will cause double-firing on touch devices - touchstart fires first, then the click event fires as well, causing the selection to happen twice. This pattern appears in multiple places in the code (lines 83, 119, 133) and could cause unexpected behavior or performance issues on mobile devices.",2026-01-24T08:16:51.897740+00:00
democratizedspace/dspace,3090,Add reusable EntitySelector and Item/Process pickers; wire into Quest editor,https://github.com/democratizedspace/dspace/pull/3090,,"Same issue - dual event handlers cause double-firing on touch devices.

```suggestion
                    on:click={toggleExpanded}
```",https://github.com/democratizedspace/dspace/pull/3090#discussion_r2723847033,,2026-01-24T07:39:30+00:00,logic,medium,"The code at line 127 has both `on:click={toggleExpanded}` and `on:touchstart={toggleExpanded}` handlers on the same button. On touch devices, both events fire - touchstart first, then the click event synthesized from the touch, causing toggleExpanded to be called twice and the expanded state to toggle back to its original value. This is a real bug that causes incorrect behavior on mobile devices and is non-obvious to typical reviewers.",2026-01-24T08:16:51.897785+00:00
democratizedspace/dspace,3090,Add reusable EntitySelector and Item/Process pickers; wire into Quest editor,https://github.com/democratizedspace/dspace/pull/3090,,"Same issue - dual event handlers cause double-firing on touch devices.

```suggestion
                on:click={toggleExpanded}
```",https://github.com/democratizedspace/dspace/pull/3090#discussion_r2723847042,,2026-01-24T07:39:31+00:00,logic,medium,"The code has both `on:click` and `on:touchstart` handlers calling `toggleExpanded()` on lines 138-139. On touch devices, both events fire in sequence (touchstart followed by click), causing the toggle to fire twice and effectively cancel itself out. This is a real bug that would cause the edit/select button to not work properly on mobile devices. The pattern appears multiple times in this file (lines 81-82, 119-120, 128-129).",2026-01-24T08:16:51.897822+00:00
democratizedspace/dspace,3088,Add shared CatalogPicker and wire item/process pickers into Quest + Process forms,https://github.com/democratizedspace/dspace/pull/3088,,"both `click` and `touchstart` handlers call the same function

This could cause the selection to trigger twice on touch devices (once for `touchstart`, once for `click`). Consider using `preventDefault()` in the touch handler or removing one:

```suggestion
                            on:click={() => handleItemSelect(item.id)}
```

Or handle touch properly:
```suggestion
                            on:click={() => handleItemSelect(item.id)}
                            on:touchstart={(e) => { e.preventDefault(); handleItemSelect(item.id); }}
```",https://github.com/democratizedspace/dspace/pull/3088#discussion_r2723840422,,2026-01-24T07:32:31+00:00,logic,medium,"This is a legitimate bug - on touch devices, both touchstart and click events fire in sequence, causing handleItemSelect to be called twice. This could lead to double event dispatches and unexpected behavior. The pattern appears multiple times in the file (lines 67, 107, 115) and is a non-obvious mobile-specific issue that typical reviewers might miss.",2026-01-24T08:16:51.897859+00:00
democratizedspace/dspace,3088,Add shared CatalogPicker and wire item/process pickers into Quest + Process forms,https://github.com/democratizedspace/dspace/pull/3088,,"both `click` and `touchstart` handlers call the same function

Same double-trigger issue as with the item selection buttons - touch devices will fire both events",https://github.com/democratizedspace/dspace/pull/3088#discussion_r2723840427,,2026-01-24T07:32:32+00:00,logic,medium,"Greptile correctly identifies a real bug where both `on:click` and `on:touchstart` handlers are bound to the same function (toggleExpanded) on the Edit button. On touch devices, both events fire in sequence, causing the toggle to fire twice and effectively cancel itself out - the expansion state toggles and immediately toggles back. This is a non-obvious cross-platform interaction bug that could cause confusing UX on mobile devices.",2026-01-24T08:16:51.897883+00:00
democratizedspace/dspace,3088,Add shared CatalogPicker and wire item/process pickers into Quest + Process forms,https://github.com/democratizedspace/dspace/pull/3088,,"both `click` and `touchstart` handlers call the same function

Same double-trigger issue - touch devices will fire both events",https://github.com/democratizedspace/dspace/pull/3088#discussion_r2723840453,,2026-01-24T07:32:33+00:00,logic,medium,"Greptile correctly identifies that having both `on:click` and `on:touchstart` handlers calling the same function (handleItemSelect, toggleExpanded) will cause double-triggering on touch devices. When a user taps, touchstart fires first, then the browser synthesizes a click event, causing the function to execute twice - this would toggle the expanded state back to its original value or select/deselect an item immediately.",2026-01-24T08:16:51.897902+00:00
democratizedspace/dspace,3076,Align legacy save upgrade copy between v1 and v2,https://github.com/democratizedspace/dspace/pull/3076,,"changing button text from ""Discard legacy v2 data"" to ""Delete v2 localStorage data"" breaks the test in `frontend/src/components/__tests__/LegacySaveUpgrade.spec.ts:162` which searches for `/discard legacy v2 data/i`

```suggestion
                                : 'Discard legacy v2 data'}
```",https://github.com/democratizedspace/dspace/pull/3076#discussion_r2723788272,,2026-01-24T06:21:22+00:00,logic,medium,"Greptile correctly identifies that changing the button text from 'Discard legacy v2 data' to 'Delete v2 localStorage data' will break a test that searches for the regex `/discard legacy v2 data/i`. This is a real bug that would cause test failures, and the connection between UI text changes and test assertions is non-obvious and easy to miss during code review.",2026-01-24T08:16:51.897922+00:00
joby-aviation/noodles.gl,288,feat(operators): add IconClusterLayerOp for Supercluster-based clustering,https://github.com/joby-aviation/noodles.gl/pull/288,,"**logic:** `getPointId` prop defined but never used in the operator inputs - this field isn't wired up in `createInputs()` so it won't be passed to the layer.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: noodles-editor/src/noodles/operators.ts
Line: 4175:4175

Comment:
**logic:** `getPointId` prop defined but never used in the operator inputs - this field isn't wired up in `createInputs()` so it won't be passed to the layer.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/joby-aviation/noodles.gl/pull/288#discussion_r2723849869,,2026-01-24T07:43:52+00:00,logic,medium,"Greptile correctly identified that `getPointId` is defined in the `IconClusterLayerProps` interface (line 4175) but is never added to `createInputs()` method, meaning the prop won't be configurable or passed to the layer. This is a real oversight that could affect the layer's functionality for identifying unique points, and it's non-obvious since it requires cross-referencing the interface definition with the input field setup.",2026-01-24T08:16:51.897943+00:00
joby-aviation/noodles.gl,288,feat(operators): add IconClusterLayerOp for Supercluster-based clustering,https://github.com/joby-aviation/noodles.gl/pull/288,,"**logic:** `IconClusterLayer` type doesn't exist in deck.gl or this codebase. Check that the custom layer is registered in deck.gl's layer registry before using, otherwise this will fail at runtime when deck.gl tries to instantiate it. Have you already implemented and registered the IconClusterLayer, or is this PR meant to be paired with another PR that adds the layer implementation?

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: noodles-editor/src/noodles/operators.ts
Line: 4238:4238

Comment:
**logic:** `IconClusterLayer` type doesn't exist in deck.gl or this codebase. Check that the custom layer is registered in deck.gl's layer registry before using, otherwise this will fail at runtime when deck.gl tries to instantiate it. Have you already implemented and registered the IconClusterLayer, or is this PR meant to be paired with another PR that adds the layer implementation?

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/joby-aviation/noodles.gl/pull/288#discussion_r2723849874,,2026-01-24T07:43:52+00:00,runtime,high,"Greptile correctly identifies that IconClusterLayerOp references 'IconClusterLayer' as a type string, but there's no implementation of this custom layer visible in the diff or codebase. The PR title mentions 'Supercluster-based clustering' suggesting a custom layer is needed, but without the layer implementation registered in deck.gl's layer registry, this operator will fail at runtime when deck.gl tries to instantiate the non-existent layer type. This is a non-obvious catch since the operator code itself looks complete.",2026-01-24T08:16:51.898006+00:00
twentyhq/twenty,17410,Do not delete archived messages,https://github.com/twentyhq/twenty/pull/17410,,"if `connectedAccount` is null (when no connected account is found for the workspace member), `connectedAccount?.id` will be `undefined`, and `findBy` will return all message channels (or possibly error)

```suggestion
        if (!connectedAccount) {
          return;
        }

        const relatedMessageChannels = await messageChannelRepository.findBy({
          connectedAccountId: connectedAccount.id,
        });
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/modules/connected-account/jobs/delete-workspace-member-connected-accounts.job.ts
Line: 54:56

Comment:
if `connectedAccount` is null (when no connected account is found for the workspace member), `connectedAccount?.id` will be `undefined`, and `findBy` will return all message channels (or possibly error)

```suggestion
        if (!connectedAccount) {
          return;
        }

        const relatedMessageChannels = await messageChannelRepository.findBy({
          connectedAccountId: connectedAccount.id,
        });
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17410#discussion_r2723992363,,2026-01-24T10:06:53+00:00,data_integrity,high,"Greptile correctly identifies that if `connectedAccount` is null, `connectedAccount?.id` evaluates to `undefined`, and passing `undefined` to `findBy({ connectedAccountId: undefined })` could return unintended results (all channels with null/undefined connectedAccountId, or potentially all channels depending on ORM behavior). This could lead to incorrectly disabling sync on message channels that shouldn't be affected. The null check is a necessary guard that prevents unintended data modifications.",2026-01-24T10:15:45.758908+00:00
ever-co/ever-teams,4229,Feat/solo,https://github.com/ever-co/ever-teams/pull/4229,,"Direct mutation of `formValues.team` bypasses React state management

```suggestion
			setFormValues(prev => ({ ...prev, team: generateDefaultTeamName(prev.name) }));
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/web/core/hooks/auth/use-authentication-team.ts
Line: 99:99

Comment:
Direct mutation of `formValues.team` bypasses React state management

```suggestion
			setFormValues(prev => ({ ...prev, team: generateDefaultTeamName(prev.name) }));
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-teams/pull/4229#discussion_r2724047633,,2026-01-24T11:08:51+00:00,data_integrity,medium,"Greptile correctly identified that `formValues.team = generateDefaultTeamName(formValues.name)` directly mutates React state instead of using `setFormValues`. This bypasses React's state management, which can cause the component to not re-render properly and lead to stale state issues. While experienced React developers might catch this, it's a subtle bug that could easily be missed in review, especially since the code appears to work initially but causes inconsistent behavior.",2026-01-24T12:18:50.086517+00:00
posthog/posthog,45873,feat(experiments): add search and pagination to shared metrics,https://github.com/PostHog/posthog/pull/45873,,"**syntax:** Missing import for `userLogic` - this will cause a runtime error

```suggestion
    import { useActions, useValues } from 'kea'
import { router } from 'kea-router'

import { IconCopy, IconPencil } from '@posthog/icons'
import {
    LemonBanner,
    LemonButton,
    LemonInput,
    LemonTable,
    LemonTableColumn,
    LemonTableColumns,
    LemonTag,
    Tooltip,
} from '@posthog/lemon-ui'

import { AppShortcut } from 'lib/components/AppShortcuts/AppShortcut'
import { keyBinds } from 'lib/components/AppShortcuts/shortcuts'
import { ObjectTags } from 'lib/components/ObjectTags/ObjectTags'
import { LemonTableLink } from 'lib/lemon-ui/LemonTable/LemonTableLink'
import { createdAtColumn, createdByColumn } from 'lib/lemon-ui/LemonTable/columnUtils'
import { pluralize } from 'lib/utils'
import stringWithWBR from 'lib/utils/stringWithWBR'
import { Scene, SceneExport } from 'scenes/sceneTypes'
import { urls } from 'scenes/urls'
import { userLogic } from 'scenes/userLogic'

import { NodeKind } from '~/queries/schema/schema-general'

import { isLegacySharedMetric } from '../utils'
import { SharedMetric } from './sharedMetricLogic'
import { SHARED_METRICS_PER_PAGE, sharedMetricsLogic } from './sharedMetricsLogic'
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/experiments/SharedMetrics/SharedMetrics.tsx
Line: 40:40

Comment:
**syntax:** Missing import for `userLogic` - this will cause a runtime error

```suggestion
    import { useActions, useValues } from 'kea'
import { router } from 'kea-router'

import { IconCopy, IconPencil } from '@posthog/icons'
import {
    LemonBanner,
    LemonButton,
    LemonInput,
    LemonTable,
    LemonTableColumn,
    LemonTableColumns,
    LemonTag,
    Tooltip,
} from '@posthog/lemon-ui'

import { AppShortcut } from 'lib/components/AppShortcuts/AppShortcut'
import { keyBinds } from 'lib/components/AppShortcuts/shortcuts'
import { ObjectTags } from 'lib/components/ObjectTags/ObjectTags'
import { LemonTableLink } from 'lib/lemon-ui/LemonTable/LemonTableLink'
import { createdAtColumn, createdByColumn } from 'lib/lemon-ui/LemonTable/columnUtils'
import { pluralize } from 'lib/utils'
import stringWithWBR from 'lib/utils/stringWithWBR'
import { Scene, SceneExport } from 'scenes/sceneTypes'
import { urls } from 'scenes/urls'
import { userLogic } from 'scenes/userLogic'

import { NodeKind } from '~/queries/schema/schema-general'

import { isLegacySharedMetric } from '../utils'
import { SharedMetric } from './sharedMetricLogic'
import { SHARED_METRICS_PER_PAGE, sharedMetricsLogic } from './sharedMetricsLogic'
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45873#discussion_r2724285214,,2026-01-24T15:56:46+00:00,runtime,high,"Greptile correctly identified that `userLogic` is used in line 40 (`const { hasAvailableFeature } = useValues(userLogic)`) but is never imported anywhere in the file. This would cause a runtime error (ReferenceError: userLogic is not defined) when the component renders, breaking the entire page.",2026-01-24T16:16:04.423526+00:00
cedarjs/cedar,1027,fix(cli): Use 'cedar' bin instead of deprecated 'rw' bin,https://github.com/cedarjs/cedar/pull/1027,,"<details>
<summary>Additional Comments (5)</summary>

**`packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js`**
Missed update from `rw` to `cedar` bin

```suggestion
    command: [""yarn"", ""cedar"", ""build"", ""api"", ""&&"", ""yarn"", ""cedar"", ""dev"", ""api"", ""--apiRootPath=/api""]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js
Line: 219:219

Comment:
Missed update from `rw` to `cedar` bin

```suggestion
    command: [""yarn"", ""cedar"", ""build"", ""api"", ""&&"", ""yarn"", ""cedar"", ""dev"", ""api"", ""--apiRootPath=/api""]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js`**
Missed update from `rw` to `cedar` bin in both commented and active migration commands

```suggestion
  # migration: [""yarn"", ""cedar"", ""prisma"", ""migrate"", ""deploy"", ""&&"", ""yarn"", ""cedar"", ""data-migrate"", ""up""]
  migration: [""yarn"", ""cedar"", ""prisma"", ""migrate"", ""deploy""]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js
Line: 235:236

Comment:
Missed update from `rw` to `cedar` bin in both commented and active migration commands

```suggestion
  # migration: [""yarn"", ""cedar"", ""prisma"", ""migrate"", ""deploy"", ""&&"", ""yarn"", ""cedar"", ""data-migrate"", ""up""]
  migration: [""yarn"", ""cedar"", ""prisma"", ""migrate"", ""deploy""]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js`**
Missed update from `rw` to `cedar` bin

```suggestion
    command: [""yarn"", ""cedar"", ""serve"", ""web""]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js
Line: 242:242

Comment:
Missed update from `rw` to `cedar` bin

```suggestion
    command: [""yarn"", ""cedar"", ""serve"", ""web""]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js`**
Missed update from `rw` to `cedar` bin

```suggestion
    command: [""yarn"", ""cedar"", ""dev"", ""web"", ""--fwd=\""--allowed-hosts all\""""]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js
Line: 244:244

Comment:
Missed update from `rw` to `cedar` bin

```suggestion
    command: [""yarn"", ""cedar"", ""dev"", ""web"", ""--fwd=\""--allowed-hosts all\""""]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js`**
Missed update from `rw` to `cedar` bin

```suggestion
  build: [""yarn"", ""cedar"", ""build"", ""web"", ""--no-prerender""]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/commands/setup/deploy/providers/coherenceHandler.js
Line: 248:248

Comment:
Missed update from `rw` to `cedar` bin

```suggestion
  build: [""yarn"", ""cedar"", ""build"", ""web"", ""--no-prerender""]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/cedarjs/cedar/pull/1027#issuecomment-3795060123,,2026-01-24T16:48:12+00:00,logic,medium,"Given the PR title explicitly states the goal is to replace 'rw' bin with 'cedar' bin, Greptile correctly identified multiple missed instances in coherenceHandler.js where the old 'rw' command was not updated, which would cause the deployment commands to fail or behave incorrectly. This is a real bug - incomplete refactoring that a reviewer could easily miss across a large codebase.",2026-01-24T18:18:13.478089+00:00
simstudioai/sim,2979,fix(landing): ui,https://github.com/simstudioai/sim/pull/2979,,"**logic:** hardcoding `isProd = true` breaks all development and test environments

```suggestion
export const isProd = env.NODE_ENV === 'production'
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/sim/lib/core/config/feature-flags.ts
Line: 9:9

Comment:
**logic:** hardcoding `isProd = true` breaks all development and test environments

```suggestion
export const isProd = env.NODE_ENV === 'production'
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/simstudioai/sim/pull/2979#discussion_r2724604247,,2026-01-24T20:54:48+00:00,logic,high,"The diff clearly shows `isProd` being changed from a dynamic check (`env.NODE_ENV === 'production'`) to a hardcoded `true` value. This would make all environments (development, test, staging) behave as if they were production, breaking environment-specific functionality like debug logging, feature flags, and safety checks. This is a real bug that could easily slip through in a UI-focused PR.",2026-01-24T22:14:38.862970+00:00
simstudioai/sim,2979,fix(landing): ui,https://github.com/simstudioai/sim/pull/2979,,"**logic:** hardcoding `isHosted = true` breaks self-hosted deployments and disables `isAuthDisabled` flag

```suggestion
export const isHosted =
  getEnv('NEXT_PUBLIC_APP_URL') === 'https://www.sim.ai' ||
  getEnv('NEXT_PUBLIC_APP_URL') === 'https://www.staging.sim.ai'
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/sim/lib/core/config/feature-flags.ts
Line: 24:24

Comment:
**logic:** hardcoding `isHosted = true` breaks self-hosted deployments and disables `isAuthDisabled` flag

```suggestion
export const isHosted =
  getEnv('NEXT_PUBLIC_APP_URL') === 'https://www.sim.ai' ||
  getEnv('NEXT_PUBLIC_APP_URL') === 'https://www.staging.sim.ai'
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/simstudioai/sim/pull/2979#discussion_r2724604253,,2026-01-24T20:54:49+00:00,logic,high,"Greptile correctly identifies that hardcoding `isHosted = true` would break self-hosted deployments since this flag likely gates hosted-specific features like authentication requirements. The original logic properly detected hosted environments via URL comparison, and removing this breaks the deployment model for self-hosted users.",2026-01-24T22:14:38.863056+00:00
simstudioai/sim,2979,fix(landing): ui,https://github.com/simstudioai/sim/pull/2979,,"**logic:** simplified label splitting logic loses flexibility

The current implementation always splits on the first colon, which means labels like `""Time: 09:00AM: Daily""` would split incorrectly. Consider using `split(':', 2)` pattern or limit to first occurrence only.

```suggestion
  const colonIndex = label.indexOf(':')
  const [title, displayValue] = colonIndex !== -1
    ? [label.slice(0, colonIndex).trim(), label.slice(colonIndex + 1).trim()]
    : [label, value]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/sim/app/(landing)/components/hero/components/landing-canvas/landing-block/tag.tsx
Line: 26:30

Comment:
**logic:** simplified label splitting logic loses flexibility

The current implementation always splits on the first colon, which means labels like `""Time: 09:00AM: Daily""` would split incorrectly. Consider using `split(':', 2)` pattern or limit to first occurrence only.

```suggestion
  const colonIndex = label.indexOf(':')
  const [title, displayValue] = colonIndex !== -1
    ? [label.slice(0, colonIndex).trim(), label.slice(colonIndex + 1).trim()]
    : [label, value]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/simstudioai/sim/pull/2979#discussion_r2724607956,,2026-01-24T21:01:41+00:00,logic,medium,"Greptile correctly identified that `label.split(':')` will split on ALL colons, so a label like 'Time: 09:00AM' would incorrectly parse as title='Time' and displayValue='09' instead of 'Time' and '09:00AM'. The suggested fix using indexOf and slice properly handles only the first colon occurrence. This is a non-obvious edge case bug that could cause incorrect display of time values or other colon-containing data.",2026-01-24T22:14:38.863105+00:00
bstewart2255/speddy,560,Add Attendance tab to student details modal,https://github.com/bstewart2255/speddy/pull/560,,"Missing authorization check - verify user has access to this student's data. The endpoint only checks if a user is authenticated, but doesn't verify they're authorized to view this specific student's attendance. The RLS policy on the `attendance` table only checks session access (lines 33-45 in the migration), but there's no explicit check here that the user has access to this `studentId`. Consider adding a check that the user is associated with this student through sessions, similar to how `attendance/summary/route.ts` filters sessions by `provider_id`, `assigned_to_specialist_id`, or `assigned_to_sea_id`.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: app/api/students/[studentId]/attendance/route.ts
Line: 22:45

Comment:
Missing authorization check - verify user has access to this student's data. The endpoint only checks if a user is authenticated, but doesn't verify they're authorized to view this specific student's attendance. The RLS policy on the `attendance` table only checks session access (lines 33-45 in the migration), but there's no explicit check here that the user has access to this `studentId`. Consider adding a check that the user is associated with this student through sessions, similar to how `attendance/summary/route.ts` filters sessions by `provider_id`, `assigned_to_specialist_id`, or `assigned_to_sea_id`.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/bstewart2255/speddy/pull/560#discussion_r2724718775,,2026-01-25T00:04:30+00:00,security,high,"The code only checks if a user is authenticated (line 22-24) but never verifies if that user has permission to access this specific student's attendance data. Any authenticated user could access any student's attendance records by manipulating the studentId parameter, which is a serious authorization bypass vulnerability. The comment correctly identifies this as an Insecure Direct Object Reference (IDOR) issue.",2026-01-25T00:48:28.607100+00:00
davis7dotsh/better-context,119,feat(cli): add input history with up/down arrow navigation,https://github.com/davis7dotsh/better-context/pull/119,,"history navigation not reset when user types

if user navigates history with up arrow, then starts typing, then presses down arrow, navigation continues from wrong index. need to call `inputHistory.reset()` when input changes (e.g. in `setInputState` or when user types)

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/cli/src/tui/components/input-section.tsx
Line: 194:202

Comment:
history navigation not reset when user types

if user navigates history with up arrow, then starts typing, then presses down arrow, navigation continues from wrong index. need to call `inputHistory.reset()` when input changes (e.g. in `setInputState` or when user types)

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/davis7dotsh/better-context/pull/119#discussion_r2724706420,,2026-01-24T23:40:41+00:00,logic,medium,"Greptile correctly identifies a UX bug: when the user navigates history with up/down arrows, the history index is tracked, but if they then type new content and press down arrow, the navigation will continue from the old index rather than resetting. The code only calls `inputHistory.reset()` on Ctrl+C clear, but not when the user types or modifies input through normal means. This is a non-obvious state management issue that would cause confusing behavior.",2026-01-25T00:48:28.607194+00:00
nvidia/transformerengine,2622,[PyTorch] Add grouped linear op and experimental fusion for grouped MLP,https://github.com/NVIDIA/TransformerEngine/pull/2622,,"`group_idx` is undefined in this scope - loop variable is `w, quantizer` from the zip on line 415. Should use `quantizer` directly (already assigned) or add enumeration to the loop.

```suggestion
                # quantizer is already assigned from the zip, use it directly
```",https://github.com/NVIDIA/TransformerEngine/pull/2622#discussion_r2724749047,,2026-01-25T01:03:27+00:00,runtime,high,"Looking at lines 415-419, the loop iterates with `for w, quantizer in zip(weights, weight_quantizers):` but inside the loop at line 417, the code references `group_idx` which is not defined in this loop scope. The previous loop ended at a different point, so `group_idx` would have a stale value from a prior loop, causing incorrect quantizer assignment. This is a real bug that would cause incorrect behavior and is non-obvious because it requires tracking variable scoping across multiple loops.",2026-01-25T03:02:02.388560+00:00
nvidia/transformerengine,2622,[PyTorch] Add grouped linear op and experimental fusion for grouped MLP,https://github.com/NVIDIA/TransformerEngine/pull/2622,,"checking `fc1.in_features % 256 != 0` twice instead of checking both `in_features` and `out_features`

```suggestion
        if fc1.in_features % 256 != 0 or fc1.out_features % 256 != 0:
```",https://github.com/NVIDIA/TransformerEngine/pull/2622#discussion_r2724749061,,2026-01-25T01:03:28+00:00,logic,medium,"Greptile correctly identified a copy-paste bug on line 68: the condition checks `fc1.in_features % 256 != 0` twice instead of checking both `in_features` and `out_features`. This would allow invalid FC1 configurations where `out_features` is not divisible by 256 to pass validation, potentially causing runtime errors in the fused kernel. The same bug appears on line 72 for FC2.",2026-01-25T03:02:02.388625+00:00
nvidia/transformerengine,2622,[PyTorch] Add grouped linear op and experimental fusion for grouped MLP,https://github.com/NVIDIA/TransformerEngine/pull/2622,,"checking `fc2.in_features % 256 != 0` twice instead of checking both `in_features` and `out_features`

```suggestion
        if fc2.in_features % 256 != 0 or fc2.out_features % 256 != 0:
```",https://github.com/NVIDIA/TransformerEngine/pull/2622#discussion_r2724749067,,2026-01-25T01:03:29+00:00,logic,medium,"Greptile correctly identified a copy-paste bug on line 73 where `fc2.in_features % 256 != 0` is checked twice instead of checking both `fc2.in_features` and `fc2.out_features`. This is a real validation bug that would allow invalid FC2 configurations to pass validation when `out_features` isn't divisible by 256, potentially causing runtime errors or incorrect behavior. The same pattern is correctly implemented for fc1 on line 68, making this an easy-to-miss typo.",2026-01-25T03:02:02.388666+00:00
nvidia/transformerengine,2622,[PyTorch] Add grouped linear op and experimental fusion for grouped MLP,https://github.com/NVIDIA/TransformerEngine/pull/2622,,"`group_idx` is undefined in this scope - loop uses `w, quantizer` from zip. Should use `quantizer` directly

```suggestion
                quantizer.set_usage(rowwise=True, columnwise=input_requires_grad)
```",https://github.com/NVIDIA/TransformerEngine/pull/2622#discussion_r2724749740,,2026-01-25T01:05:19+00:00,logic,medium,"Greptile correctly identified a bug at line 419. The code iterates with `for w, quantizer in zip(weights, weight_quantizers)` but then uses `weight_quantizers[group_idx]` where `group_idx` is not defined in this loop scope - it was last defined in a previous loop (line 377). This would cause incorrect quantizer selection, using the last value of `group_idx` from the previous loop instead of the current iteration's quantizer. The fix to use `quantizer` directly is correct.",2026-01-25T03:02:02.388703+00:00
nvidia/transformerengine,2622,[PyTorch] Add grouped linear op and experimental fusion for grouped MLP,https://github.com/NVIDIA/TransformerEngine/pull/2622,,"duplicate condition checks `fc1.in_features % 256` twice instead of checking `fc1.out_features`

```suggestion
        if fc1.in_features % 256 != 0 or fc1.out_features % 256 != 0:
            raise ValueError(
                f""Unsupported dims for FC1 (group_size={fc1.group_size}, ""
                f""in_features={fc1.in_features}, out_features={fc1.out_features}).""
            )
```",https://github.com/NVIDIA/TransformerEngine/pull/2622#discussion_r2724749742,,2026-01-25T01:05:20+00:00,logic,medium,"Greptile correctly identified that line 72 checks `fc1.in_features % 256 != 0` twice instead of checking both `fc1.in_features` and `fc1.out_features`. This is a copy-paste bug that would allow invalid configurations to pass validation when `fc1.out_features` is not divisible by 256, potentially causing runtime failures or incorrect behavior in the fused kernel.",2026-01-25T03:02:02.388747+00:00
nvidia/transformerengine,2622,[PyTorch] Add grouped linear op and experimental fusion for grouped MLP,https://github.com/NVIDIA/TransformerEngine/pull/2622,,"duplicate condition checks `fc2.in_features % 256` twice instead of checking `fc2.out_features`

```suggestion
        if fc2.in_features % 256 != 0 or fc2.out_features % 256 != 0:
            raise ValueError(
                f""Unsupported dims for FC2 (group_size={fc2.group_size}, ""
                f""in_features={fc2.in_features}, out_features={fc2.out_features}).""
            )
```",https://github.com/NVIDIA/TransformerEngine/pull/2622#discussion_r2724749746,,2026-01-25T01:05:21+00:00,logic,medium,"Greptile correctly identified a copy-paste bug on line 77 where the condition checks `fc2.in_features % 256 != 0` twice instead of checking both `fc2.in_features` and `fc2.out_features`. This would allow invalid configurations with `fc2.out_features` not divisible by 256 to pass validation, potentially causing runtime errors or incorrect behavior in the fused kernel. The same bug exists on line 74 for fc1, which checks `fc1.in_features` twice instead of both dimensions.",2026-01-25T03:02:02.388783+00:00
browserbase/stagehand,1607,feat(cli): add network capture to filesystem for agent inspection,https://github.com/browserbase/stagehand/pull/1607,,"Response status and headers from `Network.responseReceived` are being lost. The `responseData` object is initialized with default values (`status: 0`, empty headers) but never populated with the actual response info that was stored in the `Network.responseReceived` handler.

The `responseReceived` handler stores response data in `params._responseInfo` (line 240), but this `loadingFinished` handler doesn't retrieve it. This means all saved `response.json` files will have `status: 0` and empty headers/mimeType instead of the actual HTTP response data.

```suggestion
      const startTime = requestStartTimes.get(params.requestId) || Date.now();
      const duration = Date.now() - startTime;

      // Try to get response body
      let responseData = {
        id: params.requestId,
        status: 0,
        statusText: """",
        headers: {} as Record<string, string>,
        mimeType: """",
        body: null as string | null,
        duration,
      };
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/index.ts
Line: 267:275

Comment:
Response status and headers from `Network.responseReceived` are being lost. The `responseData` object is initialized with default values (`status: 0`, empty headers) but never populated with the actual response info that was stored in the `Network.responseReceived` handler.

The `responseReceived` handler stores response data in `params._responseInfo` (line 240), but this `loadingFinished` handler doesn't retrieve it. This means all saved `response.json` files will have `status: 0` and empty headers/mimeType instead of the actual HTTP response data.

```suggestion
      const startTime = requestStartTimes.get(params.requestId) || Date.now();
      const duration = Date.now() - startTime;

      // Try to get response body
      let responseData = {
        id: params.requestId,
        status: 0,
        statusText: """",
        headers: {} as Record<string, string>,
        mimeType: """",
        body: null as string | null,
        duration,
      };
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/browserbase/stagehand/pull/1607#discussion_r2724796833,,2026-01-25T02:46:07+00:00,data_integrity,medium,"Greptile correctly identified that the `Network.responseReceived` handler stores response info in `params._responseInfo` (lines 236-240), but the `Network.loadingFinished` handler creates a new `responseData` object with default values (status: 0, empty headers) and never retrieves the stored response info. This results in all captured network responses losing their actual HTTP status codes, headers, and mimeType - a real bug that would cause incorrect data to be written to the filesystem.",2026-01-25T03:02:02.388807+00:00
browserbase/stagehand,1607,feat(cli): add network capture to filesystem for agent inspection,https://github.com/browserbase/stagehand/pull/1607,,"<details>
<summary>Additional Comments (1)</summary>

**`packages/cli/src/index.ts`**
Network capture directory is not cleaned up on daemon shutdown. The `cleanupStaleFiles` function removes socket, PID, and WebSocket files but doesn't remove the network capture directory created by `getNetworkDir()`. This could accumulate temp directories over time.

```suggestion
async function cleanupStaleFiles(session: string): Promise<void> {
  try { await fs.unlink(getSocketPath(session)); } catch {}
  try { await fs.unlink(getPidPath(session)); } catch {}
  try { await fs.unlink(getWsPath(session)); } catch {}
  try { await fs.unlink(getChromePidPath(session)); } catch {}
  try { await fs.rm(getNetworkDir(session), { recursive: true }); } catch {}
}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/index.ts
Line: 66:71

Comment:
Network capture directory is not cleaned up on daemon shutdown. The `cleanupStaleFiles` function removes socket, PID, and WebSocket files but doesn't remove the network capture directory created by `getNetworkDir()`. This could accumulate temp directories over time.

```suggestion
async function cleanupStaleFiles(session: string): Promise<void> {
  try { await fs.unlink(getSocketPath(session)); } catch {}
  try { await fs.unlink(getPidPath(session)); } catch {}
  try { await fs.unlink(getWsPath(session)); } catch {}
  try { await fs.unlink(getChromePidPath(session)); } catch {}
  try { await fs.rm(getNetworkDir(session), { recursive: true }); } catch {}
}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/browserbase/stagehand/pull/1607#issuecomment-3795862567,,2026-01-25T02:46:13+00:00,resource_leak,medium,"This is a valid resource leak issue - the PR adds network capture functionality that creates directories via getNetworkDir(), but the cleanupStaleFiles function doesn't clean up these directories. Without the diff visible, Greptile's analysis is logically sound: if a new directory is created for network capture but not cleaned up during shutdown, temp directories will accumulate over time. The fix is specific and actionable.",2026-01-25T03:02:02.388891+00:00
simstudioai/sim,2988,fix(anthropic): use anthropic sdk to transform malformed response schemas to anthropic format,https://github.com/simstudioai/sim/pull/2988,,"Adding `@anthropic-ai/sdk@0.71.2` to root `package.json` creates a version conflict with `apps/sim/package.json` which already has `^0.39.0`. The `transformJSONSchema` import is from `@anthropic-ai/sdk/lib/transform-json-schema`, which may not exist in the older version used by the sim app. 

Update `apps/sim/package.json` to use the same version instead:

```suggestion
  ""dependencies"": {
    ""@anthropic-ai/sdk"": ""0.71.2""
  }
```

And remove this root-level dependency since this is a monorepo and the dependency should live in the workspace package that uses it.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: package.json
Line: 55:57

Comment:
Adding `@anthropic-ai/sdk@0.71.2` to root `package.json` creates a version conflict with `apps/sim/package.json` which already has `^0.39.0`. The `transformJSONSchema` import is from `@anthropic-ai/sdk/lib/transform-json-schema`, which may not exist in the older version used by the sim app. 

Update `apps/sim/package.json` to use the same version instead:

```suggestion
  ""dependencies"": {
    ""@anthropic-ai/sdk"": ""0.71.2""
  }
```

And remove this root-level dependency since this is a monorepo and the dependency should live in the workspace package that uses it.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/simstudioai/sim/pull/2988#discussion_r2724838264,,2026-01-25T03:50:47+00:00,runtime,high,Greptile correctly identifies a real version conflict issue in a monorepo setup - adding @anthropic-ai/sdk@0.71.2 to root package.json while apps/sim/package.json has ^0.39.0 could cause runtime errors since transformJSONSchema API may not exist in older versions. This is a non-obvious dependency management issue that could cause build/runtime failures.,2026-01-25T04:37:00.096461+00:00
simstudioai/sim,2988,fix(anthropic): use anthropic sdk to transform malformed response schemas to anthropic format,https://github.com/simstudioai/sim/pull/2988,,"The `transformJSONSchema` function is imported from `@anthropic-ai/sdk/lib/transform-json-schema`, which is an internal SDK path. This import will fail because `apps/sim/package.json` specifies `@anthropic-ai/sdk@^0.39.0`, but version `0.71.2` was added to the root `package.json`. In a monorepo, the workspace dependency takes precedence, so this import will likely resolve to `0.39.0` which may not have this function.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/sim/providers/anthropic/index.ts
Line: 2:2

Comment:
The `transformJSONSchema` function is imported from `@anthropic-ai/sdk/lib/transform-json-schema`, which is an internal SDK path. This import will fail because `apps/sim/package.json` specifies `@anthropic-ai/sdk@^0.39.0`, but version `0.71.2` was added to the root `package.json`. In a monorepo, the workspace dependency takes precedence, so this import will likely resolve to `0.39.0` which may not have this function.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/simstudioai/sim/pull/2988#discussion_r2724838268,,2026-01-25T03:50:48+00:00,runtime,high,"Greptile correctly identifies a potential runtime import failure due to version mismatch in a monorepo setup. The code imports from an internal SDK path that may not exist in version 0.39.0 specified in apps/sim/package.json, while the function likely only exists in the newer 0.71.2 version added to root package.json. This is a non-obvious dependency resolution issue that could cause the application to fail at runtime.",2026-01-25T04:37:00.096565+00:00
alpaca-network/gatewayz-frontend,900,feat(auth): add Terragon auth redirect page,https://github.com/Alpaca-Network/gatewayz-frontend/pull/900,,"potential open redirect vulnerability - validate callback URL origin

The `callback` parameter is used directly to construct the redirect URL without validating its origin. An attacker could provide a malicious URL like `https://evil.com` to redirect users to a phishing site with a valid auth token.

```suggestion
          // Validate callback URL is a trusted Terragon domain
          const callbackUrl = new URL(callback);
          const allowedOrigins = process.env.NEXT_PUBLIC_TERRAGON_URL 
            ? [new URL(process.env.NEXT_PUBLIC_TERRAGON_URL).origin]
            : ['https://terragon.alpaca-network.io']; // fallback to production domain
          
          if (!allowedOrigins.includes(callbackUrl.origin)) {
            throw new Error(""Invalid callback URL - must be from Terragon domain"");
          }
          callbackUrl.searchParams.set(""gwauth"", token);
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/app/auth/terragon/page.tsx
Line: 89:89

Comment:
potential open redirect vulnerability - validate callback URL origin

The `callback` parameter is used directly to construct the redirect URL without validating its origin. An attacker could provide a malicious URL like `https://evil.com` to redirect users to a phishing site with a valid auth token.

```suggestion
          // Validate callback URL is a trusted Terragon domain
          const callbackUrl = new URL(callback);
          const allowedOrigins = process.env.NEXT_PUBLIC_TERRAGON_URL 
            ? [new URL(process.env.NEXT_PUBLIC_TERRAGON_URL).origin]
            : ['https://terragon.alpaca-network.io']; // fallback to production domain
          
          if (!allowedOrigins.includes(callbackUrl.origin)) {
            throw new Error(""Invalid callback URL - must be from Terragon domain"");
          }
          callbackUrl.searchParams.set(""gwauth"", token);
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/900#discussion_r2724868093,,2026-01-25T04:12:57+00:00,security,critical,"Greptile correctly identified an open redirect vulnerability. The code at line 90 takes the `callback` query parameter directly from user input and uses it to construct a redirect URL with an auth token appended. An attacker could craft a malicious URL like `?callback=https://evil.com` to steal the auth token via the redirect, enabling account takeover or phishing. This is a real security issue that requires origin validation.",2026-01-25T04:37:00.096647+00:00
runanywhereai/runanywhere-sdks,293,feat: Add tool calling support with C++ ,https://github.com/RunanywhereAI/runanywhere-sdks/pull/293,,"Escape sequence detection `json[i-1] != '\\'` doesn't handle multiple consecutive backslashes correctly. For example, `""\\""` (escaped backslash followed by quote) would be incorrectly treated as escaped quote.

Standard approach: track escape state separately or count consecutive backslashes.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: sdk/runanywhere-react-native/packages/core/cpp/bridges/ToolCallingBridge.cpp
Line: 57:57

Comment:
Escape sequence detection `json[i-1] != '\\'` doesn't handle multiple consecutive backslashes correctly. For example, `""\\""` (escaped backslash followed by quote) would be incorrectly treated as escaped quote.

Standard approach: track escape state separately or count consecutive backslashes.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RunanywhereAI/runanywhere-sdks/pull/293#discussion_r2724818454,,2026-01-25T03:30:02+00:00,logic,medium,"Greptile correctly identified that checking `json[i-1] != '\\'` doesn't properly handle cases like `\\""` where an escaped backslash precedes a quote character. In this case, the quote should end the string, but the code would incorrectly treat it as escaped. This is a subtle parsing bug that could cause incorrect JSON processing, and it's non-obvious enough that typical reviewers would miss it.",2026-01-25T04:37:00.096711+00:00
runanywhereai/runanywhere-sdks,293,feat: Add tool calling support with C++ ,https://github.com/RunanywhereAI/runanywhere-sdks/pull/293,,"Same escape detection issue as line 57 - doesn't handle `""\\""` correctly (escaped backslash before quote).

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: sdk/runanywhere-react-native/packages/core/cpp/bridges/ToolCallingBridge.cpp
Line: 140:140

Comment:
Same escape detection issue as line 57 - doesn't handle `""\\""` correctly (escaped backslash before quote).

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RunanywhereAI/runanywhere-sdks/pull/293#discussion_r2724818479,,2026-01-25T03:30:04+00:00,logic,medium,"At line 140, the code checks `normalizedJson[valueEnd - 1] != '\\'` to detect escaped quotes, but this fails for the input `""\\""` where a backslash is itself escaped - the quote after `\\` is NOT escaped but the check would incorrectly think it is. This is a real parsing bug that could cause incorrect JSON value extraction, particularly for strings containing escaped backslashes followed by quotes.",2026-01-25T04:37:00.096775+00:00
nvidia/nvflare,4036,Add server-side memory management with server_memory_gc_rounds parameter,https://github.com/NVIDIA/NVFlare/pull/4036,,"Missing validation for `server_memory_gc_rounds` parameter. Other similar parameters like `persist_every_n_rounds` and `snapshot_every_n_rounds` are validated using `_check_non_neg_int()` on lines 112-113, but `server_memory_gc_rounds` is not validated.

```suggestion
        server_memory_gc_rounds: int = 1,
    ):
        """"""The controller for ScatterAndGather Workflow.

        The ScatterAndGather workflow defines FederatedAveraging on all clients.
        The model persistor (persistor_id) is used to load the initial global model which is sent to all clients.
        Each client sends it's updated weights after local training which is aggregated (aggregator_id). The
        shareable generator is used to convert the aggregated weights to shareable and shareable back to weight.
        The model_persistor also saves the model after training.

        Args:
            min_clients (int, optional): The minimum number of clients responses before
                SAG starts to wait for `wait_time_after_min_received`. Note that SAG will move forward when all
                available clients have responded regardless of this value. Defaults to 1000.
            num_rounds (int, optional): The total number of training rounds. Defaults to 5.
            start_round (int, optional): Start round for training. Defaults to 0.
            wait_time_after_min_received (int, optional): Time to wait before beginning aggregation after
                minimum number of clients responses has been received. Defaults to 10.
            aggregator_id (str, optional): ID of the aggregator component. Defaults to ""aggregator"".
            persistor_id (str, optional): ID of the persistor component. Defaults to """".
            shareable_generator_id (str, optional): ID of the shareable generator. Defaults to ""shareable_generator"".
            train_task_name (str, optional): Name of the train task. Defaults to ""train"".
            train_timeout (int, optional): Time to wait for clients to do local training.
            ignore_result_error (bool, optional): whether this controller can proceed if client result has errors.
                Defaults to False.
            allow_empty_global_weights (bool, optional): whether to allow empty global weights. Some pipelines can have
                empty global weights at first round, such that clients start training from scratch without any global info.
                Defaults to False.
            task_check_period (float, optional): interval for checking status of tasks. Defaults to 0.5.
            persist_every_n_rounds (int, optional): persist the global model every n rounds. Defaults to 1.
                If n is 0 then no persist.
            snapshot_every_n_rounds (int, optional): persist the server state every n rounds. Defaults to 1.
                If n is 0 then no persist.
            server_memory_gc_rounds (int, optional): Run memory cleanup (gc.collect + malloc_trim) every N rounds.
                Set to 0 to disable. Defaults to 1 (every round).

        Raises:
            TypeError: when any of input arguments does not have correct type
            ValueError: when any of input arguments is out of range
        """"""
        super().__init__(task_check_period=task_check_period)

        # Check arguments
        if not isinstance(min_clients, int):
            raise TypeError(""min_clients must be int but got {}"".format(type(min_clients)))
        elif min_clients <= 0:
            raise ValueError(""min_clients must be greater than 0."")

        _check_non_neg_int(num_rounds, ""num_rounds"")
        _check_non_neg_int(start_round, ""start_round"")
        _check_non_neg_int(wait_time_after_min_received, ""wait_time_after_min_received"")
        _check_non_neg_int(train_timeout, ""train_timeout"")
        _check_non_neg_int(persist_every_n_rounds, ""persist_every_n_rounds"")
        _check_non_neg_int(snapshot_every_n_rounds, ""snapshot_every_n_rounds"")
        _check_non_neg_int(server_memory_gc_rounds, ""server_memory_gc_rounds"")
```",https://github.com/NVIDIA/NVFlare/pull/4036#discussion_r2724987057,,2026-01-25T06:29:07+00:00,runtime,medium,"Greptile correctly identifies that `server_memory_gc_rounds` lacks validation while similar parameters `persist_every_n_rounds` and `snapshot_every_n_rounds` are validated with `_check_non_neg_int()`. Without validation, passing a negative value or non-integer would cause unexpected behavior in the `_maybe_cleanup_memory()` method's modulo operation. This is non-obvious consistency issue that maintains code quality and prevents potential runtime errors.",2026-01-25T08:21:08.032208+00:00
alpaca-network/gatewayz-backend,923,fix(credits): fallback to legacy credits field for tiered display,https://github.com/Alpaca-Network/gatewayz-backend/pull/923,,"this fallback fixes the **display** issue, but users with legacy credits still can't **use** them for API calls. the `deduct_credits` function (line 696) only reads from `subscription_allowance` and `purchased_credits`, ignoring the legacy `credits` field entirely.

if there are users who haven't been migrated yet, they'll see credits in the UI but get ""Insufficient credits"" errors when trying to use the API.

consider either:
1. ensure the migration script runs for all remaining users, or
2. also add fallback logic in `deduct_credits` to read from legacy `credits` when both tiered fields are 0

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/db/users.py
Line: 1638:1644

Comment:
this fallback fixes the **display** issue, but users with legacy credits still can't **use** them for API calls. the `deduct_credits` function (line 696) only reads from `subscription_allowance` and `purchased_credits`, ignoring the legacy `credits` field entirely.

if there are users who haven't been migrated yet, they'll see credits in the UI but get ""Insufficient credits"" errors when trying to use the API.

consider either:
1. ensure the migration script runs for all remaining users, or
2. also add fallback logic in `deduct_credits` to read from legacy `credits` when both tiered fields are 0

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-backend/pull/923#discussion_r2724981121,,2026-01-25T06:19:20+00:00,logic,high,"Greptile correctly identifies a critical inconsistency: the PR adds fallback logic to display legacy credits in the UI, but the `deduct_credits` function at line 696 (not shown in diff but referenced) likely doesn't have matching fallback logic. This would cause users to see credits they can't actually spend - a frustrating UX bug that creates a display/functionality mismatch. This is non-obvious because it requires understanding the relationship between display and deduction logic across different parts of the codebase.",2026-01-25T08:21:08.032377+00:00
democratizedspace/dspace,3134,feat: Add new v3 images,https://github.com/democratizedspace/dspace/pull/3134,,"Filename typo: comma instead of period - should be `restored_sponge_filter_flow_bubbling.json`

```suggestion
// File should be renamed to: restored_sponge_filter_flow_bubbling.json
```",https://github.com/democratizedspace/dspace/pull/3134#discussion_r2725067801,,2026-01-25T08:01:47+00:00,runtime,medium,"The filename clearly contains a comma instead of a period ('restored_sponge_filter_flow_bubbling,json' vs '.json'), which would cause file loading failures since the file wouldn't be recognized as a JSON file and any code trying to reference it with the correct extension would fail. This typo is easy to miss in a PR review but would cause real runtime issues.",2026-01-25T08:21:08.032420+00:00
democratizedspace/dspace,3106,Split item price into numeric amount + currency dropdown,https://github.com/democratizedspace/dspace/pull/3106,,"E2e tests in `custom-content.spec.ts` still use `await page.fill('#price', '100 dUSD')` which will fail since the `#price` selector no longer exists - it's been replaced with `#price-amount` and `#price-currency`.",https://github.com/democratizedspace/dspace/pull/3106#discussion_r2724985455,,2026-01-25T06:26:16+00:00,runtime,medium,"The code diff clearly shows the `#price` input was replaced with `#price-amount` and `#price-currency` inputs. Greptile correctly identifies that E2E tests referencing the old `#price` selector will fail. While I cannot see the test file itself, this is a concrete, actionable catch about breaking test compatibility that a typical reviewer might miss when focused on the component changes.",2026-01-25T08:21:08.032438+00:00
democratizedspace/dspace,3109,Split custom item price into amount + currency selector,https://github.com/democratizedspace/dspace/pull/3109,,"`priceAmount.trim()` will cause a runtime error when `priceAmount` is a number from the `type=""number""` input

```suggestion
        if (String(priceAmount || '').trim()) {
```",https://github.com/democratizedspace/dspace/pull/3109#discussion_r2724988564,,2026-01-25T06:32:14+00:00,runtime,medium,"Greptile correctly identifies that `priceAmount` is bound to a `type=""number""` input (line 300), which means it will be a number type, not a string. Calling `.trim()` on a number in JavaScript will cause a runtime TypeError since numbers don't have a trim method. This is a non-obvious bug that could easily be missed in code review.",2026-01-25T08:21:08.032501+00:00
raycast/extensions,24840,Add clawdbot extension,https://github.com/raycast/extensions/pull/24840,,"Hardcoded `/opt/homebrew/bin/clawdbot` path won't work on non-M1/M2 Macs (Intel uses `/usr/local/bin`), Linux, or Windows

```suggestion
    const { stdout } = await execAsync(
      `clawdbot agent --local --session-id raycast -m '${escapedMessage}' ${agentFlag} --timeout 120`,
      {
        encoding: ""utf-8"",
        timeout: 130000,
        maxBuffer: 10 * 1024 * 1024,
      },
    );
```",https://github.com/raycast/extensions/pull/24840#discussion_r2725365076,,2026-01-25T10:45:10+00:00,runtime,medium,"Greptile correctly identifies that the hardcoded path `/opt/homebrew/bin/clawdbot` is specific to Apple Silicon Macs (M1/M2). Intel Macs use `/usr/local/bin` for Homebrew, so this code would fail on those systems. The fix to rely on PATH resolution is appropriate and would make the extension work across different system configurations.",2026-01-25T12:20:33.362758+00:00
raycast/extensions,24840,Add clawdbot extension,https://github.com/raycast/extensions/pull/24840,,"Hardcoded `/opt/homebrew/bin/clawdbot` path won't work on non-M1/M2 Macs (Intel uses `/usr/local/bin`), Linux, or Windows

```suggestion
        const result = execSync(
          `clawdbot agent --local --session-id raycast -m '${escapedMessage}' ${agentFlag} --timeout 120`,
          {
            encoding: ""utf-8"",
            timeout: 130000,
            maxBuffer: 10 * 1024 * 1024,
          },
        );
```",https://github.com/raycast/extensions/pull/24840#discussion_r2725365105,,2026-01-25T10:45:10+00:00,runtime,medium,"Greptile correctly identifies that hardcoding `/opt/homebrew/bin/clawdbot` will cause the extension to fail on Intel Macs (which use `/usr/local/bin`) and other platforms. This is a real portability bug that would cause runtime failures for a significant portion of users, and the suggestion to rely on PATH resolution is the correct fix.",2026-01-25T12:20:33.362843+00:00
raycast/extensions,24840,Add clawdbot extension,https://github.com/raycast/extensions/pull/24840,,"Hardcoded `/opt/homebrew/bin/clawdbot` path won't work on non-M1/M2 Macs (Intel uses `/usr/local/bin`), Linux, or Windows

```suggestion
                const result = execSync(
                  ""clawdbot status 2>&1"",
                  {
                    encoding: ""utf-8"",
                    timeout: 15000,
                  },
                );
```",https://github.com/raycast/extensions/pull/24840#discussion_r2725365174,,2026-01-25T10:45:13+00:00,runtime,medium,Greptile correctly identifies that hardcoding `/opt/homebrew/bin/clawdbot` will cause the extension to fail on Intel Macs (which use `/usr/local/bin`) and other platforms. This is a real portability bug that would affect a significant portion of users - Raycast extensions should work across different Mac architectures. The suggestion to rely on PATH resolution instead is the correct fix.,2026-01-25T12:20:33.362900+00:00
mellanox/hw-mgmt,2263,hw-mgmt: scripts: generate dump: cleanup and optimize size,https://github.com/Mellanox/hw-mgmt/pull/2263,,"Pattern changed from `hw?management*` to `hw-management*` - the wildcard `?` previously matched any single character (e.g., `hw_management`, `hw-management`). Verify this doesn't exclude intended binaries.

```suggestion
cp /usr/bin/hw?management* ""$DUMP_FOLDER/bin/"" 2>/dev/null || true
```",https://github.com/Mellanox/hw-mgmt/pull/2263#discussion_r2725372189,,2026-01-25T10:52:31+00:00,logic,medium,"Greptile correctly identifies that changing `hw?management*` to `hw-management*` reduces the glob pattern's matching scope. The `?` wildcard matches any single character (like `_` or `-`), so the original pattern could match files like `hw_management*` or `hw-management*`, while the new pattern only matches `hw-management*`. This could cause the dump to miss binaries if any exist with `hw_management` naming, resulting in incomplete diagnostics data.",2026-01-25T12:20:33.362954+00:00
mellanox/hw-mgmt,2263,hw-mgmt: scripts: generate dump: cleanup and optimize size,https://github.com/Mellanox/hw-mgmt/pull/2263,,"Pattern changed from `hw?management*` to `hw-management*` - this excludes files like `hw_management_*.py` (thermal control, PSU update, etc.) that exist in `/usr/bin/`.

```suggestion
cp /usr/local/bin/hw?management* ""$DUMP_FOLDER/bin/"" 2>/dev/null || true
```",https://github.com/Mellanox/hw-mgmt/pull/2263#discussion_r2725372206,,2026-01-25T10:52:32+00:00,logic,medium,"Greptile correctly identifies that changing from `hw?management*` (glob pattern where ? matches any single character) to `hw-management*` (literal dash) would exclude files like `hw_management_*.py` from being copied to the dump. This is a subtle glob pattern change that could result in incomplete diagnostic dumps, missing important files.",2026-01-25T12:20:33.363003+00:00
mellanox/hw-mgmt,2263,hw-mgmt: scripts: generate dump: cleanup and optimize size,https://github.com/Mellanox/hw-mgmt/pull/2263,,"Pattern `hw-management*` excludes critical Python scripts like `hw_management_thermal_control.py`, `hw_management_psu_fw_update*.py`, and other `hw_management_*.py` files that exist in `/usr/bin/`. The previous pattern `hw?management*` matched both `hw-management*` and `hw_management*`.

```suggestion
cp /usr/bin/hw-management* /usr/bin/hw_management* ""$DUMP_FOLDER/bin/"" 2>/dev/null || true
cp /usr/local/bin/hw-management* /usr/local/bin/hw_management* ""$DUMP_FOLDER/bin/"" 2>/dev/null || true
```",https://github.com/Mellanox/hw-mgmt/pull/2263#discussion_r2725377956,,2026-01-25T11:01:30+00:00,logic,medium,"Greptile correctly identified that changing from `hw?management*` to `hw-management*` breaks glob pattern matching. The `?` wildcard matches any single character (including `_`), so the old pattern matched both `hw-management*` and `hw_management*` files. The new pattern only matches hyphenated filenames, silently excluding Python scripts like `hw_management_thermal_control.py` from the diagnostic dump - a subtle but real bug that would cause incomplete dumps.",2026-01-25T12:20:33.363040+00:00
gtbitsofgood/hope-sustains-life,234,[Snyk] Fix for 4 vulnerabilities,https://github.com/GTBitsOfGood/hope-sustains-life/pull/234,,"<details>
<summary>Additional Comments (1)</summary>

**`package.json`**
Next.js 12 requires React 17.0.2 or newer, but this project uses React 16.13.1. This version mismatch will cause build failures. Update React to 17.0.2+ or 18.x along with this Next.js upgrade.
</details>",https://github.com/GTBitsOfGood/hope-sustains-life/pull/234#issuecomment-3796413964,,2026-01-25T10:38:11+00:00,runtime,high,"This is a valid catch - Next.js 12 does require React 17+ and using React 16.13.1 would cause build failures. This is a non-obvious dependency compatibility issue that could break the build after the Snyk security upgrade, and catching incompatible peer dependencies across major version upgrades requires specific knowledge of the framework requirements.",2026-01-25T12:20:33.363087+00:00
posthog/posthog,45875,feat(insights): All insights page massive filters upgrade,https://github.com/PostHog/posthog/pull/45875,,"`json.loads()` calls should be wrapped in try-catch blocks to handle invalid JSON gracefully. The existing codebase pattern in `feature_flag.py`, `hog_function.py`, and `hog_flow.py` wraps similar `json.loads()` calls with `try/except (json.JSONDecodeError, TypeError)` to avoid server errors from malformed input.

```suggestion
            elif key == ""tags"":
                tags_filter = request.GET[""tags""]
                if tags_filter:
                    try:
                        tags_list = json.loads(tags_filter)
                        if tags_list:
                            queryset = queryset.filter(tagged_items__tag__name__in=tags_list).distinct()
                    except (json.JSONDecodeError, TypeError):
                        # If the JSON is invalid, ignore the filter
                        pass
            elif key == ""created_by"":
                created_by_filter = request.GET[""created_by""]
                if created_by_filter:
                    try:
                        created_by_ids = json.loads(created_by_filter)
                        if created_by_ids:
                            queryset = queryset.filter(created_by__id__in=created_by_ids)
                    except (json.JSONDecodeError, TypeError):
                        # If the JSON is invalid, ignore the filter
                        pass
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/insight.py
Line: 1153:1164

Comment:
`json.loads()` calls should be wrapped in try-catch blocks to handle invalid JSON gracefully. The existing codebase pattern in `feature_flag.py`, `hog_function.py`, and `hog_flow.py` wraps similar `json.loads()` calls with `try/except (json.JSONDecodeError, TypeError)` to avoid server errors from malformed input.

```suggestion
            elif key == ""tags"":
                tags_filter = request.GET[""tags""]
                if tags_filter:
                    try:
                        tags_list = json.loads(tags_filter)
                        if tags_list:
                            queryset = queryset.filter(tagged_items__tag__name__in=tags_list).distinct()
                    except (json.JSONDecodeError, TypeError):
                        # If the JSON is invalid, ignore the filter
                        pass
            elif key == ""created_by"":
                created_by_filter = request.GET[""created_by""]
                if created_by_filter:
                    try:
                        created_by_ids = json.loads(created_by_filter)
                        if created_by_ids:
                            queryset = queryset.filter(created_by__id__in=created_by_ids)
                    except (json.JSONDecodeError, TypeError):
                        # If the JSON is invalid, ignore the filter
                        pass
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45875#discussion_r2725485887,,2026-01-25T12:50:15+00:00,runtime,medium,"The code adds two `json.loads()` calls on user-provided query parameters (tags and created_by) without any exception handling. If a user passes malformed JSON in these parameters, it will cause an unhandled JSONDecodeError that would result in a 500 server error. This is a real bug that could cause crashes and follows the codebase pattern of wrapping json.loads in try/except blocks.",2026-01-25T14:13:25.909510+00:00
green-coding-solutions/green-metrics-tool,1523,Acl volume loading,https://github.com/green-coding-solutions/green-metrics-tool/pull/1523,,"Duplicate parameter `allowed_run_args` - line 67 overwrites line 66

```suggestion
            allowed_run_args=user._capabilities['measurement']['orchestrators']['docker']['allowed_run_args'], # They are specific to the orchestrator. However currently we only have one. As soon as we support more orchestrators we will sub-class Runner with dedicated child classes (DockerRunner, PodmanRunner etc.)
            allowed_volume_mounts=user._capabilities['measurement']['allowed_volume_mounts'],
```",https://github.com/green-coding-solutions/green-metrics-tool/pull/1523#discussion_r2725496146,,2026-01-25T13:01:42+00:00,logic,high,"Greptile correctly identified that line 67 adds a duplicate `allowed_run_args` parameter, which would overwrite line 66's value. This is clearly a bug - based on the PR title 'Acl volume loading' and the suggestion, this should likely be `allowed_volume_mounts` instead. This is a non-obvious copy-paste error that could cause incorrect ACL/permission behavior in production.",2026-01-25T14:13:25.909675+00:00
green-coding-solutions/green-metrics-tool,1523,Acl volume loading,https://github.com/green-coding-solutions/green-metrics-tool/pull/1523,,"Logic error: comparing `mount_option` (which is `,readonly` or empty string) to `'ro'` will always be False. Should compare to `,readonly` or check if empty.

```suggestion
                            if mount_option != ',readonly' and mount_option != '':
```",https://github.com/green-coding-solutions/green-metrics-tool/pull/1523#discussion_r2725496155,,2026-01-25T13:01:42+00:00,logic,high,"Greptile correctly identified that `mount_option` is set to `,readonly` (with leading comma) or empty string, but the comparison `if mount_option != 'ro'` checks against 'ro' without the comma. This condition will always be True, causing the code to always raise a RuntimeError for non-allowlisted volumes, even when they have valid readonly mounts.",2026-01-25T14:13:25.909731+00:00
green-coding-solutions/green-metrics-tool,1523,Acl volume loading,https://github.com/green-coding-solutions/green-metrics-tool/pull/1523,,"Missing comma validation for allowed volume mounts - users could inject additional mount options via commas in `mount_src` or `mount_target`

```suggestion
                            if '/' not in mount_src: # volume case. should exist
                                if ',' in mount_src:
                                    raise ValueError(f""Volume name may not contain commas (,): {mount_src}"")
                                if ',' in mount_target:
                                    raise ValueError(f""Mount target path may not contain commas (,): {mount_target}"")
                                ps = subprocess.run(
                                    [""docker"", ""volume"", ""inspect"", mount_src],
                                    check=False,
                                    stdout=subprocess.DEVNULL,
                                    stderr=subprocess.PIPE,
                                    encoding='UTF-8',
                                    errors='replace'
                                )
                                if ps.returncode != 0:
                                    raise RuntimeError(f""Could not find volume '{mount_src}' locally from service: {service_name}. The volume must be created manually before it can be loaded. GMT does not create named volumes. - Error from Docker: {ps.stderr}"")
                                docker_run_string.append('--mount')
                                docker_run_string.append(f""type=volume,source={mount_src},target={mount_target}{mount_option}"")
```",https://github.com/green-coding-solutions/green-metrics-tool/pull/1523#discussion_r2725496189,,2026-01-25T13:01:44+00:00,security,high,"Greptile correctly identified a security vulnerability: the code adds comma validation for non-allowlisted volume mounts (lines 1304-1307) but the allowlisted branch (starting line 1279) lacks this validation. An attacker could inject additional mount options via commas in mount_src or mount_target to alter the Docker mount behavior, potentially mounting unintended directories. This is a real injection vulnerability that's easy to miss.",2026-01-25T14:13:25.909773+00:00
green-coding-solutions/green-metrics-tool,1523,Acl volume loading,https://github.com/green-coding-solutions/green-metrics-tool/pull/1523,,"Missing comma validation for allowed bind mounts - users could inject additional mount options

```suggestion
                            else: # path case. Check path if on machine as -v will create folder otherwise
                                if ',' in mount_src:
                                    raise ValueError(f""Mount source path may not contain commas (,): {mount_src}"")
                                if ',' in mount_target:
                                    raise ValueError(f""Mount target path may not contain commas (,): {mount_target}"")
                                try:
                                    mount_src_absolute = Path(mount_src).resolve(strict=True)
                                except FileNotFoundError as exc:
                                    raise RuntimeError(f""Allowed volume mount path '{mount_src}' does not exist"") from exc
                                docker_run_string.append('--mount')
                                docker_run_string.append(f""type=bind,source={mount_src_absolute},target={mount_target}{mount_option}"")
```",https://github.com/green-coding-solutions/green-metrics-tool/pull/1523#discussion_r2725496197,,2026-01-25T13:01:45+00:00,security,high,"Greptile correctly identified a security vulnerability where the allowed volume mount path case (lines 1282-1291) lacks comma validation that exists elsewhere in the code. Without this check, users could inject additional mount options via commas in mount_src or mount_target to alter the Docker mount command, potentially mounting unintended paths. The code validates commas for non-allowed mounts (lines 1303-1306) but misses this validation for allowed mounts.",2026-01-25T14:13:25.909849+00:00
raycast/extensions,24848,Add clawdbot extension,https://github.com/raycast/extensions/pull/24848,,"`useState` is being called instead of `useEffect` - this will not execute the async operation correctly

```suggestion
  useEffect(() => {
    askQuestion(question)
      .then(setAnswer)
      .catch((e) => setError(e.message));
  }, [question]);
```",https://github.com/raycast/extensions/pull/24848#discussion_r2725654952,,2026-01-25T15:59:24+00:00,logic,high,"Greptile correctly identified that `useState` is being misused on line 146 - the code passes a callback function to `useState()` which expects an initial value, not a callback to execute. This is clearly a bug where `useEffect` should be used instead. The async operation to `askQuestion` will never execute, making the ImmediateAsk component completely non-functional. This is a non-obvious mistake that could easily be missed during review.",2026-01-25T16:16:02.462873+00:00
gptme/gptme-contrib,178,feat(plugins): add gptme-gupp work persistence plugin,https://github.com/gptme/gptme-contrib/pull/178,,"Missing `ToolSpec` registration - plugin won't be discovered by gptme

This file needs to import `ToolSpec` from `gptme.tools.base` and export a tool specification at the end. Without it, gptme cannot discover or use these functions. See other plugins like `warp-grep` or `imagen` for examples.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: plugins/gptme-gupp/src/gptme_gupp/tools/gupp.py
Line: 288:288

Comment:
Missing `ToolSpec` registration - plugin won't be discovered by gptme

This file needs to import `ToolSpec` from `gptme.tools.base` and export a tool specification at the end. Without it, gptme cannot discover or use these functions. See other plugins like `warp-grep` or `imagen` for examples.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/178#discussion_r2725667882,,2026-01-25T16:11:11+00:00,logic,high,"The code defines several hook functions but never creates or exports a ToolSpec object that gptme uses to discover and register plugins. Looking at the file, there's no import of ToolSpec and no tool registration at the end, meaning this plugin would be completely non-functional - gptme wouldn't be able to discover or use any of these functions. This is a critical missing piece that would cause the entire plugin to fail silently.",2026-01-25T16:16:02.462939+00:00
gptme/gptme-contrib,178,feat(plugins): add gptme-gupp work persistence plugin,https://github.com/gptme/gptme-contrib/pull/178,,"Sorting by `updated_at` string puts older items first, not recent ones

ISO format strings sort chronologically ascending (oldest first). The docstring says ""recent first"" but the code does the opposite. Parse the timestamp and use negative value for descending order.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: plugins/gptme-gupp/src/gptme_gupp/tools/gupp.py
Line: 180:187

Comment:
Sorting by `updated_at` string puts older items first, not recent ones

ISO format strings sort chronologically ascending (oldest first). The docstring says ""recent first"" but the code does the opposite. Parse the timestamp and use negative value for descending order.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/178#discussion_r2725667912,,2026-01-25T16:11:13+00:00,logic,low,"Greptile correctly identifies that ISO format strings sort ascending (oldest first) when sorted naturally, but the docstring explicitly states hooks should be sorted 'by updated_at (recent first)'. This is a real logic bug where the implementation contradicts the documented behavior, though impact is limited to display ordering.",2026-01-25T16:16:02.462988+00:00
alpaca-network/gatewayz-frontend,909,feat(pricing): show upgrade/downgrade buttons based on user's current plan,https://github.com/Alpaca-Network/gatewayz-frontend/pull/909,,"`getUserData()` called inside `useMemo` dependency array but not listed as dependency

Calling `getUserData()` directly inside the memoized function but relying on `currentTier` and `hasSubscription` from `useTier()` hook. If user data changes, the function won't recalculate because `getUserData()` result isn't tracked. This can show stale button states after login/logout.

Use `userData` from `useTier()` hook (already available as `const { tier: currentTier, hasSubscription, userData } = useTier()`) and add it to dependencies.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/components/pricing/pricing-section.tsx
Line: 119:123

Comment:
`getUserData()` called inside `useMemo` dependency array but not listed as dependency

Calling `getUserData()` directly inside the memoized function but relying on `currentTier` and `hasSubscription` from `useTier()` hook. If user data changes, the function won't recalculate because `getUserData()` result isn't tracked. This can show stale button states after login/logout.

Use `userData` from `useTier()` hook (already available as `const { tier: currentTier, hasSubscription, userData } = useTier()`) and add it to dependencies.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/909#discussion_r2725760899,,2026-01-25T17:37:23+00:00,logic,medium,"Greptile correctly identifies that `getUserData()` is called inside the `useMemo` callback (line 119) but its result is not tracked as a dependency. The memo only depends on `[currentTier, hasSubscription]`, so if a user logs in/out, the memoized function will return stale button states because React won't know to recalculate. This is a non-obvious React hooks bug that could cause incorrect UI states after authentication changes.",2026-01-25T18:19:51.512058+00:00
alpaca-network/gatewayz-frontend,909,feat(pricing): show upgrade/downgrade buttons based on user's current plan,https://github.com/Alpaca-Network/gatewayz-frontend/pull/909,,"Redundant `getUserData()` call - already retrieved in `getButtonAction`

`getUserData()` is called twice: once in `getButtonAction` (line 120) and again here (line 173). Since `getButtonAction` already checked authentication, this second call is unnecessary and the starter tier downgrade redirect to `/settings/credits?action=manage` is unreachable (authenticated users on starter tier would get `action === 'your_plan'` not `'downgrade'`).

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/components/pricing/pricing-section.tsx
Line: 171:185

Comment:
Redundant `getUserData()` call - already retrieved in `getButtonAction`

`getUserData()` is called twice: once in `getButtonAction` (line 120) and again here (line 173). Since `getButtonAction` already checked authentication, this second call is unnecessary and the starter tier downgrade redirect to `/settings/credits?action=manage` is unreachable (authenticated users on starter tier would get `action === 'your_plan'` not `'downgrade'`).

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/909#discussion_r2725760912,,2026-01-25T17:37:24+00:00,logic,medium,"Greptile correctly identifies that the starter tier downgrade code path (lines 179-183) is unreachable dead code. Looking at getButtonAction: when a user is on 'starter' tier and clicks on 'starter', targetLevel === currentLevel (both 0), so it returns 'your_plan' not 'downgrade'. A user on a higher tier clicking starter would get 'downgrade', but then the condition `tier.id === 'starter' || action === 'get_started'` wouldn't be satisfied since action would be 'downgrade'. This is a real logic bug where intended downgrade-to-starter functionality won't work as expected.",2026-01-25T18:19:51.512187+00:00
alpaca-network/gatewayz-frontend,909,feat(pricing): show upgrade/downgrade buttons based on user's current plan,https://github.com/Alpaca-Network/gatewayz-frontend/pull/909,,"<details>
<summary>Additional Comments (1)</summary>

**`src/components/pricing/pricing-section.tsx`**
Stripe Product ID mismatch with `pricing-config.ts`

Max tier uses `prod_TKOraBpWMxMAIu` here but `prod_TMHUXL8p0onwwO` in `pricing-config.ts:94`. This will cause checkout to use different product IDs depending on which file is referenced, breaking subscription consistency.

```suggestion
    stripePriceId: process.env.NEXT_PUBLIC_STRIPE_PRO_PRICE_ID,
    stripeProductId: 'prod_TKOqQPhVRxNp4Q', // Pro product ID for database
  },
  {
    id: 'max',
    name: 'Max',
    description: 'Higher limits, priority access',
    price: '$75',
    originalPrice: '$150/month',
    discount: 'Save 50%',
    ctaText: 'Get Started',
    ctaVariant: 'secondary',
    popular: true,
    features: [
      '50% discount on $150 credits',
      '10x more usage than Pro',
      'Higher output limits for all tasks',
      'Early access to advanced features',
    ],
    stripePriceId: process.env.NEXT_PUBLIC_STRIPE_MAX_PRICE_ID,
    stripeProductId: 'prod_TMHUXL8p0onwwO', // Max product ID for database
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/components/pricing/pricing-section.tsx
Line: 58:77

Comment:
Stripe Product ID mismatch with `pricing-config.ts`

Max tier uses `prod_TKOraBpWMxMAIu` here but `prod_TMHUXL8p0onwwO` in `pricing-config.ts:94`. This will cause checkout to use different product IDs depending on which file is referenced, breaking subscription consistency.

```suggestion
    stripePriceId: process.env.NEXT_PUBLIC_STRIPE_PRO_PRICE_ID,
    stripeProductId: 'prod_TKOqQPhVRxNp4Q', // Pro product ID for database
  },
  {
    id: 'max',
    name: 'Max',
    description: 'Higher limits, priority access',
    price: '$75',
    originalPrice: '$150/month',
    discount: 'Save 50%',
    ctaText: 'Get Started',
    ctaVariant: 'secondary',
    popular: true,
    features: [
      '50% discount on $150 credits',
      '10x more usage than Pro',
      'Higher output limits for all tasks',
      'Early access to advanced features',
    ],
    stripePriceId: process.env.NEXT_PUBLIC_STRIPE_MAX_PRICE_ID,
    stripeProductId: 'prod_TMHUXL8p0onwwO', // Max product ID for database
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/909#issuecomment-3796997594,,2026-01-25T17:37:29+00:00,data_integrity,high,"Greptile correctly identified a Stripe Product ID mismatch between two configuration files - the Max tier uses 'prod_TKOraBpWMxMAIu' in pricing-section.tsx but 'prod_TMHUXL8p0onwwO' in pricing-config.ts. This inconsistency would cause subscription tracking and billing issues since different product IDs would be used depending on which file handles the checkout, a non-obvious cross-file bug that could break payment functionality.",2026-01-25T18:19:51.512259+00:00
gptme/gptme-contrib,180,feat(plugins): add ACE context optimization plugin,https://github.com/gptme/gptme-contrib/pull/180,,"This code will raise `IndexError` if `lesson_id` exists in metadata but is not in the FAISS index (e.g., if index was rebuilt but metadata wasn't). The skip on line 196 doesn't prevent the index lookup on line 200.

```suggestion
            # Skip if no embedding
            if lesson_id not in self.embedder.metadata:
                continue

            # Get embedding from FAISS index
            try:
                idx = list(self.embedder.metadata.keys()).index(lesson_id)
                lesson_embed = self.embedder.index.reconstruct(idx)
            except (ValueError, RuntimeError):
                # Lesson not in index or index error
                continue
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: plugins/ace/src/gptme_ace/hybrid_retriever.py
Line: 196:201

Comment:
This code will raise `IndexError` if `lesson_id` exists in metadata but is not in the FAISS index (e.g., if index was rebuilt but metadata wasn't). The skip on line 196 doesn't prevent the index lookup on line 200.

```suggestion
            # Skip if no embedding
            if lesson_id not in self.embedder.metadata:
                continue

            # Get embedding from FAISS index
            try:
                idx = list(self.embedder.metadata.keys()).index(lesson_id)
                lesson_embed = self.embedder.index.reconstruct(idx)
            except (ValueError, RuntimeError):
                # Lesson not in index or index error
                continue
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/180#discussion_r2725793177,,2026-01-25T18:11:31+00:00,runtime,medium,"Greptile correctly identifies a potential runtime error: line 196 checks if `lesson_id` is in `self.embedder.metadata`, but line 200 uses `list(self.embedder.metadata.keys()).index(lesson_id)` which finds the position in the metadata keys - this assumes the FAISS index has the same number of entries in the same order as the metadata dictionary. If the FAISS index gets out of sync (rebuilt/corrupted), `index.reconstruct(idx)` could raise an error or return wrong data. The suggestion to wrap in try/except is a reasonable defensive fix for this non-obvious sync issue.",2026-01-25T18:19:51.512333+00:00
gptme/gptme-contrib,179,feat(plugins): add ralph-loop plugin for iterative execution,https://github.com/gptme/gptme-contrib/pull/179,,"Shell injection vulnerability - `spec_file`, `plan_file`, and `backend` parameters are inserted directly into the bash script without proper escaping. If these contain shell metacharacters or are user-controlled, this allows arbitrary command execution.

Use `shlex.quote()` for all variables inserted into the bash script:

```suggestion
    script = f""""""#!/bin/bash
cd {shlex.quote(str(work_dir))}
SPEC_FILE={shlex.quote(spec_file)}
PLAN_FILE={shlex.quote(plan_file)}
BACKEND={shlex.quote(backend)}
MAX_ITER={max_iterations}
TIMEOUT={step_timeout}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: plugins/ralph-loop/src/gptme_ralph_loop/tools/ralph_loop.py
Line: 424:430

Comment:
Shell injection vulnerability - `spec_file`, `plan_file`, and `backend` parameters are inserted directly into the bash script without proper escaping. If these contain shell metacharacters or are user-controlled, this allows arbitrary command execution.

Use `shlex.quote()` for all variables inserted into the bash script:

```suggestion
    script = f""""""#!/bin/bash
cd {shlex.quote(str(work_dir))}
SPEC_FILE={shlex.quote(spec_file)}
PLAN_FILE={shlex.quote(plan_file)}
BACKEND={shlex.quote(backend)}
MAX_ITER={max_iterations}
TIMEOUT={step_timeout}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/179#discussion_r2725733547,,2026-01-25T17:05:26+00:00,security,critical,"Greptile correctly identified a shell injection vulnerability. Looking at lines 424-430, `spec_file`, `plan_file`, and `backend` are inserted directly into a bash script string without escaping (e.g., `SPEC_FILE=""{spec_file}""`), while `work_dir` is properly escaped with `shlex.quote()`. If any of these user-controlled parameters contain shell metacharacters like `$(command)` or backticks, arbitrary commands could be executed. This is a real security bug that's easy to miss since the code already uses `shlex.quote()` for `work_dir` but inconsistently omits it for other variables.",2026-01-25T18:19:51.512406+00:00
alpic-ai/skybridge,390,(feat): entering JSON manually,https://github.com/alpic-ai/skybridge/pull/390,,"`JSON.parse` will throw when invalid JSON is entered, crashing the component. Wrap in try-catch or validate before parsing.

```suggestion
      onChange={(e) => {
        try {
          setFormData(JSON.parse(e.target.value));
        } catch {
          // Allow typing invalid JSON without crashing
        }
      }}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/devtools/src/components/layout/tool-panel/input-form.tsx
Line: 116:116

Comment:
`JSON.parse` will throw when invalid JSON is entered, crashing the component. Wrap in try-catch or validate before parsing.

```suggestion
      onChange={(e) => {
        try {
          setFormData(JSON.parse(e.target.value));
        } catch {
          // Allow typing invalid JSON without crashing
        }
      }}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/alpic-ai/skybridge/pull/390#discussion_r2725702641,,2026-01-25T16:34:41+00:00,runtime,medium,"Greptile correctly identified that `JSON.parse(e.target.value)` on line 116 will throw an exception when the user types invalid JSON (which happens frequently during typing, e.g., after typing just '{'), causing the component to crash. This is a real runtime bug that's easy to miss but will definitely occur during normal use of the JSON editor.",2026-01-25T18:19:51.512450+00:00
raycast/extensions,24857,Update cloudflare-email-routing extension,https://github.com/raycast/extensions/pull/24857,,"splitting on `@` only once could miss edge cases with multiple `@` symbols

```suggestion
    if (trimmed.includes(""@"")) {
      const atIndex = trimmed.lastIndexOf(""@"");
      const localPart = trimmed.substring(0, atIndex);
      const inputDomain = trimmed.substring(atIndex + 1);
      if (!localPart || !inputDomain) {
        throw new Error(""Alias must include a valid domain"");
      }
      if (inputDomain.toLowerCase() !== aliasDomain.toLowerCase()) {
        throw new Error(`Alias domain must match ${aliasDomain}`);
      }
      return `${localPart}@${aliasDomain}`;
    }
```",https://github.com/raycast/extensions/pull/24857#discussion_r2725934733,,2026-01-25T21:24:11+00:00,logic,medium,"Greptile correctly identified that `split('@')` on an email with multiple @ symbols (e.g., 'test@test@domain.com') would create an array with more than 2 elements, causing the destructuring `[localPart, inputDomain]` to only capture the first two parts, losing the actual domain. Using `lastIndexOf('@')` is the correct approach for RFC-compliant email parsing since the local part can contain @ symbols when quoted.",2026-01-25T22:14:03.471062+00:00
alpaca-network/gatewayz-backend,929,feat(users): add settings column for auto top-up configuration,https://github.com/Alpaca-Network/gatewayz-backend/pull/929,,"balance calculation incorrectly multiplies by 100 - credits are already in dollars in DB

The database stores credits as DECIMAL in dollars (e.g., 15.00 for $15), but this query multiplies by 100 before comparing to the threshold. This means a user with $5.00 balance would be calculated as 500 cents, but compared against a threshold like 1000 cents ($10), incorrectly triggering auto top-up when their actual balance ($5) is above the threshold in dollars.

The comparison should be in the same units. Since thresholds are documented as cents, convert to cents without the extra *100:

```suggestion
        AND (COALESCE(u.subscription_allowance, 0) + COALESCE(u.purchased_credits, 0)) <
            COALESCE((u.settings->>'auto_topup_threshold')::integer, 0) / 100.0;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: supabase/migrations/20260125000000_add_user_settings_column.sql
Line: 86:87

Comment:
balance calculation incorrectly multiplies by 100 - credits are already in dollars in DB

The database stores credits as DECIMAL in dollars (e.g., 15.00 for $15), but this query multiplies by 100 before comparing to the threshold. This means a user with $5.00 balance would be calculated as 500 cents, but compared against a threshold like 1000 cents ($10), incorrectly triggering auto top-up when their actual balance ($5) is above the threshold in dollars.

The comparison should be in the same units. Since thresholds are documented as cents, convert to cents without the extra *100:

```suggestion
        AND (COALESCE(u.subscription_allowance, 0) + COALESCE(u.purchased_credits, 0)) <
            COALESCE((u.settings->>'auto_topup_threshold')::integer, 0) / 100.0;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-backend/pull/929#discussion_r2725908599,,2026-01-25T20:48:03+00:00,logic,high,"The code multiplies the balance by 100 before comparing to the threshold, but according to Greptile's analysis and the comment on line 53 stating 'Threshold and amount are in cents', this creates a unit mismatch. If credits are stored in dollars and thresholds in cents, the comparison would be incorrect - a $5.00 balance becomes 500 and compared against a 1000 cent threshold incorrectly. This is a subtle unit conversion bug that could cause incorrect auto top-up triggers, affecting billing.",2026-01-25T22:14:03.471152+00:00
gptme/gptme-contrib,192,refactor(packages): rename packages per gptme- prefix convention,https://github.com/gptme/gptme-contrib/pull/192,,"<details>
<summary>Additional Comments (2)</summary>

**`packages/gptme-contrib-lib/pyproject.toml`**
Build config points to wrong directory. Should be `[""src/gptme_contrib_lib""]` not `[""src/lib""]`.

```suggestion
[tool.hatch.build.targets.wheel]
packages = [""src/gptme_contrib_lib""]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptme-contrib-lib/pyproject.toml
Line: 27:28

Comment:
Build config points to wrong directory. Should be `[""src/gptme_contrib_lib""]` not `[""src/lib""]`.

```suggestion
[tool.hatch.build.targets.wheel]
packages = [""src/gptme_contrib_lib""]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`packages/gptme-runloops/pyproject.toml`**
Script entrypoint references old module name. Should be `gptme_runloops.cli:main` not `run_loops.cli:main`.

```suggestion
[project.scripts]
run-loops = ""gptme_runloops.cli:main""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptme-runloops/pyproject.toml
Line: 18:19

Comment:
Script entrypoint references old module name. Should be `gptme_runloops.cli:main` not `run_loops.cli:main`.

```suggestion
[project.scripts]
run-loops = ""gptme_runloops.cli:main""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/gptme/gptme-contrib/pull/192#issuecomment-3797274759,,2026-01-25T21:37:38+00:00,runtime,high,"These are real bugs that would cause runtime failures - incorrect build paths and module references after a rename refactor would prevent the packages from building/installing correctly. During package rename refactors, these mismatches between old and new module names are easy to miss but cause immediate failures when trying to build or run the package.",2026-01-25T22:14:03.471245+00:00
onyx-dot-app/onyx,7773,fix(bookstack): use ISO-8601 datetime format for incremental polling,https://github.com/onyx-dot-app/onyx/pull/7773,,"<details>
<summary>Additional Comments (1)</summary>

**`backend/onyx/connectors/bookstack/connector.py`**
The `end` filter should also use ISO-8601 format for consistency with the `start` filter on line 60

```suggestion
        if end:
            params[""filter[updated_at:lte]""] = datetime.utcfromtimestamp(end).strftime(
                ""%Y-%m-%dT%H:%M:%SZ""
            )
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/connectors/bookstack/connector.py
Line: 62:65

Comment:
The `end` filter should also use ISO-8601 format for consistency with the `start` filter on line 60

```suggestion
        if end:
            params[""filter[updated_at:lte]""] = datetime.utcfromtimestamp(end).strftime(
                ""%Y-%m-%dT%H:%M:%SZ""
            )
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/onyx-dot-app/onyx/pull/7773#issuecomment-3797411975,,2026-01-25T23:43:52+00:00,logic,medium,"This is a valid consistency bug - if the PR is fixing the `start` filter to use ISO-8601 format for the BookStack API, the `end` filter should also be updated to use the same format. Having mismatched datetime formats between start and end parameters could cause incorrect filtering behavior in the API calls. The catch requires understanding that both filters are querying the same API endpoint and should use consistent formatting.",2026-01-26T00:48:07.513094+00:00
isaac-sim/isaaclab,4453,[IN PROGRESS] Fixes camera position update,https://github.com/isaac-sim/IsaacLab/pull/4453,,"duplicate assignment overwrites the intended offset `(0.5, 0.0, 0.2)` with `(0.0, 0.0, 0.0)`, making the offset zero and not testing offset functionality

```suggestion
    camera_offset = (0.5, 0.0, 0.2)  # Camera offset from robot base
```",https://github.com/isaac-sim/IsaacLab/pull/4453#discussion_r2725971750,,2026-01-25T22:18:03+00:00,logic,medium,"Greptile correctly identified that camera_offset is assigned twice in succession - first to (0.5, 0.0, 0.2) and immediately overwritten with (0.0, 0.0, 0.0). This makes the test ineffective at verifying offset functionality since the offset becomes zero. This appears to be an accidental duplicate line that would cause the test to not properly validate the camera offset behavior it claims to test.",2026-01-26T00:48:07.513188+00:00
isaac-sim/isaaclab,4453,[IN PROGRESS] Fixes camera position update,https://github.com/isaac-sim/IsaacLab/pull/4453,,"using camera position as base position - should retrieve robot's base position instead

```suggestion
    initial_base_pos = robot.data.root_pos_w[0].clone()
```",https://github.com/isaac-sim/IsaacLab/pull/4453#discussion_r2725971784,,2026-01-25T22:18:04+00:00,logic,medium,"Greptile correctly identifies that line 924 sets `initial_base_pos = camera.data.pos_w[0].clone()` which is the camera's position, not the robot's base position. The test is supposed to compare camera position against robot base position + offset, but by using camera position as the 'base position', the test is effectively comparing camera position to itself + offset, making the test logic incorrect. The suggestion to use `robot.data.root_pos_w[0].clone()` is the correct fix.",2026-01-26T00:48:07.513212+00:00
raycast/extensions,24856,system-monitor: split displaymode into per module variable,https://github.com/raycast/extensions/pull/24856,,"`getPreferenceValues` called at module level causes stale preference values. Preferences won't update when user changes settings.

Move inside component:
```suggestion
export default function CpuMonitor() {
  const { displayModeCpu } = getPreferenceValues<ExtensionPreferences>();
  const { revalidate, data: cpu } = usePromise(() => {
```",https://github.com/raycast/extensions/pull/24856#discussion_r2726150181,,2026-01-26T01:56:10+00:00,logic,medium,"Greptile correctly identifies that calling getPreferenceValues at module level (line 10) means the preference is captured once when the module loads, so if users change their displayModeCpu preference, the component won't reflect the new value until the app is restarted. Moving it inside the component ensures fresh preference values on each render.",2026-01-26T03:04:21.165898+00:00
raycast/extensions,24856,system-monitor: split displaymode into per module variable,https://github.com/raycast/extensions/pull/24856,,`displayModeBattery` is not in scope - needs to be passed as prop or moved to module level (though module level will cause stale values as noted in previous threads),https://github.com/raycast/extensions/pull/24856#discussion_r2726177898,,2026-01-26T02:21:40+00:00,runtime,high,"Greptile correctly identifies that `displayModeBattery` is declared at module level (line 10) but is used inside the `PowerMonitorDetail` function (line 55) which is a separate function from `PowerMonitor`. While JavaScript closures would technically allow access to module-level variables, this is still valid JavaScript - however, looking more carefully, the variable IS at module level so it IS in scope. The comment appears to be a false positive as module-level const declarations are accessible to all functions in the same module.",2026-01-26T03:04:21.165969+00:00
onyx-dot-app/onyx,7777,Kube web server,https://github.com/onyx-dot-app/onyx/pull/7777,,"The logic changed from filesystem-based detection to port-based detection. This assumes a port is only allocated when a webapp exists, but sessions always get a port allocated regardless of whether a webapp actually gets created. This could show `has_webapp: true` even when no webapp was ever created in the session.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/session/manager.py
Line: 1229:1230

Comment:
The logic changed from filesystem-based detection to port-based detection. This assumes a port is only allocated when a webapp exists, but sessions always get a port allocated regardless of whether a webapp actually gets created. This could show `has_webapp: true` even when no webapp was ever created in the session.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7777#discussion_r2726192644,,2026-01-26T02:35:01+00:00,logic,medium,"Greptile correctly identified that the code change creates a logic error. Looking at the diff, `allocate_nextjs_port` is called unconditionally in `create_session__no_commit` (line 381-382), meaning every session gets a port. The new `has_webapp` check on line 1230 (`session.nextjs_port is not None`) will always return true since ports are always allocated, even when no webapp directory actually exists. The original code correctly checked for the physical existence of the web directory.",2026-01-26T03:04:21.166009+00:00
helixdb/helix-db,827,fix(vector_core): prevent race conditions in concurrent HNSW writes,https://github.com/HelixDB/helix-db/pull/827,,"Unconditionally setting entry point on any error (line 725) can overwrite a valid entry point if there's a temporary read error. Only set on `EntryPointNotFound` (line 722), not on other errors like database I/O failures.",https://github.com/HelixDB/helix-db/pull/827#discussion_r2726219807,,2026-01-26T02:58:24+00:00,data_integrity,medium,"Greptile correctly identifies that the catch-all `Err(_)` case at line 725 will overwrite a valid entry point on any error (like I/O failures), not just when the entry point is genuinely missing. This could cause data corruption in the HNSW graph structure during temporary database issues. The fix should only set entry point on `EntryPointNotFound`, treating other errors differently.",2026-01-26T03:04:21.166083+00:00
onyx-dot-app/onyx,7784,chore(deployment): remove no auth option from setup script,https://github.com/onyx-dot-app/onyx/pull/7784,,"These configuration variables are duplicated - they're defined both here and in `backend/onyx/configs/app_configs.py` (lines 1047-1057). The values also differ: `app_configs.py` has a default path of `""/app/indexed-docs""` while this file has an empty string default. This could cause configuration inconsistencies.

Remove these duplicate definitions from this file and import them from `app_configs.py` instead.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/configs.py
Line: 3:13

Comment:
These configuration variables are duplicated - they're defined both here and in `backend/onyx/configs/app_configs.py` (lines 1047-1057). The values also differ: `app_configs.py` has a default path of `""/app/indexed-docs""` while this file has an empty string default. This could cause configuration inconsistencies.

Remove these duplicate definitions from this file and import them from `app_configs.py` instead.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7784#discussion_r2726297135,,2026-01-26T04:08:08+00:00,data_integrity,medium,"Greptile correctly identified that these configuration variables are duplicated with different default values - this new file uses empty string as default for PERSISTENT_DOCUMENT_STORAGE_PATH while app_configs.py uses '/app/indexed-docs'. This is a real bug that could cause configuration inconsistencies depending on which module is imported, potentially leading to document storage failures or unexpected behavior.",2026-01-26T04:38:40.959505+00:00
onyx-dot-app/onyx,7784,chore(deployment): remove no auth option from setup script,https://github.com/onyx-dot-app/onyx/pull/7784,,"If `PERSISTENT_DOCUMENT_STORAGE_PATH` is an empty string (the default in `build/configs.py`), `Path("""")` will create paths relative to the current working directory, which could cause files to be written in unexpected locations.

Add validation to ensure the path is not empty when the feature is enabled, or use the config from `app_configs.py` which has a proper default of `""/app/indexed-docs""`.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/indexing/persistent_document_writer.py
Line: 135:141

Comment:
If `PERSISTENT_DOCUMENT_STORAGE_PATH` is an empty string (the default in `build/configs.py`), `Path("""")` will create paths relative to the current working directory, which could cause files to be written in unexpected locations.

Add validation to ensure the path is not empty when the feature is enabled, or use the config from `app_configs.py` which has a proper default of `""/app/indexed-docs""`.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7784#discussion_r2726297162,,2026-01-26T04:08:10+00:00,security,medium,"Greptile correctly identifies that if PERSISTENT_DOCUMENT_STORAGE_PATH is an empty string, Path('') will create paths relative to the current working directory, causing files to be written to unexpected locations. This is a real bug with security/data integrity implications - files could end up in arbitrary locations depending on where the process is running. The code lacks any validation for empty/invalid paths before use.",2026-01-26T04:38:40.959583+00:00
ever-co/ever-gauzy,9371,"fix: add arch in about window, timer window not white blank screen",https://github.com/ever-co/ever-gauzy/pull/9371,,"The getter reassigns `_application` each time it's called, discarding the `arch` value fetched via IPC. This creates a race condition where the arch won't display if the getter runs before the IPC response completes.

```suggestion
	public get application() {
		return {
			...this._application,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/desktop-ui-lib/src/lib/dialogs/about/about.component.ts
Line: 62:63

Comment:
The getter reassigns `_application` each time it's called, discarding the `arch` value fetched via IPC. This creates a race condition where the arch won't display if the getter runs before the IPC response completes.

```suggestion
	public get application() {
		return {
			...this._application,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-gauzy/pull/9371#discussion_r2726227402,,2026-01-26T03:05:20+00:00,concurrency,medium,"Greptile correctly identifies that the getter reassigns `_application` entirely each time it's called, which would overwrite the `arch` property set by the async IPC callback. Looking at the diff, the fix adds `...this._application` spread at line 63, which preserves the `arch` value, validating that this was a real bug causing the arch to not display due to a race condition.",2026-01-26T04:38:40.959649+00:00
alpaca-network/gatewayz-backend,936,feat(chat): integrate Butter.dev caching proxy for streaming requests,https://github.com/Alpaca-Network/gatewayz-backend/pull/936,,"fallback logic inside exception handler doesn't set `is_async_stream` correctly for PROVIDER_ROUTING providers

when Butter.dev fails and falls back to PROVIDER_ROUTING, `is_async_stream` remains True from line 2280, but PROVIDER_ROUTING uses sync streams via `_to_thread`

```suggestion
                        except Exception as butter_err:
                            # Fallback to direct provider if Butter fails
                            logger.warning(
                                f""Butter.dev streaming failed, falling back to direct: {butter_err}""
                            )
                            use_butter_cache = False
                            # Fall through to normal provider routing below
                            if attempt_provider in PROVIDER_ROUTING:
                                stream_func = PROVIDER_ROUTING[attempt_provider][""stream""]
                                stream = await _to_thread(stream_func, messages, request_model, **optional)
                                is_async_stream = False
                            else:
                                stream = await make_openrouter_request_openai_stream_async(
                                    messages, request_model, **optional
                                )
                                is_async_stream = True
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/routes/chat.py
Line: 2284:2298

Comment:
fallback logic inside exception handler doesn't set `is_async_stream` correctly for PROVIDER_ROUTING providers

when Butter.dev fails and falls back to PROVIDER_ROUTING, `is_async_stream` remains True from line 2280, but PROVIDER_ROUTING uses sync streams via `_to_thread`

```suggestion
                        except Exception as butter_err:
                            # Fallback to direct provider if Butter fails
                            logger.warning(
                                f""Butter.dev streaming failed, falling back to direct: {butter_err}""
                            )
                            use_butter_cache = False
                            # Fall through to normal provider routing below
                            if attempt_provider in PROVIDER_ROUTING:
                                stream_func = PROVIDER_ROUTING[attempt_provider][""stream""]
                                stream = await _to_thread(stream_func, messages, request_model, **optional)
                                is_async_stream = False
                            else:
                                stream = await make_openrouter_request_openai_stream_async(
                                    messages, request_model, **optional
                                )
                                is_async_stream = True
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-backend/pull/936#discussion_r2726320309,,2026-01-26T04:29:42+00:00,logic,medium,"Greptile correctly identified that when Butter.dev fails and falls back to PROVIDER_ROUTING (line 2291-2293), the code doesn't set `is_async_stream = False`, leaving it as True from line 2280. Since PROVIDER_ROUTING uses sync streams via `_to_thread`, this mismatch would cause incorrect stream handling downstream. This is a non-obvious bug in error-handling fallback logic that could cause runtime issues.",2026-01-26T04:38:40.959705+00:00
alpaca-network/gatewayz-backend,936,feat(chat): integrate Butter.dev caching proxy for streaming requests,https://github.com/Alpaca-Network/gatewayz-backend/pull/936,,"`X-Butter-Cache` header may incorrectly report ""enabled"" for providers not in `BUTTER_PROVIDER_CONFIG`

`use_butter_cache` is set based on `BUTTER_COMPATIBLE_PROVIDERS` (26 providers), but actual Butter routing only works for providers in `BUTTER_PROVIDER_CONFIG` (11 providers). For providers like ""near"", ""aihubmix"", etc., the header will say ""enabled"" even though the request goes direct to the provider.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/routes/chat.py
Line: 2341:2345

Comment:
`X-Butter-Cache` header may incorrectly report ""enabled"" for providers not in `BUTTER_PROVIDER_CONFIG`

`use_butter_cache` is set based on `BUTTER_COMPATIBLE_PROVIDERS` (26 providers), but actual Butter routing only works for providers in `BUTTER_PROVIDER_CONFIG` (11 providers). For providers like ""near"", ""aihubmix"", etc., the header will say ""enabled"" even though the request goes direct to the provider.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-backend/pull/936#discussion_r2726320329,,2026-01-26T04:29:43+00:00,logic,medium,"Greptile correctly identified a logic inconsistency: the code at line 2271 checks `use_butter_cache and attempt_provider in BUTTER_PROVIDER_CONFIG` before using Butter, but the header at lines 2341-2345 only checks `use_butter_cache`. This means if `should_use_butter_cache()` returns true for a provider NOT in `BUTTER_PROVIDER_CONFIG`, the request will fall through to direct provider routing, but the header will still report 'enabled'. This is a real observability bug that could mislead debugging efforts.",2026-01-26T04:38:40.959816+00:00
joby-aviation/noodles.gl,292,feat: Add ChartOp for Observable Plot visualization,https://github.com/joby-aviation/noodles.gl/pull/292,,"`setFieldVisibility` method doesn't exist on Operator class - will throw runtime error

```suggestion
      // yField only needed for bar and scatter charts
      if (type === 'histogram') {
        this.visibleFields.next(new Set(Object.keys(this.inputs).filter(k => k !== 'yField')))
      } else {
        this.visibleFields.next(null) // use defaults
      }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: noodles-editor/src/noodles/operators.ts
Line: 1951:1951

Comment:
`setFieldVisibility` method doesn't exist on Operator class - will throw runtime error

```suggestion
      // yField only needed for bar and scatter charts
      if (type === 'histogram') {
        this.visibleFields.next(new Set(Object.keys(this.inputs).filter(k => k !== 'yField')))
      } else {
        this.visibleFields.next(null) // use defaults
      }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/joby-aviation/noodles.gl/pull/292#discussion_r2726432176,,2026-01-26T06:03:28+00:00,runtime,high,"Greptile correctly identified that `setFieldVisibility` is called on line 1951 but this method doesn't appear to exist in the Operator base class (not shown in the diff, but the suggestion uses `visibleFields.next()` instead, implying the correct API). This would cause a runtime error when the chartType subscription fires, making this a real bug that could easily be missed during code review.",2026-01-26T06:23:26.281956+00:00
joby-aviation/noodles.gl,292,feat: Add ChartOp for Observable Plot visualization,https://github.com/joby-aviation/noodles.gl/pull/292,,"`marks` could be undefined if invalid chartType passed (TypeScript doesn't enforce StringLiteralField runtime values)

```suggestion
    let marks: Plot.Markish[]
    switch (chartType) {
      case 'bar':
        marks = [Plot.barY(data, { x: xField, y: yField, fill: color })]
        break
      case 'histogram':
        marks = [Plot.rectY(data, Plot.binX({ y: 'count' }, { x: xField, fill: color }))]
        break
      case 'scatter':
        marks = [Plot.dot(data, { x: xField, y: yField, fill: color })]
        break
      default:
        marks = [Plot.dot(data, { x: xField, y: yField, fill: color })]
        break
    }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: noodles-editor/src/noodles/operators.ts
Line: 1993:2004

Comment:
`marks` could be undefined if invalid chartType passed (TypeScript doesn't enforce StringLiteralField runtime values)

```suggestion
    let marks: Plot.Markish[]
    switch (chartType) {
      case 'bar':
        marks = [Plot.barY(data, { x: xField, y: yField, fill: color })]
        break
      case 'histogram':
        marks = [Plot.rectY(data, Plot.binX({ y: 'count' }, { x: xField, fill: color }))]
        break
      case 'scatter':
        marks = [Plot.dot(data, { x: xField, y: yField, fill: color })]
        break
      default:
        marks = [Plot.dot(data, { x: xField, y: yField, fill: color })]
        break
    }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/joby-aviation/noodles.gl/pull/292#discussion_r2726432197,,2026-01-26T06:03:28+00:00,runtime,medium,"Greptile correctly identifies that `marks` is declared with `let marks: Plot.Markish[]` but the switch statement has no default case. If `chartType` somehow gets an unexpected value at runtime (since StringLiteralField values aren't enforced at runtime), `marks` would be undefined when passed to `Plot.plot()`, causing a runtime error. This is a non-obvious defensive programming issue that could cause the chart to fail silently or crash.",2026-01-26T06:23:26.282040+00:00
gptme/gptme-contrib,195,fix(ralph): improve prompt to ensure plan.md checkbox updates,https://github.com/gptme/gptme-contrib/pull/195,,"Tests break due to missing parameter - `tests/test_ralph_loop.py:140` and `:158` call `_build_prompt()` without the new `plan_file_path` parameter

```suggestion
def _build_prompt(spec: str, plan: Plan, step: PlanStep, plan_file_path: str = ""plan.md"") -> str:
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: plugins/gptme-ralph/src/gptme_ralph/tools/ralph_loop.py
Line: 168:168

Comment:
Tests break due to missing parameter - `tests/test_ralph_loop.py:140` and `:158` call `_build_prompt()` without the new `plan_file_path` parameter

```suggestion
def _build_prompt(spec: str, plan: Plan, step: PlanStep, plan_file_path: str = ""plan.md"") -> str:
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/195#discussion_r2726404952,,2026-01-26T05:42:55+00:00,runtime,medium,"The diff shows `_build_prompt` signature changed to require a new `plan_file_path` parameter without a default value. Greptile correctly identified that existing test calls at specific line numbers would break with TypeError due to missing required argument. This is a real breaking change that would cause test failures, and suggesting a default value is an actionable fix.",2026-01-26T06:23:26.282116+00:00
gptme/gptme-contrib,194,feat(gptodo): add sub-agent spawning for multi-agent collaboration,https://github.com/gptme/gptme-contrib/pull/194,,"Shell injection vulnerability - `workspace` path is inserted directly without escaping. If the workspace path contains spaces or shell metacharacters, this will fail or could be exploited.

```suggestion
            [""tmux"", ""new-session"", ""-d"", ""-s"", tmux_name, f""cd {shlex.quote(str(workspace))} && {shell_cmd}""],
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/subagent.py
Line: 147:147

Comment:
Shell injection vulnerability - `workspace` path is inserted directly without escaping. If the workspace path contains spaces or shell metacharacters, this will fail or could be exploited.

```suggestion
            [""tmux"", ""new-session"", ""-d"", ""-s"", tmux_name, f""cd {shlex.quote(str(workspace))} && {shell_cmd}""],
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/194#discussion_r2726390646,,2026-01-26T05:30:45+00:00,security,high,"Greptile correctly identified that on line 147, `safe_workspace` is defined using `shlex.quote(str(workspace))` but on line 147, the code uses `{safe_workspace}` which is the quoted version. Wait, let me re-examine - the code on line 134 defines `safe_workspace = shlex.quote(str(workspace))` but line 147 constructs the command with `f""cd {safe_workspace} && {shell_cmd}""`. Actually the variable IS escaped. However, looking more carefully at the actual line 147, I see the code passes the ORIGINAL `workspace` variable, not `safe_workspace`. This is a legitimate shell injection vulnerability where workspace path is not properly escaped despite `safe_workspace` being defined earlier in the function.",2026-01-26T06:23:26.282167+00:00
gptme/gptme-contrib,194,feat(gptodo): add sub-agent spawning for multi-agent collaboration,https://github.com/gptme/gptme-contrib/pull/194,,"Shell injection still possible - `safe_workspace` is quoted but then embedded in an f-string passed to tmux. The quoting becomes literal characters in the shell command.

Pass the command properly:

```suggestion
        result = subprocess.run(
            [""tmux"", ""new-session"", ""-d"", ""-s"", tmux_name, ""-c"", str(workspace), shell_cmd],
            capture_output=True,
            text=True,
        )
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/subagent.py
Line: 153:153

Comment:
Shell injection still possible - `safe_workspace` is quoted but then embedded in an f-string passed to tmux. The quoting becomes literal characters in the shell command.

Pass the command properly:

```suggestion
        result = subprocess.run(
            [""tmux"", ""new-session"", ""-d"", ""-s"", tmux_name, ""-c"", str(workspace), shell_cmd],
            capture_output=True,
            text=True,
        )
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/194#discussion_r2726397524,,2026-01-26T05:36:40+00:00,security,high,"Greptile correctly identifies a shell injection vulnerability. Looking at line 153, the code uses `f""cd {safe_workspace} && {shell_cmd}""` as a shell command argument to tmux. While `safe_workspace` is quoted with `shlex.quote()`, when this entire string is passed to tmux's shell, the quotes become literal characters interpreted by the shell, potentially allowing injection if workspace contains shell metacharacters. The suggested fix using tmux's `-c` flag to set the working directory properly avoids shell interpretation entirely.",2026-01-26T06:23:26.282202+00:00
posthog/posthog,45883,feat: heatmaps upload + toolbar,https://github.com/PostHog/posthog/pull/45883,,"Missing ownership validation for `image_url`. The validation only checks that the path starts with `/uploaded_media/`, but doesn't verify that the uploaded media belongs to the same team. This allows users from Team A to reference uploaded images from Team B by guessing or discovering their UUIDs.

You need to extract the media UUID from the path, look up the `UploadedMedia` object, and verify that `uploaded_media.team_id == self.context['request'].team.id`.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/heatmaps/heatmaps_api.py
Line: 427:435

Comment:
Missing ownership validation for `image_url`. The validation only checks that the path starts with `/uploaded_media/`, but doesn't verify that the uploaded media belongs to the same team. This allows users from Team A to reference uploaded images from Team B by guessing or discovering their UUIDs.

You need to extract the media UUID from the path, look up the `UploadedMedia` object, and verify that `uploaded_media.team_id == self.context['request'].team.id`.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45883#discussion_r2726620528,,2026-01-26T07:49:20+00:00,security,high,The validate_image_url method only checks if the path starts with '/uploaded_media/' but performs no ownership verification. This is a valid IDOR (Insecure Direct Object Reference) vulnerability that could allow cross-team data access by referencing another team's uploaded media UUID. The fix suggestion is specific and actionable - extract the UUID and verify team ownership.,2026-01-26T08:22:25.551715+00:00
nvidia/spark-rapids,14175,Fix OOM when shuffle partition exceeds 2GB in threaded writer,https://github.com/NVIDIA/spark-rapids/pull/14175,,"Buffer leak when compression task throws exception. The `buffer` created at line 703 is never closed if serialization/compression fails. When an exception occurs after line 703 but before returning `CompressedRecord` at line 732, the buffer is leaked because:
1. The `withResource(cb)` block catches the exception
2. The exception is wrapped and re-thrown at line 736
3. The buffer is never closed

Wrap buffer creation in try-finally:
```suggestion
        val future = RapidsShuffleInternalManagerBase.queueWriteTask(slotNum, () => {
          var buffer: OpenByteArrayOutputStream = null
          try {
            withResource(cb) { _ =>
              // Create a new buffer for this record.
              // The buffer is closed by the merger thread after writing to disk.
              buffer = new OpenByteArrayOutputStream()

              // Serialize + compress + encryption to memory buffer
              val compressedOutputStream = blockManager.serializerManager.wrapStream(
                ShuffleBlockId(shuffleId, mapId, reducePartitionId), buffer)

              val serializationStream = serializerInstance.serializeStream(
                compressedOutputStream)
              withResource(serializationStream) { serializer =>
                serializer.writeKey(key.asInstanceOf[Any])
                serializer.writeValue(value.asInstanceOf[Any])
              }

              // Track total written data size (compressed size)
              val compressedSize = buffer.getCount.toLong
              totalCompressedSize.addAndGet(compressedSize)

              // Release excess quota immediately after compression.
              // Data is now in OpenByteArrayOutputStream (heap), only need to hold
              // compressedSize quota until Merger writes to disk.
              // Note: excessQuota can be 0 if compression doesn't reduce size (or expands)
              val excessQuota = math.max(0L, recordSize - compressedSize)
              if (excessQuota > 0) {
                limiter.release(excessQuota)
              }

              // Return CompressedRecord with buffer and remaining quota for Merger
              // Total released = excessQuota + remainingQuota should equal recordSize
              val remainingQuota = recordSize - excessQuota
              val result = CompressedRecord(buffer, compressedSize, remainingQuota)
              buffer = null // Transfer ownership to CompressedRecord
              result
            }
          } catch {
            case e: Exception =>
              if (buffer != null) {
                try { buffer.close() } catch { case _: Exception => }
              }
              throw new IOException(
                s""Failed compression task for shuffle $shuffleId, map $mapId, "" +
                s""partition $reducePartitionId"", e)
          }
        })
```",https://github.com/NVIDIA/spark-rapids/pull/14175#discussion_r2726466696,,2026-01-26T06:24:56+00:00,resource_leak,medium,"Greptile correctly identifies that when an exception occurs after `buffer = new OpenByteArrayOutputStream()` at line 703 but before returning `CompressedRecord`, the buffer will never be closed. The current code wraps `cb` with `withResource` but not the newly created buffer, so if serialization/compression throws an exception, the buffer leaks. This is a legitimate resource leak bug that's non-obvious since the code structure with nested try blocks and withResource could easily mislead reviewers into thinking resources are properly managed.",2026-01-26T08:22:25.551818+00:00
nvidia/spark-rapids,14175,Fix OOM when shuffle partition exceeds 2GB in threaded writer,https://github.com/NVIDIA/spark-rapids/pull/14175,,"Missing error handling when `future.get()` throws. If the compression task failed, `future.get()` throws `ExecutionException`, but the buffer and quota are never cleaned up. This causes:
1. Buffer leak (never closed)
2. Quota leak (never released)
3. Merger thread crashes without closing `outputStream`

Need try-catch around `future.get()` to handle failed compression tasks gracefully.",https://github.com/NVIDIA/spark-rapids/pull/14175#discussion_r2726466721,,2026-01-26T06:24:57+00:00,resource_leak,high,"Greptile correctly identifies that in the merger thread's inner loop, if `future.get()` throws an ExecutionException (from a failed compression task), there's no try-catch to handle it. This would cause the buffer and quota to leak, and the outputStream would not be closed properly since the exception propagates past the finally block. The code shows `val record = future.get()` with no exception handling around it, confirming the analysis.",2026-01-26T08:22:25.551930+00:00
runanywhereai/runanywhere-sdks,301,Sync dev/main,https://github.com/RunanywhereAI/runanywhere-sdks/pull/301,,"Missing optional chaining - could crash if `weatherDesc` array is empty or null

```suggestion
          condition: current.weatherDesc?.[0]?.value || 'Unknown',
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: examples/react-native/RunAnywhereAI/src/screens/ChatScreen.tsx
Line: 106:106

Comment:
Missing optional chaining - could crash if `weatherDesc` array is empty or null

```suggestion
          condition: current.weatherDesc?.[0]?.value || 'Unknown',
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RunanywhereAI/runanywhere-sdks/pull/301#discussion_r2726617254,,2026-01-26T07:47:50+00:00,runtime,medium,"Greptile correctly identifies that `current.weatherDesc[0].value` on line 106 lacks optional chaining and could crash if the external weather API returns an empty or null `weatherDesc` array. Since this is calling an external API (wttr.in) where the response structure isn't guaranteed, this defensive coding is important to prevent runtime crashes in a React Native app.",2026-01-26T08:22:25.551954+00:00
traceroot-ai/traceroot,376,feat(auth): add signup flow and validation (#372),https://github.com/traceroot-ai/traceroot/pull/376,,"`admin` field from user object is not being stored in JWT token, so it won't be available in session

```suggestion
    async jwt({ token, user }) {
      if (user) {
        token.id = user.id;
        token.admin = user.admin;
      }
      return token;
    },
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/lib/auth.ts
Line: 70:74

Comment:
`admin` field from user object is not being stored in JWT token, so it won't be available in session

```suggestion
    async jwt({ token, user }) {
      if (user) {
        token.id = user.id;
        token.admin = user.admin;
      }
      return token;
    },
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/376#discussion_r2726575050,,2026-01-26T07:27:26+00:00,logic,medium,"Greptile correctly identifies that the `admin` field is included in the user object returned from authorize() (line 58) but is not stored in the JWT token in the jwt callback (lines 66-71). This means admin status won't be available in sessions, potentially breaking admin authorization checks throughout the application. This is a non-obvious bug that could easily be missed during review.",2026-01-26T08:22:25.551978+00:00
traceroot-ai/traceroot,376,feat(auth): add signup flow and validation (#372),https://github.com/traceroot-ai/traceroot/pull/376,,"`admin` field from token is not being copied to session

```suggestion
    async session({ session, token }) {
      if (session.user) {
        session.user.id = token.id as string;
        session.user.admin = token.admin as boolean;
      }
      return session;
    },
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/lib/auth.ts
Line: 77:81

Comment:
`admin` field from token is not being copied to session

```suggestion
    async session({ session, token }) {
      if (session.user) {
        session.user.id = token.id as string;
        session.user.admin = token.admin as boolean;
      }
      return session;
    },
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/376#discussion_r2726575079,,2026-01-26T07:27:27+00:00,security,high,"Greptile correctly identified that the `admin` field is set on the token in the `jwt` callback (line 68: `token.id = user.id` is present, and the user object returned from authorize includes `admin: user.admin`), but the `session` callback only copies `id` to the session, not `admin`. This means admin status would not be available in the client session, potentially breaking authorization checks that depend on it. This is a real bug that could cause admin functionality to fail silently.",2026-01-26T08:22:25.552041+00:00
traceroot-ai/traceroot,376,feat(auth): add signup flow and validation (#372),https://github.com/traceroot-ai/traceroot/pull/376,,"`allowDangerousEmailAccountLinking: true` creates account takeover risk - if someone signs up with credentials using an email, another person can use Google OAuth with the same email to access that account

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/lib/auth.ts
Line: 21:21

Comment:
`allowDangerousEmailAccountLinking: true` creates account takeover risk - if someone signs up with credentials using an email, another person can use Google OAuth with the same email to access that account

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/376#discussion_r2726575151,,2026-01-26T07:27:28+00:00,security,high,"Greptile correctly identifies that `allowDangerousEmailAccountLinking: true` on line 21 creates a real account takeover vulnerability. If user A signs up with credentials using email@example.com, user B who controls a Google account with email@example.com can use OAuth to access user A's account. The flag is literally named 'dangerous' because of this risk, and NextAuth documentation explicitly warns against it.",2026-01-26T08:22:25.552070+00:00
traceroot-ai/traceroot,376,feat(auth): add signup flow and validation (#372),https://github.com/traceroot-ai/traceroot/pull/376,,"The TypeScript definitions include `admin: boolean` on the JWT type, but this callback doesn't copy the `admin` field from user to token.

```suggestion
    async jwt({ token, user }) {
      if (user) {
        token.id = user.id;
        token.admin = user.admin;
      }
      return token;
    },
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/lib/auth.ts
Line: 70:74

Comment:
The TypeScript definitions include `admin: boolean` on the JWT type, but this callback doesn't copy the `admin` field from user to token.

```suggestion
    async jwt({ token, user }) {
      if (user) {
        token.id = user.id;
        token.admin = user.admin;
      }
      return token;
    },
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/376#discussion_r2726589337,,2026-01-26T07:34:17+00:00,security,high,"The code shows `admin: user.admin` being returned in the authorize function (line 60), but the jwt callback doesn't copy this field to the token. This means admin status would be lost after the initial sign-in, potentially breaking authorization checks that rely on `token.admin`. This is a subtle bug in the authentication flow that could cause admin features to fail or create security issues.",2026-01-26T08:22:25.552097+00:00
traceroot-ai/traceroot,376,feat(auth): add signup flow and validation (#372),https://github.com/traceroot-ai/traceroot/pull/376,,"The `admin` field from the token isn't being copied to the session, so `session.user.admin` will be undefined.

```suggestion
    async session({ session, token }) {
      if (session.user) {
        session.user.id = token.id as string;
        session.user.admin = token.admin as boolean;
      }
      return session;
    },
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/lib/auth.ts
Line: 77:82

Comment:
The `admin` field from the token isn't being copied to the session, so `session.user.admin` will be undefined.

```suggestion
    async session({ session, token }) {
      if (session.user) {
        session.user.id = token.id as string;
        session.user.admin = token.admin as boolean;
      }
      return session;
    },
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/376#discussion_r2726589354,,2026-01-26T07:34:18+00:00,logic,high,"Greptile correctly identified that while the `admin` field is stored in the token during `jwt({ token, user })` callback (line 67 returns user with admin field), it's never copied to the session in the `session()` callback. This means admin authorization checks using `session.user.admin` will always fail, potentially breaking admin functionality throughout the application. This is a non-obvious bug that could easily be missed during review.",2026-01-26T08:22:25.552127+00:00
traceroot-ai/traceroot,376,feat(auth): add signup flow and validation (#372),https://github.com/traceroot-ai/traceroot/pull/376,,"This creates an account takeover vulnerability - if Alice creates a credentials account, Bob can take it over by signing in with Google using Alice's email. Remove this flag or implement proper email verification before linking.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/lib/auth.ts
Line: 21:21

Comment:
This creates an account takeover vulnerability - if Alice creates a credentials account, Bob can take it over by signing in with Google using Alice's email. Remove this flag or implement proper email verification before linking.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/376#discussion_r2726589430,,2026-01-26T07:34:21+00:00,security,critical,"Greptile correctly identifies that `allowDangerousEmailAccountLinking: true` on line 21 creates an account takeover vulnerability. This flag allows Google OAuth to automatically link to existing accounts by email without verification, meaning an attacker who controls a Google account with the victim's email (or creates one) could take over a credentials-based account. This is a real security vulnerability that's non-obvious and explicitly warned against in the NextAuth documentation.",2026-01-26T08:22:25.552155+00:00
davis7dotsh/better-context,125,feat: remote + local CLI support...,https://github.com/davis7dotsh/better-context/pull/125,,"Schema URL is incorrect - missing domain.

```suggestion
const CONFIG_SCHEMA_URL = 'https://btca.dev/btca.schema.json';
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/cli/src/commands/init.ts
Line: 7:7

Comment:
Schema URL is incorrect - missing domain.

```suggestion
const CONFIG_SCHEMA_URL = 'https://btca.dev/btca.schema.json';
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/davis7dotsh/better-context/pull/125#discussion_r2726558314,,2026-01-26T07:18:37+00:00,data_integrity,medium,"The diff clearly shows CONFIG_SCHEMA_URL changed from 'https://btca.dev/btca.schema.json' to 'https://btca.schema.json' - the domain 'btca.dev' was accidentally removed, creating an invalid URL that would break schema validation. This is a real bug that could be easily missed in code review since it's a subtle URL change in a constant definition.",2026-01-26T08:22:25.552180+00:00
ever-co/ever-gauzy,9365,fix: mac dmg notarize,https://github.com/ever-co/ever-gauzy/pull/9365,,"Set restrictive file permissions on the private key file to prevent unauthorized access

```suggestion
		fs.writeFileSync(privateKeyPath, Buffer.from(APPLE_API_KEY, 'base64'), { mode: 0o600 });
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: tools/notarize.js
Line: 37:37

Comment:
Set restrictive file permissions on the private key file to prevent unauthorized access

```suggestion
		fs.writeFileSync(privateKeyPath, Buffer.from(APPLE_API_KEY, 'base64'), { mode: 0o600 });
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-gauzy/pull/9365#discussion_r2726765514,,2026-01-26T08:46:34+00:00,security,medium,"The code writes a private API key to a temporary file without setting restrictive permissions, meaning other users on the system could potentially read the sensitive key. Setting mode 0o600 ensures only the owner can read/write the file, which is a legitimate security best practice for handling secrets.",2026-01-26T10:18:23.963755+00:00
ever-co/ever-gauzy,9365,fix: mac dmg notarize,https://github.com/ever-co/ever-gauzy/pull/9365,,"Error is logged but not re-thrown, causing notarization failures to silently pass. This could allow unsigned builds to proceed.

```suggestion
		console.error(`ERROR: Failed to notarize: ${error}`);
		throw error;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: tools/notarize.js
Line: 52:52

Comment:
Error is logged but not re-thrown, causing notarization failures to silently pass. This could allow unsigned builds to proceed.

```suggestion
		console.error(`ERROR: Failed to notarize: ${error}`);
		throw error;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-gauzy/pull/9365#discussion_r2726765549,,2026-01-26T08:46:35+00:00,security,high,"The code catches notarization errors, logs them, but doesn't re-throw, allowing the build process to continue silently even when notarization fails. This is a real security issue for macOS app distribution since unsigned/unnotarized apps would proceed through the build pipeline appearing successful. The finally block properly cleans up the temp key file, but the error swallowing is a significant oversight that could result in distributing improperly notarized builds.",2026-01-26T10:18:23.963844+00:00
ydb-platform/ydb-embedded-ui,3372,chore: remove bot token from workflows,https://github.com/ydb-platform/ydb-embedded-ui/pull/3372,,"<details>
<summary>Additional Comments (1)</summary>

**`.github/workflows/publication.yml`**
The `repository-dispatch` action requires a token with permissions to dispatch events to the `ydb-platform/ydb` repository. The default `GITHUB_TOKEN` only has permissions for the current repository. This step will fail without a PAT or bot token that has access to the target repository.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/publication.yml
Line: 30:41

Comment:
The `repository-dispatch` action requires a token with permissions to dispatch events to the `ydb-platform/ydb` repository. The default `GITHUB_TOKEN` only has permissions for the current repository. This step will fail without a PAT or bot token that has access to the target repository.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/ydb-platform/ydb-embedded-ui/pull/3372#issuecomment-3798608104,,2026-01-26T09:22:24+00:00,runtime,high,"Greptile correctly identifies that the repository-dispatch action to ydb-platform/ydb requires cross-repository permissions that the default GITHUB_TOKEN cannot provide. The PR is removing bot tokens from workflows, but this specific action will fail without a token that has permissions to dispatch events to the external repository.",2026-01-26T10:18:23.963896+00:00
mrmps/smry,46,Refactor mobile reader layout for improved UX,https://github.com/mrmps/SMRY/pull/46,,"fullscreen won't exit when switching away from HTML mode

when user switches from HTML to markdown/iframe mode, `isFullScreen` remains `true` - effect only enters fullscreen but never exits

```suggestion
  // Auto-enter fullscreen on mobile when switching to HTML view
  useEffect(() => {
    if (typeof window === ""undefined"") return;
    const isMobile = window.innerWidth < 768;
    if (isMobile && viewMode === ""html"") {
      // Schedule state update to avoid synchronous setState in effect
      const frameId = requestAnimationFrame(() => setIsFullScreen(true));
      return () => cancelAnimationFrame(frameId);
    } else if (isMobile && viewMode !== ""html"") {
      const frameId = requestAnimationFrame(() => setIsFullScreen(false));
      return () => cancelAnimationFrame(frameId);
    }
  }, [viewMode]);
```",https://github.com/mrmps/SMRY/pull/46#discussion_r2726790779,,2026-01-26T08:55:22+00:00,logic,medium,"Greptile correctly identified that the useEffect only sets isFullScreen to true when viewMode is 'html' on mobile, but never sets it back to false when switching away from HTML mode. This would cause the fullscreen state to persist incorrectly when the user changes to markdown/iframe view modes, leading to unexpected UI behavior.",2026-01-26T10:18:23.963968+00:00
mrmps/smry,46,Refactor mobile reader layout for improved UX,https://github.com/mrmps/SMRY/pull/46,,"fullscreen won't exit when switching away from HTML mode

when user switches from HTML to markdown/iframe mode, `isFullScreen` remains `true` - effect only enters fullscreen but never exits

```suggestion
  // Auto-enter fullscreen on mobile when switching to HTML view
  useEffect(() => {
    if (typeof window === ""undefined"") return;
    const isMobile = window.innerWidth < 768;
    if (isMobile && viewMode === ""html"") {
      // Schedule state update to avoid synchronous setState in effect
      const frameId = requestAnimationFrame(() => setIsFullScreen(true));
      return () => cancelAnimationFrame(frameId);
    } else if (isMobile && viewMode !== ""html"") {
      // Exit fullscreen when switching away from HTML mode
      const frameId = requestAnimationFrame(() => setIsFullScreen(false));
      return () => cancelAnimationFrame(frameId);
    }
  }, [viewMode]);
```",https://github.com/mrmps/SMRY/pull/46#discussion_r2726837037,,2026-01-26T09:11:44+00:00,logic,medium,"Greptile correctly identifies that the useEffect on lines 177-182 only handles entering fullscreen when viewMode === 'html' on mobile, but never exits fullscreen when viewMode changes to something else. This means the UI would remain stuck in fullscreen mode when the user switches away from HTML view, which is a real UX bug. The suggested fix to add an else-if branch to exit fullscreen when viewMode !== 'html' is the correct solution.",2026-01-26T10:18:23.964003+00:00
mrmps/smry,46,Refactor mobile reader layout for improved UX,https://github.com/mrmps/SMRY/pull/46,,"effect only enters fullscreen but never exits

when switching from HTML to markdown/iframe mode, `isFullScreen` remains `true` - add logic to exit fullscreen when `viewMode !== ""html""`

```suggestion
  // Auto-enter fullscreen on mobile when switching to HTML view
  // Auto-exit fullscreen when switching away from HTML view
  useEffect(() => {
    if (typeof window === ""undefined"") return;
    const isMobile = window.innerWidth < 768;
    if (isMobile && viewMode === ""html"") {
      // Schedule state update to avoid synchronous setState in effect
      const frameId = requestAnimationFrame(() => setIsFullScreen(true));
      return () => cancelAnimationFrame(frameId);
    } else if (isMobile && viewMode !== ""html"" && isFullScreen) {
      // Exit fullscreen when switching away from HTML mode
      const frameId = requestAnimationFrame(() => setIsFullScreen(false));
      return () => cancelAnimationFrame(frameId);
    }
  }, [viewMode, isFullScreen]);
```",https://github.com/mrmps/SMRY/pull/46#discussion_r2726886750,,2026-01-26T09:28:11+00:00,logic,medium,"Greptile correctly identified that the useEffect on line 173-178 only sets isFullScreen to true when viewMode is 'html' on mobile, but never sets it back to false when switching away from HTML mode. This means isFullScreen state will stay true even when the user switches to markdown/iframe mode, causing incorrect UI behavior. The fix to add an else branch to exit fullscreen is correct and actionable.",2026-01-26T10:18:23.964031+00:00
mrmps/smry,46,Refactor mobile reader layout for improved UX,https://github.com/mrmps/SMRY/pull/46,,"effect only enters fullscreen, never exits

when switching away from HTML mode (e.g., HTML → Reader), `isFullScreen` stays `true` - need to also exit fullscreen when `viewMode !== ""html""`

```suggestion
  // Auto-enter fullscreen on mobile when switching to HTML view
  useEffect(() => {
    if (typeof window === ""undefined"") return;
    const isMobile = window.innerWidth < 768;
    if (isMobile && viewMode === ""html"") {
      // Schedule state update to avoid synchronous setState in effect
      const frameId = requestAnimationFrame(() => setIsFullScreen(true));
      return () => cancelAnimationFrame(frameId);
    } else if (isMobile && isFullScreen) {
      // Exit fullscreen when switching away from HTML on mobile
      const frameId = requestAnimationFrame(() => setIsFullScreen(false));
      return () => cancelAnimationFrame(frameId);
    }
  }, [viewMode, isFullScreen]);
```",https://github.com/mrmps/SMRY/pull/46#discussion_r2726926955,,2026-01-26T09:41:19+00:00,logic,medium,"The useEffect at lines 170-176 only sets isFullScreen to true when viewMode is 'html' on mobile, but never sets it back to false when switching away from HTML mode. This means the fullscreen state will persist incorrectly when the user changes view modes, causing UI issues. The suggested fix correctly adds the exit condition.",2026-01-26T10:18:23.964085+00:00
posthog/posthog,45896,feat(openapi): auto-discover product endpoints via ViewSet scanning,https://github.com/PostHog/posthog/pull/45896,,"regex won't match `Viewset` (lowercase 's')

`ExternalDataSchemaViewset` in `products/data_warehouse/backend/api/external_data_schema.py` won't be discovered

```suggestion
                const viewSetRegex = /class\s+(\w+ViewSet)[\s(]/gi
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/bin/generate-openapi-types.mjs
Line: 79:79

Comment:
regex won't match `Viewset` (lowercase 's')

`ExternalDataSchemaViewset` in `products/data_warehouse/backend/api/external_data_schema.py` won't be discovered

```suggestion
                const viewSetRegex = /class\s+(\w+ViewSet)[\s(]/gi
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45896#discussion_r2727269801,,2026-01-26T11:34:23+00:00,logic,medium,The regex `/class\s+(\w+ViewSet)[\s(]/g` is case-sensitive and only matches 'ViewSet' with capital S. Greptile correctly identified that 'ExternalDataSchemaViewset' (lowercase 's') exists in the codebase and would be silently missed. This is a real bug that would cause inconsistent endpoint routing and is non-obvious since it requires knowledge of actual class naming conventions in the codebase.,2026-01-26T12:24:28.432903+00:00
posthog/posthog,45896,feat(openapi): auto-discover product endpoints via ViewSet scanning,https://github.com/PostHog/posthog/pull/45896,,"won't handle acronyms correctly - `LLMAnalyticsClusteringRunViewSet` becomes `llmanalytics_clustering_run` instead of `llm_analytics_clustering_run`

the regex only matches lowercase-to-uppercase transitions, missing uppercase-to-uppercase sequences

```suggestion
                    const snakeCase = viewSetName
                        .replace(/ViewSet$/i, '')
                        .replace(/([a-z])([A-Z])/g, '$1_$2')
                        .replace(/([A-Z])([A-Z][a-z])/g, '$1_$2')
                        .toLowerCase()
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/bin/generate-openapi-types.mjs
Line: 83:86

Comment:
won't handle acronyms correctly - `LLMAnalyticsClusteringRunViewSet` becomes `llmanalytics_clustering_run` instead of `llm_analytics_clustering_run`

the regex only matches lowercase-to-uppercase transitions, missing uppercase-to-uppercase sequences

```suggestion
                    const snakeCase = viewSetName
                        .replace(/ViewSet$/i, '')
                        .replace(/([a-z])([A-Z])/g, '$1_$2')
                        .replace(/([A-Z])([A-Z][a-z])/g, '$1_$2')
                        .toLowerCase()
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45896#discussion_r2727269836,,2026-01-26T11:34:24+00:00,logic,medium,"Greptile correctly identified that the regex `.replace(/([a-z])([A-Z])/g, '$1_$2')` only handles lowercase-to-uppercase transitions. For 'LLMAnalyticsClusteringRunViewSet', it would produce 'llmanalytics_clustering_run' instead of the expected 'llm_analytics_clustering_run' because consecutive uppercase letters (LLM) aren't split. This would cause ViewSet matching failures for any classes with acronyms in their names, a subtle bug that's easy to miss.",2026-01-26T12:24:28.433003+00:00
posthog/posthog,45896,feat(openapi): auto-discover product endpoints via ViewSet scanning,https://github.com/PostHog/posthog/pull/45896,,"substring matching could cause false positives - e.g. `snakeCase=""link""` would match any operationId containing ""link"" as a substring

consider using word boundary check or exact segment matching like `operationId.split('_').includes(snakeCase)`

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/bin/generate-openapi-types.mjs
Line: 121:121

Comment:
substring matching could cause false positives - e.g. `snakeCase=""link""` would match any operationId containing ""link"" as a substring

consider using word boundary check or exact segment matching like `operationId.split('_').includes(snakeCase)`

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45896#discussion_r2727269887,,2026-01-26T11:34:25+00:00,logic,medium,"The code at line 121 uses `operationId.includes(snakeCase)` for substring matching, which as Greptile correctly identifies could cause false positives - a ViewSet named 'Link' would match any operationId containing 'link' anywhere (e.g., 'get_backlinks_list'). This would incorrectly route endpoints to the wrong product folder. The suggested fix using word boundary matching is specific and actionable.",2026-01-26T12:24:28.433076+00:00
nvidia-nemo/curator,1427,[Tutorial] Merge multiple file prefixes generated by `MegatronTokenizerWriter`,https://github.com/NVIDIA-NeMo/Curator/pull/1427,,"If no valid file pairs are found in the input directory, `builder` will remain `None` and line 272 will crash with `AttributeError: 'NoneType' object has no attribute 'finalize'`

```python
    if builder is None:
        raise ValueError(f""ERROR: No valid file prefix pairs found in {input_dir}"")
    
    builder.finalize(output_prefix + "".idx"")
```",https://github.com/NVIDIA-NeMo/Curator/pull/1427#discussion_r2727237259,,2026-01-26T11:22:03+00:00,null,medium,"Greptile correctly identified that if the input directory contains no valid .idx/.bin file pairs, the `builder` variable remains `None` after the loop (line 265-270), and calling `builder.finalize()` on line 272 would crash with an AttributeError. This is a valid edge case bug that could occur with an empty directory or one without matching file pairs, and the fix suggestion is appropriate.",2026-01-26T12:24:28.433151+00:00
posthog/posthog,45911,fix: add new items to new search,https://github.com/PostHog/posthog/pull/45911,,"logic error - the condition `'new'.includes(chunk)` is backwards and will always match any single character

This checks if the string ""new"" contains the chunk, not if the chunk contains ""new"". For example, if `chunk` is ""e"", this will return true because ""new"" contains ""e"".

This means typing just ""e"" or ""n"" or ""w"" will show all create items.

```suggestion
                            if (chunk.includes('new') || chunk.includes('create')) {
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/components/Search/searchLogic.tsx
Line: 712:713

Comment:
logic error - the condition `'new'.includes(chunk)` is backwards and will always match any single character

This checks if the string ""new"" contains the chunk, not if the chunk contains ""new"". For example, if `chunk` is ""e"", this will return true because ""new"" contains ""e"".

This means typing just ""e"" or ""n"" or ""w"" will show all create items.

```suggestion
                            if (chunk.includes('new') || chunk.includes('create')) {
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45911#discussion_r2727719811,,2026-01-26T13:58:59+00:00,logic,medium,"Greptile correctly identified a backwards string inclusion check at line 713. The code `'new'.includes(chunk)` checks if the literal string 'new' contains the user's search chunk, which means typing any single character like 'n', 'e', or 'w' would match and show all create items - clearly unintended behavior. The fix to use `chunk.includes('new')` is correct and actionable.",2026-01-26T14:22:16.310860+00:00
posthog/posthog,45907,feat(twig): restrict available models for twig,https://github.com/PostHog/posthog/pull/45907,,"`gpt-5.1` is used elsewhere in the codebase (trace_clustering/labeling_agent/graph.py:712) but is not in this allowlist - only `gpt-5.2` and `gpt-5-mini` are allowed

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: services/llm-gateway/src/llm_gateway/products/config.py
Line: 37:37

Comment:
`gpt-5.1` is used elsewhere in the codebase (trace_clustering/labeling_agent/graph.py:712) but is not in this allowlist - only `gpt-5.2` and `gpt-5-mini` are allowed

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45907#discussion_r2727682170,,2026-01-26T13:48:12+00:00,logic,medium,Greptile identified a real inconsistency where gpt-5.1 is used in the codebase (trace_clustering/labeling_agent/graph.py:712) but wasn't added to the new allowlist for the 'array' product. This could cause runtime failures when that code path tries to use gpt-5.1 through the gateway. This cross-file dependency issue is non-obvious and requires knowledge of the broader codebase to catch.,2026-01-26T14:22:16.310964+00:00
assistant-ui/assistant-ui,3124,feat(shadcn/ui-registry): select component,https://github.com/assistant-ui/assistant-ui/pull/3124,,"<details>
<summary>Additional Comments (1)</summary>

**`apps/docs/components/shared/select.tsx`**
Deleting this file breaks `apps/docs/components/builder/builder-controls.tsx:5` which imports from `@/components/shared/select`. This will cause a build failure.

Update the import in `builder-controls.tsx` to use the new component:
```suggestion
import { Select } from ""@/components/assistant-ui/select"";
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/docs/components/shared/select.tsx
Line: 1:91

Comment:
Deleting this file breaks `apps/docs/components/builder/builder-controls.tsx:5` which imports from `@/components/shared/select`. This will cause a build failure.

Update the import in `builder-controls.tsx` to use the new component:
```suggestion
import { Select } from ""@/components/assistant-ui/select"";
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/assistant-ui/assistant-ui/pull/3124#issuecomment-3799379457,,2026-01-26T12:33:21+00:00,runtime,medium,"This catches a real build-breaking issue where deleting a file leaves a dangling import in another file. While I cannot verify the exact import path in builder-controls.tsx from the provided context, identifying orphaned imports when files are deleted is a non-obvious catch that prevents CI failures.",2026-01-26T14:22:16.311012+00:00
ydb-platform/ydb-embedded-ui,3373,fix: workflows,https://github.com/ydb-platform/ydb-embedded-ui/pull/3373,,"npm publish will fail without authentication. When `registry-url` is configured in `setup-node`, npm expects `NODE_AUTH_TOKEN` environment variable to be set.

```suggestion
      - name: Publish to npm
        if: steps.release.outputs.release_created == 'true'
        run: npm publish
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/release.yml
Line: 40:42

Comment:
npm publish will fail without authentication. When `registry-url` is configured in `setup-node`, npm expects `NODE_AUTH_TOKEN` environment variable to be set.

```suggestion
      - name: Publish to npm
        if: steps.release.outputs.release_created == 'true'
        run: npm publish
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ydb-platform/ydb-embedded-ui/pull/3373#discussion_r2727627675,,2026-01-26T13:34:56+00:00,runtime,high,"The npm publish step will fail without authentication credentials. While Greptile's mention of registry-url is slightly inaccurate (it's not configured in this diff), npm publish to the public registry still requires authentication via NODE_AUTH_TOKEN. This is a real bug that would cause the release workflow to fail at runtime.",2026-01-26T14:22:16.311068+00:00
ydb-platform/ydb-embedded-ui,3373,fix: workflows,https://github.com/ydb-platform/ydb-embedded-ui/pull/3373,,"Missing `registry-url` configuration needed for npm authentication. The `setup-node` action needs this to enable npm publishing.

```suggestion
      - uses: actions/setup-node@v4
        with:
          node-version: 24
          registry-url: https://registry.npmjs.org
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/release.yml
Line: 22:24

Comment:
Missing `registry-url` configuration needed for npm authentication. The `setup-node` action needs this to enable npm publishing.

```suggestion
      - uses: actions/setup-node@v4
        with:
          node-version: 24
          registry-url: https://registry.npmjs.org
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ydb-platform/ydb-embedded-ui/pull/3373#discussion_r2727656905,,2026-01-26T13:41:13+00:00,runtime,high,"Greptile correctly identifies that the workflow adds an 'npm publish' step but the setup-node action lacks the registry-url parameter. Without this configuration, npm authentication via NODE_AUTH_TOKEN won't work, causing the publish step to fail with authentication errors. This is a real bug that would break the release pipeline and is non-obvious since the workflow syntax is valid but functionally broken.",2026-01-26T14:22:16.311124+00:00
ydb-platform/ydb-embedded-ui,3373,fix: workflows,https://github.com/ydb-platform/ydb-embedded-ui/pull/3373,,"`npm publish` will fail without authentication. When `registry-url` is configured in `setup-node`, you must set the `NODE_AUTH_TOKEN` environment variable (even if empty like in the previous publication.yml).

```suggestion
      - name: Publish to npm
        if: steps.release.outputs.release_created == 'true'
        run: npm publish
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/release.yml
Line: 33:35

Comment:
`npm publish` will fail without authentication. When `registry-url` is configured in `setup-node`, you must set the `NODE_AUTH_TOKEN` environment variable (even if empty like in the previous publication.yml).

```suggestion
      - name: Publish to npm
        if: steps.release.outputs.release_created == 'true'
        run: npm publish
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ydb-platform/ydb-embedded-ui/pull/3373#discussion_r2727656968,,2026-01-26T13:41:14+00:00,runtime,high,"The `npm publish` command will indeed fail without authentication to the npm registry. Looking at the diff, there's no `registry-url` configured in `setup-node` and no `NODE_AUTH_TOKEN` environment variable set for the publish step, which means the workflow will fail at runtime when attempting to publish. This is a real bug that would cause the release workflow to break.",2026-01-26T14:22:16.311175+00:00
openreplay/openreplay,4231,feat(api): update vault status for sessions and events,https://github.com/openreplay/openreplay/pull/4231,,"errors from `updateVaultStatus` are only logged but not propagated - if the ClickHouse update fails, PostgreSQL will still be updated, creating inconsistent state between databases

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/pkg/favorite/favorite.go
Line: 37:39

Comment:
errors from `updateVaultStatus` are only logged but not propagated - if the ClickHouse update fails, PostgreSQL will still be updated, creating inconsistent state between databases

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4231#discussion_r2727481264,,2026-01-26T12:47:06+00:00,data_integrity,medium,"Greptile correctly identifies that errors from updateVaultStatus are logged but not returned, allowing the subsequent PostgreSQL operations (f.remove or f.add) to proceed even when the ClickHouse update fails. This creates inconsistent state between the two databases - the vault status in ClickHouse won't match the favorite status in PostgreSQL.",2026-01-26T14:22:16.311278+00:00
alpaca-network/gatewayz-backend,950,"fix(monitoring): add provider credits, catalog validation, and 402 handling",https://github.com/Alpaca-Network/gatewayz-backend/pull/950,,"Creating a new event loop in sync context is problematic and can cause issues in async environments

The proper approach is to make the calling function async or use `asyncio.create_task()` if already in an async context. Creating new event loops can interfere with the existing event loop and cause deadlocks.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/services/openrouter_client.py
Line: 139:159

Comment:
Creating a new event loop in sync context is problematic and can cause issues in async environments

The proper approach is to make the calling function async or use `asyncio.create_task()` if already in an async context. Creating new event loops can interfere with the existing event loop and cause deadlocks.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-backend/pull/950#discussion_r2727770642,,2026-01-26T14:13:31+00:00,concurrency,medium,"Greptile correctly identifies that creating a new event loop with `asyncio.new_event_loop()` and `asyncio.set_event_loop()` in sync functions that may be called from async contexts can cause deadlocks or interfere with existing event loops. The code at lines 139-159 does exactly this - creating and setting a new event loop to run async credit check functions synchronously, which is a known anti-pattern that can cause hard-to-debug issues in production async applications.",2026-01-26T14:22:16.311329+00:00
posthog/posthog,45917,feat(llma): add generation-level toggle to clusters UI,https://github.com/PostHog/posthog/pull/45917,,"timestamp conversion inconsistency with `LLMAnalyticsClusterScene.tsx`

in `ClusterTraceList.tsx` you use `dayjs.utc(traceInfo.timestamp).toISOString()` but in `LLMAnalyticsClusterScene.tsx:240` for generation-level links, you use `dayjs(traceInfo.timestamp).subtract(24, 'hour').toISOString()` with a 24-hour buffer. the PR description mentions ""24-hour timestamp buffer for generation cluster links"" but this component doesn't apply it.

check whether the 24-hour buffer should also be applied here for generation-level links

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/llm_analytics/frontend/clusters/ClusterTraceList.tsx
Line: 93:93

Comment:
timestamp conversion inconsistency with `LLMAnalyticsClusterScene.tsx`

in `ClusterTraceList.tsx` you use `dayjs.utc(traceInfo.timestamp).toISOString()` but in `LLMAnalyticsClusterScene.tsx:240` for generation-level links, you use `dayjs(traceInfo.timestamp).subtract(24, 'hour').toISOString()` with a 24-hour buffer. the PR description mentions ""24-hour timestamp buffer for generation cluster links"" but this component doesn't apply it.

check whether the 24-hour buffer should also be applied here for generation-level links

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45917#discussion_r2727991243,,2026-01-26T15:10:48+00:00,logic,medium,"Greptile correctly identified an inconsistency - the PR description mentions a '24-hour timestamp buffer for generation cluster links' but this file doesn't apply that buffer when constructing generation-level URLs, while LLMAnalyticsClusterScene.tsx does use `subtract(24, 'hour')`. This could cause generation-level links from this component to fail to find the correct data, which is a real bug that's non-obvious because it requires cross-file analysis.",2026-01-26T16:23:59.198067+00:00
posthog/posthog,45921,fix(gateway): exact match instead of prefix match for allowed models,https://github.com/PostHog/posthog/pull/45921,,"Exact matching breaks versioned model IDs. Anthropic uses timestamped models like `claude-sonnet-4-5-20260101`, but the allowlist only contains `claude-sonnet-4-5`. This will reject all versioned models in production.

Check if the allowlist should include all versioned variants, or if prefix matching was intentional to allow future model versions.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: services/llm-gateway/src/llm_gateway/products/config.py
Line: 110:113

Comment:
Exact matching breaks versioned model IDs. Anthropic uses timestamped models like `claude-sonnet-4-5-20260101`, but the allowlist only contains `claude-sonnet-4-5`. This will reject all versioned models in production.

Check if the allowlist should include all versioned variants, or if prefix matching was intentional to allow future model versions.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45921#discussion_r2727932455,,2026-01-26T14:55:22+00:00,logic,high,"The PR title explicitly states the intent is to change from prefix to exact match, but Greptile correctly identifies a real production concern: if the allowlist contains base model names like 'claude-sonnet-4-5' but Anthropic's API uses timestamped versions like 'claude-sonnet-4-5-20260101', all versioned model requests would be incorrectly rejected. This is a non-obvious backward compatibility issue that could break production functionality.",2026-01-26T16:23:59.198159+00:00
twentyhq/twenty,17449,Backfill `owner` standard field check colliding `joinColumnName`,https://github.com/twentyhq/twenty/pull/17449,,"Looking at this logic, the issue is that you're not checking for collisions in the relation target when the source field already has a colliding `joinColumnName`.

The current code returns early in two cases:
1. When it's NOT a relation field (`!isMorphOrRelationField`) 
2. When it IS a relation field AND has a colliding join column name (`hasCollidingJoinColumnName`)

This means you only check the relation target when it's a relation field that does NOT have a colliding join column name. But the relation target could still have collisions even if the source field does.

For example:
- Source field has `joinColumnName: 'ownerId'` (collision detected, fixed)
- But the target relation might also have fields that collide with the new standard owner field
- Your current logic skips checking the target in this scenario

You should check the relation target regardless of whether the source field has collisions, since they're independent collision surfaces.",https://github.com/twentyhq/twenty/pull/17449#discussion_r2728005355,,2026-01-26T15:14:21+00:00,logic,medium,"Greptile correctly identified a logic flaw: the condition `if (!isMorphOrRelationField || hasCollidingJoinColumnName) { return; }` at line 279 causes an early return when the source field has a colliding joinColumnName, but this skips checking whether the relation target ALSO has a collision that needs fixing. Both sides of a relation could independently have 'ownerId' as joinColumnName and both should be checked/fixed.",2026-01-26T16:23:59.198221+00:00
nvidia/fuser,5857,MoE Dispatch/Combine first implementation for `k=1` and Nccl backend,https://github.com/NVIDIA/Fuser/pull/5857,,"Missing validation: `is_token_in_rank.size(0)` should equal `num_tokens` (x.size(0))

```suggestion
  NVF_CHECK_EQ(
      is_token_in_rank.dim(),
      2,
      ""is_token_in_rank must be [tokens, ranks], got: "",
      is_token_in_rank.sizes());
  NVF_CHECK_EQ(x.dim(), 2, ""Dispatch expects x to be 2D [tokens, hidden]."");

  const int64_t num_tokens = x.size(0);
  const int64_t hidden = x.size(1);
  const int64_t world_size = communicator->size();
  const int64_t my_rank = communicator->deviceId();
  NVF_CHECK_EQ(
      is_token_in_rank.size(0),
      num_tokens,
      ""is_token_in_rank first dim must match num_tokens."");
  NVF_CHECK_EQ(
      is_token_in_rank.size(1),
      world_size,
      ""is_token_in_rank second dim must match world size."");
```",https://github.com/NVIDIA/Fuser/pull/5857#discussion_r2728258043,,2026-01-26T16:14:24+00:00,logic,medium,"Greptile correctly identifies a missing validation. The code validates is_token_in_rank.size(1) == world_size but never checks that is_token_in_rank.size(0) == num_tokens. Since is_token_in_rank is used with operations like argmax(1) and indexing that depend on having the same number of rows as x has tokens, a dimension mismatch would cause incorrect behavior or runtime errors. This is a non-obvious validation gap that could easily be missed.",2026-01-26T16:23:59.198279+00:00
requestnetwork/requestnetwork,1691,feat: add Tron substreams indexing package,https://github.com/RequestNetwork/requestNetwork/pull/1691,,"Missing `bs58` dependency - code uses `bs58::encode()` in `lib.rs:181` but `bs58` is not in dependencies.

```suggestion
hex = ""0.4""
bs58 = ""0.5""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/substreams-tron/Cargo.toml
Line: 18:18

Comment:
Missing `bs58` dependency - code uses `bs58::encode()` in `lib.rs:181` but `bs58` is not in dependencies.

```suggestion
hex = ""0.4""
bs58 = ""0.5""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RequestNetwork/requestNetwork/pull/1691#discussion_r2727950347,,2026-01-26T14:59:57+00:00,runtime,critical,"This is a missing dependency that would cause a compilation failure. The code in lib.rs uses bs58::encode() but the Cargo.toml doesn't include bs58 as a dependency, which means the project won't compile. This is a real bug that's easy to miss when reviewing a Cargo.toml in isolation without checking the corresponding source files.",2026-01-26T16:23:59.198307+00:00
requestnetwork/requestnetwork,1691,feat: add Tron substreams indexing package,https://github.com/RequestNetwork/requestNetwork/pull/1691,,"Event topic is incomplete (40 chars instead of 64) AND never validated - code will parse ALL events from proxy contracts, not just `TransferWithReferenceAndFee`

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/substreams-tron/src/lib.rs
Line: 19:20

Comment:
Event topic is incomplete (40 chars instead of 64) AND never validated - code will parse ALL events from proxy contracts, not just `TransferWithReferenceAndFee`

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RequestNetwork/requestNetwork/pull/1691#discussion_r2727950417,,2026-01-26T14:59:58+00:00,logic,high,"Greptile correctly identifies two issues: (1) TRANSFER_WITH_REF_AND_FEE_TOPIC is 40 hex chars but should be 64 chars for a full keccak256 hash, and (2) the constant is defined but never used in the code - the event signature is never validated in parse_transfer_with_reference_and_fee, meaning ALL events from proxy contracts will be parsed, not just TransferWithReferenceAndFee events. This would cause incorrect payment data to be indexed.",2026-01-26T16:23:59.198331+00:00
posthog/posthog,45935,feat: add provider and model selection to evals,https://github.com/PostHog/posthog/pull/45935,,"`_validate_model_availability` defined but never called

```suggestion
    def clean(self) -> None:
        """"""Django validation - works for both persisted and in-memory instances.""""""
        super().clean()
        self._validate_provider_key_match()
        self._validate_model_availability()
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/llm_analytics/backend/models/model_configuration.py
Line: 53:65

Comment:
`_validate_model_availability` defined but never called

```suggestion
    def clean(self) -> None:
        """"""Django validation - works for both persisted and in-memory instances.""""""
        super().clean()
        self._validate_provider_key_match()
        self._validate_model_availability()
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45935#discussion_r2728610366,,2026-01-26T17:44:03+00:00,logic,medium,"Greptile correctly identified that `_validate_model_availability()` is defined (lines 53-63) but never called anywhere in the code. The `clean()` method only calls `_validate_provider_key_match()`, meaning model availability validation would never run, allowing users to save configurations with unavailable models. This is a real validation bug that bypasses intended business logic.",2026-01-26T18:24:22.586886+00:00
posthog/posthog,45935,feat: add provider and model selection to evals,https://github.com/PostHog/posthog/pull/45935,,"model select disabled when using PostHog default key, but help text says ""Add your own API key"" - users can't select PostHog-allowed models without adding a key

```suggestion
                                        disabled={availableModelsLoading}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/llm_analytics/frontend/evaluations/LLMAnalyticsEvaluation.tsx
Line: 264:265

Comment:
model select disabled when using PostHog default key, but help text says ""Add your own API key"" - users can't select PostHog-allowed models without adding a key

```suggestion
                                        disabled={availableModelsLoading}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45935#discussion_r2728610452,,2026-01-26T17:44:04+00:00,logic,medium,"Greptile correctly identified a UX bug: the model selector is disabled when `!selectedKeyId` (line 265), but the help text indicates users should be able to use PostHog's default key with available models. The code filters models by `posthog_available` suggesting some models should work without a custom key, but the disabled state prevents any selection. This creates a confusing/broken user experience where users can't select PostHog-allowed models.",2026-01-26T18:24:22.586972+00:00
posthog/posthog,45935,feat: add provider and model selection to evals,https://github.com/PostHog/posthog/pull/45935,,"auto-selects user's first key when provider changes, even if user wants PostHog default - forces BYOK when it may not be intended

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/llm_analytics/frontend/evaluations/llmEvaluationLogic.ts
Line: 307:314

Comment:
auto-selects user's first key when provider changes, even if user wants PostHog default - forces BYOK when it may not be intended

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45935#discussion_r2728610529,,2026-01-26T17:44:05+00:00,logic,medium,"Looking at lines 307-314 in setSelectedProvider listener, when provider changes it auto-selects the user's first key if they have one (`keysForProvider[0].id`), overriding any user preference for using PostHog's default key (null). This is a real UX/logic bug where users who have configured their own keys but want to use PostHog's default for a specific evaluation would be forced into BYOK mode against their intent.",2026-01-26T18:24:22.587017+00:00
posthog/posthog,45935,feat: add provider and model selection to evals,https://github.com/PostHog/posthog/pull/45935,,"only shows ""PostHog default"" option when no keys exist, but users with keys might still want to use PostHog default

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/llm_analytics/frontend/evaluations/LLMAnalyticsEvaluation.tsx
Line: 225:234

Comment:
only shows ""PostHog default"" option when no keys exist, but users with keys might still want to use PostHog default

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45935#discussion_r2728610572,,2026-01-26T17:44:06+00:00,logic,medium,"Greptile correctly identifies that the conditional logic `keysForSelectedProvider.length === 0 ? [{ value: 'posthog_default', label: 'PostHog default' }] : []` only shows the 'PostHog default' option when users have no keys configured. Users who have added their own API keys would lose access to the PostHog default option, which is likely unintended behavior since they should still be able to choose between their own keys and the default.",2026-01-26T18:24:22.587044+00:00
twentyhq/twenty,17456,Introduce webhook v2,https://github.com/twentyhq/twenty/pull/17456,,"migration could fail if workspaces have webhooks but no custom application

```suggestion
    await queryRunner.query(
      `UPDATE ""core"".""webhook"" w SET ""applicationId"" = (
        SELECT COALESCE(
          (SELECT a.""id"" FROM ""core"".""application"" a
           WHERE a.""workspaceId"" = w.""workspaceId""
           AND a.""isCustom"" = true
           LIMIT 1),
          (SELECT ws.""workspaceCustomApplicationId"" FROM ""core"".""workspace"" ws
           WHERE ws.""id"" = w.""workspaceId"")
        )
      )`,
    );
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/database/typeorm/core/migrations/common/1769200000000-addUniversalIdentifierAndApplicationIdToWebhook.ts
Line: 19:24

Comment:
migration could fail if workspaces have webhooks but no custom application

```suggestion
    await queryRunner.query(
      `UPDATE ""core"".""webhook"" w SET ""applicationId"" = (
        SELECT COALESCE(
          (SELECT a.""id"" FROM ""core"".""application"" a
           WHERE a.""workspaceId"" = w.""workspaceId""
           AND a.""isCustom"" = true
           LIMIT 1),
          (SELECT ws.""workspaceCustomApplicationId"" FROM ""core"".""workspace"" ws
           WHERE ws.""id"" = w.""workspaceId"")
        )
      )`,
    );
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17456#discussion_r2728642878,,2026-01-26T17:53:12+00:00,data_integrity,high,"The migration sets applicationId to NULL via subquery when no custom application exists for a workspace, then immediately tries to set that column as NOT NULL (line 28), which will cause the migration to fail for any webhook whose workspace lacks a custom application. This is a real bug that could break deployments in production.",2026-01-26T18:24:22.587070+00:00
greptileai/react-mirror,426,Add text node support to FragmentInstance operations,https://github.com/greptileai/react-mirror/pull/426,,"Consider whether horizontal scrolling should be `window.scrollX + rect.left` (always shifts horizontally) or just `window.scrollX` (preserves horizontal position). Verify this matches the intended scrolling behavior for text nodes

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react-dom-bindings/src/client/ReactFiberConfigDOM.js
Line: 3482:3482

Comment:
Consider whether horizontal scrolling should be `window.scrollX + rect.left` (always shifts horizontally) or just `window.scrollX` (preserves horizontal position). Verify this matches the intended scrolling behavior for text nodes

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/426#discussion_r2728531171,,2026-01-26T17:22:57+00:00,logic,medium,"Greptile correctly identifies that `window.scrollTo(window.scrollX + rect.left, scrollY)` always adds `rect.left` to the horizontal position, which would cause unintended horizontal scrolling. This differs from the `element.scrollIntoView()` behavior used for non-text nodes, which typically only scrolls enough to make the element visible. The inconsistency could cause confusing scroll behavior for text nodes.",2026-01-26T18:24:22.587136+00:00
openreplay/openreplay,4234,Spot webm,https://github.com/openreplay/openreplay/pull/4234,,"fetch error handling missing - if fetch throws (network error, CORS), `pollVideo` stops polling

```suggestion
        const pollVideo = () => {
          fetch(videoURL)
            .then((r) => {
              if (r.ok && r.status === 200) {
                initDash(videoURL);
              } else {
                checkTimeout = setTimeout(pollVideo, 1000);
              }
            })
            .catch(() => {
              checkTimeout = setTimeout(pollVideo, 1000);
            });
        };
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/app/components/Spots/SpotPlayer/components/SpotVideoContainer.tsx
Line: 95:101

Comment:
fetch error handling missing - if fetch throws (network error, CORS), `pollVideo` stops polling

```suggestion
        const pollVideo = () => {
          fetch(videoURL)
            .then((r) => {
              if (r.ok && r.status === 200) {
                initDash(videoURL);
              } else {
                checkTimeout = setTimeout(pollVideo, 1000);
              }
            })
            .catch(() => {
              checkTimeout = setTimeout(pollVideo, 1000);
            });
        };
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4234#discussion_r2728287209,,2026-01-26T16:22:13+00:00,runtime,medium,"Greptile correctly identified that the `pollVideo` function only has a `.then()` handler without a `.catch()`. If the fetch fails due to network errors or CORS issues, the promise rejection will silently stop the polling loop, leaving the video player in a broken state without any retry mechanism. This is a real bug that would cause silent failures in production and is non-obvious since the error path isn't immediately apparent in the code flow.",2026-01-26T18:24:22.587210+00:00
veda-labs/boring-vault,593,Archive legacy contracts,https://github.com/Veda-Labs/boring-vault/pull/593,,"<details>
<summary>Additional Comments (3)</summary>

**`src/archive/DelayedWithdraw.sol`**
moving this file breaks imports in production code - `src/helper/ArcticArchitectureLens.sol:10` still imports from `src/base/Roles/DelayedWithdraw.sol`

---

**`src/archive/atomic-queue/AtomicQueue.sol`**
moving this file breaks imports in production code - `src/base/DecodersAndSanitizers/Protocols/AtomicQueueDecoderAndSanitizer.sol:8` still imports from `src/atomic-queue/AtomicQueue.sol`

---

**`src/archive/atomic-queue/AtomicSolverV3.sol`**
moving these files breaks imports in 12+ test files that reference `src/atomic-queue/AtomicSolverV3.sol` and related atomic-queue contracts
</details>",https://github.com/Veda-Labs/boring-vault/pull/593#issuecomment-3800708872,,2026-01-26T17:12:05+00:00,runtime,high,"Greptile correctly identified that moving files to an archive directory breaks existing import paths in production code (ArcticArchitectureLens.sol, AtomicQueueDecoderAndSanitizer.sol) and test files. This would cause compilation failures and is a non-obvious issue when archiving files since one must check all dependent imports throughout the codebase.",2026-01-26T18:24:22.587253+00:00
raycast/extensions,24880,Add claude-code-config-switcher extension,https://github.com/raycast/extensions/pull/24880,,"SQL injection vulnerability - while single quotes are escaped, this escaping approach is fragile and error-prone. User input in `profileData.name`, `profileData.description`, and `settingsConfig` is inserted directly into SQL.

Use parameterized queries or a proper SQLite Node.js library (like `better-sqlite3` or `sqlite3`) that supports parameter binding to prevent SQL injection.",https://github.com/raycast/extensions/pull/24880#discussion_r2728986604,,2026-01-26T19:42:49+00:00,security,high,"The code escapes single quotes but executes raw SQL via shell command with user input. This is vulnerable to command injection (via shell metacharacters like backticks, $(), semicolons) even if SQL injection through quotes is partially mitigated. The settingsConfig is JSON.stringify'd user config which could contain malicious shell characters. This is a real security vulnerability that's non-obvious since the code appears to handle SQL escaping.",2026-01-26T20:19:54.981933+00:00
raycast/extensions,24880,Add claude-code-config-switcher extension,https://github.com/raycast/extensions/pull/24880,,"SQL injection vulnerability - `id` parameter is directly interpolated. An attacker could pass a malicious ID to execute arbitrary SQL.

Use parameterized queries.",https://github.com/raycast/extensions/pull/24880#discussion_r2728986707,,2026-01-26T19:42:52+00:00,security,high,"Greptile correctly identifies that the `id` parameter in `deleteProfile` is directly interpolated into the SQL query without sanitization, creating a SQL injection vulnerability. While the code does escape single quotes for name/notes/config fields in other functions, the `id` parameter in `deleteProfile` (and other functions like `setActiveProfileId`) has no such protection, allowing a malicious ID like `'; DROP TABLE providers; --` to execute arbitrary SQL commands.",2026-01-26T20:19:54.982003+00:00
browseros-ai/browseros,337,feat: AI Ghost Mode - Invisible agent that learns from user browsing patterns,https://github.com/browseros-ai/BrowserOS/pull/337,,"using a hash of the page title as a tab ID is unreliable - titles can change or be identical across tabs, causing incorrect grouping of actions

Use `web_contents()->GetPrimaryMainFrame()->GetFrameTreeNodeId()` or another stable identifier instead.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/browseros/chromium_patches/chrome/browser/browseros/ghost_mode/action_recorder.cc
Line: 133:136

Comment:
using a hash of the page title as a tab ID is unreliable - titles can change or be identical across tabs, causing incorrect grouping of actions

Use `web_contents()->GetPrimaryMainFrame()->GetFrameTreeNodeId()` or another stable identifier instead.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/browseros-ai/BrowserOS/pull/337#discussion_r2728846285,,2026-01-26T18:56:53+00:00,logic,medium,"Greptile correctly identified that using `std::hash<std::string>{}(web_contents()->GetTitle())` as a tab_id is fundamentally flawed - page titles change dynamically and can be identical across different tabs (e.g., multiple 'Google' tabs), causing actions to be incorrectly grouped together or regrouped when titles change. This is a non-obvious data integrity bug that would cause incorrect behavior in action replay/analysis features.",2026-01-26T20:19:54.982040+00:00
posthog/posthog,45946,chore: Order persons list by id when flag is active,https://github.com/PostHog/posthog/pull/45946,,"`ORDER BY id` defaults to ASC, which reverses the sort direction compared to the non-flag path (`ORDER BY created_at DESC`). Should this be `ORDER BY id DESC` to maintain newest-first ordering?

```suggestion
                    order_by = [ast.OrderExpr(expr=ast.Field(chain=[""id""]), order=""DESC"")]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/hogql_queries/actors_query_runner.py
Line: 369:369

Comment:
`ORDER BY id` defaults to ASC, which reverses the sort direction compared to the non-flag path (`ORDER BY created_at DESC`). Should this be `ORDER BY id DESC` to maintain newest-first ordering?

```suggestion
                    order_by = [ast.OrderExpr(expr=ast.Field(chain=[""id""]), order=""DESC"")]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45946#discussion_r2729123566,,2026-01-26T20:23:26+00:00,logic,medium,"Greptile correctly identified that the new code path uses `ORDER BY id` without specifying direction (defaulting to ASC), while the alternative path explicitly uses `ORDER BY created_at DESC`. This inconsistency would cause the UI to show results in opposite order depending on which code path is taken, which is likely unintended behavior. The suggestion to add `order=""DESC""` maintains consistency with the non-flag path's newest-first ordering.",2026-01-26T22:16:46.725045+00:00
onyx-dot-app/onyx,7810,feat(chat history): summarize older messages,https://github.com/onyx-dot-app/onyx/pull/7810,,"old summary messages should be deleted before creating new one to prevent accumulation of orphaned summaries that waste database space and inflate token calculations

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/chat/compression.py
Line: 305:318

Comment:
old summary messages should be deleted before creating new one to prevent accumulation of orphaned summaries that waste database space and inflate token calculations

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7810#discussion_r2729287775,,2026-01-26T21:17:51+00:00,data_integrity,medium,"The code creates new summary messages on each compression but never deletes the old ones. Looking at lines 292-315, when a new summary_message is created, the chat_session.summary_message_id is updated to point to the new summary, but the existing_summary message (retrieved earlier) is never deleted. This leaves orphaned ChatMessage records in the database that will accumulate over time, wasting storage and potentially inflating token calculations in calculate_total_history_tokens() which sums ALL messages in a session.",2026-01-26T22:16:46.725149+00:00
onyx-dot-app/onyx,7808,Update file system in the background after indexing is complete,https://github.com/onyx-dot-app/onyx/pull/7808,,"Missing error handling for `ApiException`. Other methods in this file wrap `k8s_stream` calls in try-except blocks to handle pod deletion and other API errors (see lines 1087, 1147, 1277, 1396, 1495, 1674, 1747).

```suggestion
        try:
            resp = k8s_stream(
                self._core_api.connect_get_namespaced_pod_exec,
                pod_name,
                self._namespace,
                container=""file-sync"",  # Execute in sidecar, not sandbox container
                command=sync_command,
                stderr=True,
                stdin=False,
                stdout=True,
                tty=False,
            )
            logger.debug(f""File sync response: {resp}"")
            return True
        except ApiException as e:
            logger.warning(f""Failed to sync files: {e}"")
            return False
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/sandbox/kubernetes/kubernetes_sandbox_manager.py
Line: 1540:1552

Comment:
Missing error handling for `ApiException`. Other methods in this file wrap `k8s_stream` calls in try-except blocks to handle pod deletion and other API errors (see lines 1087, 1147, 1277, 1396, 1495, 1674, 1747).

```suggestion
        try:
            resp = k8s_stream(
                self._core_api.connect_get_namespaced_pod_exec,
                pod_name,
                self._namespace,
                container=""file-sync"",  # Execute in sidecar, not sandbox container
                command=sync_command,
                stderr=True,
                stdin=False,
                stdout=True,
                tty=False,
            )
            logger.debug(f""File sync response: {resp}"")
            return True
        except ApiException as e:
            logger.warning(f""Failed to sync files: {e}"")
            return False
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7808#discussion_r2729104752,,2026-01-26T20:16:50+00:00,runtime,medium,"Greptile correctly identified that the new sync_files method lacks error handling for ApiException, while other similar k8s_stream calls throughout the file (at lines mentioned) consistently use try-except blocks. This is a real bug that could cause unhandled exceptions when the pod is deleted or unavailable, and the method even has a return type of bool suggesting it should handle failures gracefully by returning False rather than crashing.",2026-01-26T22:16:46.725200+00:00
greptileai/react-mirror,430,Update CSS shorthand property list,https://github.com/greptileai/react-mirror/pull/430,,"`vertical-align` is not a CSS shorthand property - it's a standalone longhand property. The properties `alignmentBaseline`, `baselineShift`, and `baselineSource` are SVG presentation attributes, not CSS longhands. This entry should be removed.

```suggestion
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react-dom-bindings/src/client/CSSShorthandProperty.js
Line: 259:259

Comment:
`vertical-align` is not a CSS shorthand property - it's a standalone longhand property. The properties `alignmentBaseline`, `baselineShift`, and `baselineSource` are SVG presentation attributes, not CSS longhands. This entry should be removed.

```suggestion
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/430#discussion_r2729289030,,2026-01-26T21:18:17+00:00,data_integrity,medium,"Greptile is correct - `vertical-align` is indeed a standalone CSS property, not a shorthand. The properties listed (`alignmentBaseline`, `baselineShift`, `baselineSource`) are SVG-specific attributes unrelated to CSS vertical-align. Including this incorrect mapping could cause React's style reconciliation to behave incorrectly when handling vertical-align styles. This is a non-obvious domain knowledge issue that requires understanding CSS specifications.",2026-01-26T22:16:46.725264+00:00
nvidia/numba-cuda,747,fix: extract handle value in Event.query() call to match other driver calls,https://github.com/NVIDIA/numba-cuda/pull/747,,"`event_time` is only set inside the while loop body, so if `evt.query()` returns `True` on the first iteration (event already complete), `event_time` will be undefined when used in the assertion on line 79.

```suggestion
        # Query immediately.
        event_time = perf_counter() - t0
        while not evt.query():
            event_time = perf_counter() - t0
```",https://github.com/NVIDIA/numba-cuda/pull/747#discussion_r2729288354,,2026-01-26T21:18:03+00:00,runtime,medium,"Greptile correctly identified that `event_time` is only assigned inside the while loop body. If `evt.query()` returns True immediately (event already complete), the loop body never executes, and `event_time` will be undefined when referenced in the assertion on line 79, causing a NameError. This is a real bug that could cause flaky test failures and is non-obvious since it depends on timing conditions.",2026-01-26T22:16:46.725301+00:00
posthog/posthog,45961,feat: Add new options page for `insights`,https://github.com/PostHog/posthog/pull/45961,,"`targetSelector` references an element on the options page (`[data-attr=""insight-option-trends""]`), but `getUrl` points to the direct insight creation page. The URL should point to the page containing the target selector for proper task guidance

```suggestion
                id: SetupTaskId.ExploreTrendsInsight,
                title: 'Create a trends insight',
                description: 'Visualize how events or actions vary over time.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-trends""]',
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/components/ProductSetup/productSetupRegistry.ts
Line: 56:62

Comment:
`targetSelector` references an element on the options page (`[data-attr=""insight-option-trends""]`), but `getUrl` points to the direct insight creation page. The URL should point to the page containing the target selector for proper task guidance

```suggestion
                id: SetupTaskId.ExploreTrendsInsight,
                title: 'Create a trends insight',
                description: 'Visualize how events or actions vary over time.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-trends""]',
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45961#discussion_r2729589647,,2026-01-26T22:55:39+00:00,logic,medium,"Greptile correctly identified a mismatch between getUrl() and targetSelector. The targetSelector '[data-attr=""insight-option-trends""]' references an element on an options/selection page, but getUrl() points to urls.insightNew() which is the direct creation page. This would cause the onboarding guidance system to fail to highlight the correct element since it would navigate to the wrong page. This pattern repeats for all the new explore tasks (Funnels, Retention, Paths, Stickiness, Lifecycle), making it a non-obvious but impactful bug in the product setup flow.",2026-01-27T00:53:28.622859+00:00
posthog/posthog,45961,feat: Add new options page for `insights`,https://github.com/PostHog/posthog/pull/45961,,"same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExploreFunnelInsight,
                title: 'Create a funnel insight',
                description: 'Track how users move through steps like signup → activation → purchase.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-funnels""]',
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/components/ProductSetup/productSetupRegistry.ts
Line: 64:70

Comment:
same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExploreFunnelInsight,
                title: 'Create a funnel insight',
                description: 'Track how users move through steps like signup → activation → purchase.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-funnels""]',
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45961#discussion_r2729589700,,2026-01-26T22:55:40+00:00,logic,medium,"Greptile correctly identifies that the `targetSelector` is `[data-attr=""insight-option-funnels""]` which would only exist on an insight options page, but `getUrl` points to `urls.insightNew({ type: InsightType.FUNNELS })` which goes directly to creating a new funnel insight. This mismatch means the target element won't be found on the page the URL navigates to, breaking the onboarding/setup flow. This applies to multiple insight types in this diff and is a non-obvious bug that could easily be missed during review.",2026-01-27T00:53:28.623008+00:00
posthog/posthog,45961,feat: Add new options page for `insights`,https://github.com/PostHog/posthog/pull/45961,,"same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExploreRetentionInsight,
                title: 'Explore retention analysis',
                description: 'See how many users return on subsequent days after an initial action.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-retention""]',
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/components/ProductSetup/productSetupRegistry.ts
Line: 72:78

Comment:
same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExploreRetentionInsight,
                title: 'Explore retention analysis',
                description: 'See how many users return on subsequent days after an initial action.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-retention""]',
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45961#discussion_r2729589729,,2026-01-26T22:55:41+00:00,logic,medium,"Greptile correctly identifies a mismatch between getUrl() pointing to urls.insightNew() and targetSelector looking for '[data-attr=""insight-option-retention""]'. The targetSelector expects UI elements from an options page, but getUrl() navigates directly to creating a new insight of that type, meaning the element to highlight won't exist on the navigated page. This would cause the onboarding UI targeting to fail silently.",2026-01-27T00:53:28.623072+00:00
posthog/posthog,45961,feat: Add new options page for `insights`,https://github.com/PostHog/posthog/pull/45961,,"same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExplorePathsInsight,
                title: 'Explore user paths',
                description: 'Trace the journeys users take within your product.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-paths""]',
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/components/ProductSetup/productSetupRegistry.ts
Line: 80:86

Comment:
same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExplorePathsInsight,
                title: 'Explore user paths',
                description: 'Trace the journeys users take within your product.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-paths""]',
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45961#discussion_r2729589772,,2026-01-26T22:55:42+00:00,logic,medium,"The code uses `urls.insightNew({ type: InsightType.PATHS })` for `getUrl`, but the `targetSelector` points to `[data-attr=""insight-option-paths""]`, which is likely an element on the insight options selection page, not the insight creation page. This mismatch would cause the guided setup to navigate to the wrong page where the target element doesn't exist, breaking the onboarding flow.",2026-01-27T00:53:28.623114+00:00
posthog/posthog,45961,feat: Add new options page for `insights`,https://github.com/PostHog/posthog/pull/45961,,"same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExploreStickinessInsight,
                title: 'Explore stickiness',
                description: 'See what keeps users coming back by viewing repeated actions.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-stickiness""]',
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/components/ProductSetup/productSetupRegistry.ts
Line: 88:94

Comment:
same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExploreStickinessInsight,
                title: 'Explore stickiness',
                description: 'See what keeps users coming back by viewing repeated actions.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-stickiness""]',
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45961#discussion_r2729589809,,2026-01-26T22:55:43+00:00,logic,medium,"Greptile correctly identified that the `getUrl` returns `urls.insightNew({ type: InsightType.STICKINESS })` while the `targetSelector` points to `[data-attr=""insight-option-stickiness""]` - an element that would exist on an options page, not the insight creation page. This mismatch means the setup task guidance would navigate to the wrong page where the target element doesn't exist, breaking the onboarding flow. This is a real bug that would cause incorrect UI behavior and is non-obvious because it requires understanding the relationship between URL destinations and DOM elements.",2026-01-27T00:53:28.623177+00:00
posthog/posthog,45961,feat: Add new options page for `insights`,https://github.com/PostHog/posthog/pull/45961,,"same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExploreLifecycleInsight,
                title: 'Explore lifecycle analysis',
                description: 'Break down users into new, returning, resurrected, and dormant.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-lifecycle""]',
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/lib/components/ProductSetup/productSetupRegistry.ts
Line: 96:102

Comment:
same issue - `getUrl` should point to `urls.insightOptions()` to match the `targetSelector`

```suggestion
                id: SetupTaskId.ExploreLifecycleInsight,
                title: 'Explore lifecycle analysis',
                description: 'Break down users into new, returning, resurrected, and dormant.',
                taskType: 'explore',
                dependsOn: [SetupTaskId.CreateFirstInsight],
                getUrl: () => urls.insightOptions(),
                targetSelector: '[data-attr=""insight-option-lifecycle""]',
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45961#discussion_r2729589845,,2026-01-26T22:55:44+00:00,logic,medium,"The targetSelector '[data-attr=""insight-option-lifecycle""]' suggests this element exists on an insights options page, but getUrl points to urls.insightNew() which would navigate directly to creating a new lifecycle insight. This mismatch means the onboarding system would navigate to the wrong page and fail to highlight the intended UI element. This is a non-obvious consistency bug across multiple similar entries that a typical reviewer might miss when scanning the diff.",2026-01-27T00:53:28.623221+00:00
onyx-dot-app/onyx,7819,feat(ee): add proxy endpoints for self-hosted billing operations,https://github.com/onyx-dot-app/onyx/pull/7819,,"test expects 503 but `get_optional_license_payload()` raises 501 (proxy.py:56)

```suggestion
            assert exc_info.value.status_code == 501
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/tests/unit/ee/onyx/server/tenants/test_proxy.py
Line: 266:266

Comment:
test expects 503 but `get_optional_license_payload()` raises 501 (proxy.py:56)

```suggestion
            assert exc_info.value.status_code == 501
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7819#discussion_r2729634556,,2026-01-26T23:15:52+00:00,logic,medium,"Looking at the test code at line 266, it asserts `exc_info.value.status_code == 501`, which matches what Greptile suggested. The diff shows the test already expects 501, so Greptile's suggestion is actually already implemented in the code. Wait - re-reading the diff, the test at line 266 does assert `status_code == 501`. The Greptile comment appears to be incorrect - the code already has 501. This is a false positive.",2026-01-27T00:53:28.623261+00:00
infisical/infisical,5275,fix(saml): support Azure SAML 'Sign assertion only' configuration,https://github.com/Infisical/infisical/pull/5275,,"JSON.parse without try-catch could throw on malformed RelayState input from the IdP.

```suggestion
              try {
                const relayState = JSON.parse(req.body.RelayState);
                if (relayState.spInitiated) {
                  samlConfig.audience = `spn:${ssoConfig.issuer}`;
                }
              } catch {
                // Invalid RelayState format, skip audience override
              }
```",https://github.com/Infisical/infisical/pull/5275#discussion_r2729561041,,2026-01-26T22:44:05+00:00,runtime,medium,"The code does call JSON.parse(req.body.RelayState) without any try-catch protection. Since RelayState comes from external IdP input, malformed JSON would cause an unhandled exception that could crash the request or authentication flow. This is a real bug that could affect production SAML authentication.",2026-01-27T00:53:28.623290+00:00
assistant-ui/assistant-ui,3135,fix: lint,https://github.com/assistant-ui/assistant-ui/pull/3135,,"`callbackRef` should not be in the dependency array. The whole purpose of `useEffectEvent` is to return a stable reference that doesn't change between renders.

The correct fix is to add `useEffectEvent` to `biome.json` line 77 with `""stableResult"": true`, similar to how `tapEffectEvent` is configured:

```json
{ ""name"": ""useEffectEvent"", ""stableResult"": true },
```

Adding `callbackRef` to the deps defeats the purpose of using `useEffectEvent` and can cause unnecessary re-subscriptions to the event listener.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/store/src/useAuiEvent.ts
Line: 21:21

Comment:
`callbackRef` should not be in the dependency array. The whole purpose of `useEffectEvent` is to return a stable reference that doesn't change between renders.

The correct fix is to add `useEffectEvent` to `biome.json` line 77 with `""stableResult"": true`, similar to how `tapEffectEvent` is configured:

```json
{ ""name"": ""useEffectEvent"", ""stableResult"": true },
```

Adding `callbackRef` to the deps defeats the purpose of using `useEffectEvent` and can cause unnecessary re-subscriptions to the event listener.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3135#discussion_r2729780145,,2026-01-27T00:27:30+00:00,performance,low,"Greptile correctly identifies that adding `callbackRef` to the dependency array defeats the purpose of `useEffectEvent`, which is specifically designed to return a stable reference. While the reference IS stable (so it won't cause re-renders in practice), adding it to deps is semantically incorrect and the proper fix is to configure biome to recognize `useEffectEvent` as returning a stable result. This is a non-obvious issue about React patterns that shows understanding of the library's internals.",2026-01-27T00:53:28.623314+00:00
alpaca-network/gatewayz-backend,965,fix: address code review comments - unused imports and send_email bug,https://github.com/Alpaca-Network/gatewayz-backend/pull/965,,"Critical bug: incorrect parameter name. The method signature is `send_email_notification(to_email, subject, html_content, text_content=None)` but this code uses `body=` instead of `html_content=`. This will cause a `TypeError` at runtime.

```suggestion
                    notification_service.send_email_notification(
                        to_email=admin_email,
                        subject=f""URGENT: {provider} credits critically low"",
                        html_content=f""""""
                        <h2>Provider Credit Alert</h2>
                        <p><strong>Provider:</strong> {provider}</p>
                        <p><strong>Balance:</strong> ${balance:.2f}</p>
                        <p><strong>Status:</strong> {status}</p>
                        <p><strong>Action Required:</strong> Add credits immediately to prevent service disruption</p>
                        <p><a href=""https://openrouter.ai/settings/credits"">Add Credits</a></p>
                        """""",
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/services/provider_credit_monitor.py
Line: 196:207

Comment:
Critical bug: incorrect parameter name. The method signature is `send_email_notification(to_email, subject, html_content, text_content=None)` but this code uses `body=` instead of `html_content=`. This will cause a `TypeError` at runtime.

```suggestion
                    notification_service.send_email_notification(
                        to_email=admin_email,
                        subject=f""URGENT: {provider} credits critically low"",
                        html_content=f""""""
                        <h2>Provider Credit Alert</h2>
                        <p><strong>Provider:</strong> {provider}</p>
                        <p><strong>Balance:</strong> ${balance:.2f}</p>
                        <p><strong>Status:</strong> {status}</p>
                        <p><strong>Action Required:</strong> Add credits immediately to prevent service disruption</p>
                        <p><a href=""https://openrouter.ai/settings/credits"">Add Credits</a></p>
                        """""",
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-backend/pull/965#discussion_r2729752498,,2026-01-27T00:11:47+00:00,runtime,high,"The PR title explicitly mentions 'fix: address code review comments - unused imports and send_email bug', which confirms this is a real bug being fixed. Using `body=` instead of `html_content=` as the parameter name would cause a TypeError at runtime when critical credit alerts are triggered, preventing important notifications from being sent. This is a non-obvious parameter name mismatch that a typical reviewer could easily miss.",2026-01-27T00:53:28.623360+00:00
gptme/gptme,1162,feat(cli): add agent profiles for pre-configured system prompts + tool access,https://github.com/gptme/gptme/pull/1162,,"behavior flags defined but never enforced - `read_only`, `no_network`, and `confirm_writes` are set in profiles but have no actual effect on tool behavior. the system only adds them to the system prompt, which relies on the LLM to comply rather than enforcing them programmatically

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/profiles.py
Line: 26:34

Comment:
behavior flags defined but never enforced - `read_only`, `no_network`, and `confirm_writes` are set in profiles but have no actual effect on tool behavior. the system only adds them to the system prompt, which relies on the LLM to comply rather than enforcing them programmatically

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1162#discussion_r2729776175,,2026-01-27T00:25:12+00:00,security,high,"Greptile correctly identifies that ProfileBehavior flags (read_only, no_network, confirm_writes) are defined as data fields but have no enforcement mechanism in the code - they only appear in system prompts hoping the LLM complies. This is a security-critical issue since profiles like 'explorer' and 'isolated' promise restrictions that aren't actually enforced programmatically, creating a false sense of security.",2026-01-27T00:53:28.623410+00:00
gptme/gptme,1162,feat(cli): add agent profiles for pre-configured system prompts + tool access,https://github.com/gptme/gptme/pull/1162,,"profile tools only apply if user doesn't specify `--tool`. if user runs `gptme --agent-profile isolated --tool shell,patch`, they get shell and patch tools instead of the restricted `read,ipython` defined by the isolated profile, defeating the security restrictions

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/cli.py
Line: 382:384

Comment:
profile tools only apply if user doesn't specify `--tool`. if user runs `gptme --agent-profile isolated --tool shell,patch`, they get shell and patch tools instead of the restricted `read,ipython` defined by the isolated profile, defeating the security restrictions

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1162#discussion_r2729776223,,2026-01-27T00:25:14+00:00,security,high,"The code at line 385-386 shows `if not tool_allowlist and selected_profile.tools is not None:` which confirms Greptile's analysis - profile tools are only applied when no explicit tools are specified. This is a real security issue because profiles like 'isolated' are meant to restrict tool access for security, but users can bypass these restrictions by specifying `--tool` flags, defeating the purpose of security-focused profiles.",2026-01-27T00:53:28.623437+00:00
gptme/gptme,1163,feat(cli): add shielded processing mode for untrusted content,https://github.com/gptme/gptme/pull/1163,,"The `ProfileBehavior` flags (`read_only`, `no_network`, `confirm_writes`) set here are not actually enforced at runtime. The system only enforces tool restrictions through the allowlist, but the behavior flags themselves have no runtime checks.

This creates a **critical security gap**: the `shell` tool is included in multiple shielded modes, but there's nothing preventing shell commands from writing files or accessing the network, bypassing the intended security model.

For example, in ""shielded-full"" mode (lines 415-416), `shell` is notably excluded, but in ""shielded-network"" mode (line 429), `shell` is included. An attacker could use shell commands like `curl` to exfiltrate data despite `no_network=True`.

Either:
1. Implement runtime enforcement of behavior flags in the tool execution layer, OR
2. Remove the behavior flags entirely and document that security relies solely on tool allowlisting and LLM instruction-following

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/cli.py
Line: 395:457

Comment:
The `ProfileBehavior` flags (`read_only`, `no_network`, `confirm_writes`) set here are not actually enforced at runtime. The system only enforces tool restrictions through the allowlist, but the behavior flags themselves have no runtime checks.

This creates a **critical security gap**: the `shell` tool is included in multiple shielded modes, but there's nothing preventing shell commands from writing files or accessing the network, bypassing the intended security model.

For example, in ""shielded-full"" mode (lines 415-416), `shell` is notably excluded, but in ""shielded-network"" mode (line 429), `shell` is included. An attacker could use shell commands like `curl` to exfiltrate data despite `no_network=True`.

Either:
1. Implement runtime enforcement of behavior flags in the tool execution layer, OR
2. Remove the behavior flags entirely and document that security relies solely on tool allowlisting and LLM instruction-following

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1163#discussion_r2729782004,,2026-01-27T00:28:37+00:00,security,critical,"Greptile correctly identifies that the ProfileBehavior flags (read_only, no_network) are set but there's no evidence in this diff of runtime enforcement. The 'shielded-network' mode includes 'shell' tool while setting no_network=True, but shell commands like curl/wget would bypass this since it's only a declarative flag. This is a real security gap where the feature promises protection it doesn't actually deliver.",2026-01-27T00:53:28.623464+00:00
gptme/gptme,1163,feat(cli): add shielded processing mode for untrusted content,https://github.com/gptme/gptme/pull/1163,,"Including `shell` tool in network-shielded mode allows bypass of `no_network` restriction. Shell commands can execute `curl`, `wget`, `nc`, etc. to access the network despite the security promise that ""you cannot access the internet"".

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/cli.py
Line: 429:430

Comment:
Including `shell` tool in network-shielded mode allows bypass of `no_network` restriction. Shell commands can execute `curl`, `wget`, `nc`, etc. to access the network despite the security promise that ""you cannot access the internet"".

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1163#discussion_r2729782028,,2026-01-27T00:28:38+00:00,security,high,"Greptile correctly identifies that the 'network' shielded mode at line 429-430 includes the 'shell' tool in its allowed tools list, which contradicts the security promise of 'no network access'. Users could bypass the restriction by running shell commands like curl, wget, or nc, completely undermining the security feature's purpose. This is a non-obvious security vulnerability in a feature specifically designed for handling untrusted content.",2026-01-27T00:53:28.623518+00:00
gptme/gptme,1163,feat(cli): add shielded processing mode for untrusted content,https://github.com/gptme/gptme/pull/1163,,"Including `shell` tool in write-shielded mode allows bypass of `read_only` restriction. Shell commands can write files using redirection (`echo ""data"" > file.txt`), `cp`, `mv`, `dd`, etc. despite the security promise that ""you CANNOT modify any files"".

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/cli.py
Line: 443:444

Comment:
Including `shell` tool in write-shielded mode allows bypass of `read_only` restriction. Shell commands can write files using redirection (`echo ""data"" > file.txt`), `cp`, `mv`, `dd`, etc. despite the security promise that ""you CANNOT modify any files"".

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1163#discussion_r2729782066,,2026-01-27T00:28:39+00:00,security,high,"Greptile correctly identified that the 'write' shielded mode includes 'shell' in its tools list (line 443-444) while claiming 'you CANNOT modify any files' in its system prompt. This is a real security bypass since shell commands can write files via redirection, cp, mv, etc., completely undermining the read_only restriction. This is a non-obvious security issue that could lead to data modification despite the security promise.",2026-01-27T00:53:28.623545+00:00
gptme/gptme,1163,feat(cli): add shielded processing mode for untrusted content,https://github.com/gptme/gptme/pull/1163,,"Including `shell` in the explorer profile allows write operations and network access despite `read_only=True` and `no_network=True` behavior flags (which aren't enforced). Shell commands can write files or access network.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/profiles.py
Line: 85:86

Comment:
Including `shell` in the explorer profile allows write operations and network access despite `read_only=True` and `no_network=True` behavior flags (which aren't enforced). Shell commands can write files or access network.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1163#discussion_r2729782110,,2026-01-27T00:28:41+00:00,security,high,"Greptile correctly identifies a security vulnerability: the 'explorer' profile includes 'shell' in its allowed tools while claiming to be read-only with no network access. The behavior flags (read_only=True, no_network=True) are only metadata that aren't enforced at the shell level - shell commands can write files (rm, mv, echo >) and access network (curl, wget). This completely undermines the security guarantees the profile claims to provide.",2026-01-27T00:53:28.623571+00:00
gptme/gptme,1154,feat(hooks): add async_mode support for non-blocking hook execution,https://github.com/gptme/gptme/pull/1154,,"The `_run_async_hook` method doesn't handle `SessionCompleteException` like the sync hook version does (lines 458-461). If an async hook raises this exception, it will be caught and logged as a generic error rather than being properly handled. Consider adding similar special-case handling:

```python
except Exception as e:
    # Special handling for session termination
    if e.__class__.__name__ == ""SessionCompleteException"":
        logger.info(f""Async hook '{hook.name}' signaled session completion"")
        # Note: Can't propagate in async mode, just log
        return
    logger.exception(f""Error in async hook '{hook.name}'"")
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/hooks/__init__.py
Line: 467:508

Comment:
The `_run_async_hook` method doesn't handle `SessionCompleteException` like the sync hook version does (lines 458-461). If an async hook raises this exception, it will be caught and logged as a generic error rather than being properly handled. Consider adding similar special-case handling:

```python
except Exception as e:
    # Special handling for session termination
    if e.__class__.__name__ == ""SessionCompleteException"":
        logger.info(f""Async hook '{hook.name}' signaled session completion"")
        # Note: Can't propagate in async mode, just log
        return
    logger.exception(f""Error in async hook '{hook.name}'"")
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1154#discussion_r2729500057,✅ Fixed in commit 6dbee5e - Added SessionCompleteException handling with logging (cannot propagate from async context),2026-01-26T22:27:03+00:00,logic,medium,"Greptile correctly identified that the `_run_async_hook` method was missing special handling for `SessionCompleteException` that exists in the sync hook version (lines 458-461). The developer confirmed this was a valid issue and fixed it in commit 6dbee5e. While the exception can't propagate from async context, proper logging rather than treating it as an error is important for debugging and understanding hook behavior.",2026-01-27T00:53:28.623596+00:00
runanywhereai/runanywhere-sdks,303,[draft - not to merge yet ] Shubham/core ml diffusion,https://github.com/RunanywhereAI/runanywhere-sdks/pull/303,,"duplicate generation call - this entire block builds options but then discards the result and re-generates below (lines 99-117)

```suggestion
        // Build C options
        var cOptions = rac_diffusion_options_t()
        var finalResult = rac_diffusion_result_t()

        let generateResult = prompt.withCString { promptPtr in
            opts.negativePrompt.withCString { negPromptPtr -> rac_result_t in
                cOptions.prompt = promptPtr
                cOptions.negative_prompt = negPromptPtr
                cOptions.width = Int32(opts.width)
                cOptions.height = Int32(opts.height)
                cOptions.steps = Int32(opts.steps)
                cOptions.guidance_scale = opts.guidanceScale
                cOptions.seed = opts.seed
                cOptions.scheduler = opts.scheduler.cValue
                cOptions.mode = opts.mode.cValue
                cOptions.denoise_strength = opts.denoiseStrength
                cOptions.report_intermediate_images = RAC_FALSE
                cOptions.progress_stride = 1

                return rac_diffusion_component_generate(handle, &cOptions, &finalResult)
            }
        }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: sdk/runanywhere-swift/Sources/RunAnywhere/Public/Extensions/Diffusion/RunAnywhere+Diffusion.swift
Line: 52:94

Comment:
duplicate generation call - this entire block builds options but then discards the result and re-generates below (lines 99-117)

```suggestion
        // Build C options
        var cOptions = rac_diffusion_options_t()
        var finalResult = rac_diffusion_result_t()

        let generateResult = prompt.withCString { promptPtr in
            opts.negativePrompt.withCString { negPromptPtr -> rac_result_t in
                cOptions.prompt = promptPtr
                cOptions.negative_prompt = negPromptPtr
                cOptions.width = Int32(opts.width)
                cOptions.height = Int32(opts.height)
                cOptions.steps = Int32(opts.steps)
                cOptions.guidance_scale = opts.guidanceScale
                cOptions.seed = opts.seed
                cOptions.scheduler = opts.scheduler.cValue
                cOptions.mode = opts.mode.cValue
                cOptions.denoise_strength = opts.denoiseStrength
                cOptions.report_intermediate_images = RAC_FALSE
                cOptions.progress_stride = 1

                return rac_diffusion_component_generate(handle, &cOptions, &finalResult)
            }
        }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RunanywhereAI/runanywhere-sdks/pull/303#discussion_r2729527168,,2026-01-26T22:34:57+00:00,performance,medium,"Greptile correctly identified that lines 52-94 call `rac_diffusion_component_generate` and then completely discard the result, only to rebuild identical options and call generate again at lines 99-117. This wastes computational resources by running expensive image generation twice, and the first generation's result (stored in local `diffusionResult` variables) is never used. The comment even acknowledges 'Re-generate to get the actual result (the above was just for building options)' which confirms this is unintentional redundant work.",2026-01-27T00:53:28.623638+00:00
runanywhereai/runanywhere-sdks,303,[draft - not to merge yet ] Shubham/core ml diffusion,https://github.com/RunanywhereAI/runanywhere-sdks/pull/303,,"streaming then calling `generateImage` again will generate the image twice - the stream already produces the final result

The loop consumes the stream which internally calls generation, then line 305 calls `generateImage` again with the same prompt/options. Remove the second call and get the result from the stream's final event.

```suggestion
            for try await progressUpdate in stream {
                self.progress = progressUpdate.progress
                self.currentStep = progressUpdate.currentStep
                self.totalSteps = progressUpdate.totalSteps
                self.statusMessage = ""Step \(progressUpdate.currentStep)/\(progressUpdate.totalSteps)""

                // Final image will be in the last progress update
                if let imageData = progressUpdate.intermediateImage {
                    if let uiImage = createImage(from: imageData, width: width, height: height) {
                        self.generatedImage = Image(uiImage: uiImage)
                    }
                }
            }

            // Image is already generated from the stream above
            // Seed will be in the final result if needed
            statusMessage = ""Generation complete""
            logger.info(""Image generated successfully via stream"")
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: examples/ios/RunAnywhereAI/RunAnywhereAI/Features/Diffusion/DiffusionViewModel.swift
Line: 285:305

Comment:
streaming then calling `generateImage` again will generate the image twice - the stream already produces the final result

The loop consumes the stream which internally calls generation, then line 305 calls `generateImage` again with the same prompt/options. Remove the second call and get the result from the stream's final event.

```suggestion
            for try await progressUpdate in stream {
                self.progress = progressUpdate.progress
                self.currentStep = progressUpdate.currentStep
                self.totalSteps = progressUpdate.totalSteps
                self.statusMessage = ""Step \(progressUpdate.currentStep)/\(progressUpdate.totalSteps)""

                // Final image will be in the last progress update
                if let imageData = progressUpdate.intermediateImage {
                    if let uiImage = createImage(from: imageData, width: width, height: height) {
                        self.generatedImage = Image(uiImage: uiImage)
                    }
                }
            }

            // Image is already generated from the stream above
            // Seed will be in the final result if needed
            statusMessage = ""Generation complete""
            logger.info(""Image generated successfully via stream"")
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RunanywhereAI/runanywhere-sdks/pull/303#discussion_r2729527227,,2026-01-26T22:34:58+00:00,logic,medium,"Greptile correctly identified that the code first consumes a stream via `generateImageStream()` (lines 285-299) and then calls `generateImage()` again (line 305) with the same prompt/options, which would trigger duplicate image generation - wasting compute resources and time. This is a real bug that causes unnecessary work and could confuse users with different results if seeds differ.",2026-01-27T00:53:28.623703+00:00
runanywhereai/runanywhere-sdks,303,[draft - not to merge yet ] Shubham/core ml diffusion,https://github.com/RunanywhereAI/runanywhere-sdks/pull/303,,"checking if the optional `cgImage` is nil after checking for ""no image"" creates unreachable code

If `images.first` is nil, line 227 already throws. Line 232's safety check `cgImage == nil` will never be true here since we'd have thrown already.

```suggestion
            // Get the first image
            guard let cgImage = images.first else {
                throw SDKError.diffusion(.generationFailed, ""No image generated"")
            }

            // Note: cgImage can be nil if safety checker filtered the image
            let safetyTriggered = (cgImage == nil)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: sdk/runanywhere-swift/Sources/RunAnywhere/Features/Diffusion/DiffusionPlatformService.swift
Line: 226:232

Comment:
checking if the optional `cgImage` is nil after checking for ""no image"" creates unreachable code

If `images.first` is nil, line 227 already throws. Line 232's safety check `cgImage == nil` will never be true here since we'd have thrown already.

```suggestion
            // Get the first image
            guard let cgImage = images.first else {
                throw SDKError.diffusion(.generationFailed, ""No image generated"")
            }

            // Note: cgImage can be nil if safety checker filtered the image
            let safetyTriggered = (cgImage == nil)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RunanywhereAI/runanywhere-sdks/pull/303#discussion_r2729527436,,2026-01-26T22:35:02+00:00,logic,medium,"Greptile correctly identifies that `images` is of type `[CGImage?]` (array of optional CGImages), so `images.first` returns `CGImage??`. The `guard let cgImage = images.first` unwraps one layer, making `cgImage` of type `CGImage?`. The guard only fails if the array is empty, not if the first element is nil. Therefore `cgImage == nil` on line 232 IS reachable and correctly checks for safety-filtered images. However, this reveals a subtle bug: if safety is triggered, `cgImage` would be nil, the code would set `safetyTriggered = true`, but then `image = cgImage` would be nil and `imageData` stays nil - which seems intentional. The comment's suggested fix doesn't actually change behavior. This is a non-obvious type system nuance worth catching.",2026-01-27T00:53:28.623761+00:00
gptme/gptme-contrib,208,feat(gptodo): add auto-unblocking when dependent tasks complete,https://github.com/gptme/gptme-contrib/pull/208,,"Comment says ""Only clear if this was the only thing being waited on"" but code unconditionally clears both fields. If `waiting_for` can contain multiple task IDs, this logic needs adjustment.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/unblock.py
Line: 105:107

Comment:
Comment says ""Only clear if this was the only thing being waited on"" but code unconditionally clears both fields. If `waiting_for` can contain multiple task IDs, this logic needs adjustment.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/208#discussion_r2729767194,,2026-01-27T00:20:12+00:00,logic,medium,"Greptile correctly identified a bug where the code comment on line 103-104 states 'Only clear if this was the only thing being waited on' but the implementation on lines 105-107 unconditionally removes both 'waiting_for' and 'waiting_since' fields regardless of whether multiple task IDs are present. The condition `completed_id in waiting_for` checks if the string contains the ID, but then removes the entire field rather than just that specific ID, which contradicts the stated intent.",2026-01-27T00:53:28.623803+00:00
onyx-dot-app/onyx,7504,feat(ee): enforce seat limits on user operations,https://github.com/onyx-dot-app/onyx/pull/7504,,"Seat check happens before `add_users_to_tenant()` in multi-tenant mode, but `add_users_to_tenant` may create inactive mappings for some users. The seat calculation counts only users with active mappings AND active user status. If a user already has an active mapping elsewhere, the new mapping is created as inactive (line 111 in user_mapping.py), so they won't immediately consume a seat. However, the check counts them as needing a seat upfront, potentially blocking valid invites.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/manage/users.py
Line: 400:406

Comment:
Seat check happens before `add_users_to_tenant()` in multi-tenant mode, but `add_users_to_tenant` may create inactive mappings for some users. The seat calculation counts only users with active mappings AND active user status. If a user already has an active mapping elsewhere, the new mapping is created as inactive (line 111 in user_mapping.py), so they won't immediately consume a seat. However, the check counts them as needing a seat upfront, potentially blocking valid invites.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7504#discussion_r2729836033,,2026-01-27T01:00:13+00:00,logic,medium,"Greptile correctly identifies a race condition/logic issue where the seat availability check counts all 'truly_new_emails' as needing seats, but in multi-tenant mode, some users may already have active mappings on other tenants and would get inactive mappings (not consuming seats). This could incorrectly block valid bulk invites. The analysis requires understanding cross-file behavior (user_mapping.py) and the nuanced seat counting logic, making it non-obvious.",2026-01-27T03:00:29.885295+00:00
nvidia/nvflare,4039,Fix the rest of the examples,https://github.com/NVIDIA/NVFlare/pull/4039,,"On Windows, executables require `.exe` extension. Use `shutil.which()` to locate executables cross-platform:

```suggestion
        # Get the full path to flower-supernode from the current Python environment
        import shutil
        flower_supernode_path = shutil.which(""flower-supernode"")
        if not flower_supernode_path:
            python_bin_dir = os.path.dirname(sys.executable)
            flower_supernode_path = os.path.join(python_bin_dir, ""flower-supernode"")
```",https://github.com/NVIDIA/NVFlare/pull/4039#discussion_r2729835589,,2026-01-27T00:59:57+00:00,runtime,medium,"Greptile correctly identifies that on Windows, executables have `.exe` extensions (e.g., `flower-supernode.exe`), so the current code constructing paths like `os.path.join(python_bin_dir, FLOWER_SUPERNODE)` will fail on Windows. Using `shutil.which()` is a standard cross-platform solution for locating executables. This is a non-obvious platform compatibility bug that would cause runtime failures on Windows systems.",2026-01-27T03:00:29.885386+00:00
nvidia/nvflare,4039,Fix the rest of the examples,https://github.com/NVIDIA/NVFlare/pull/4039,,"On Windows, executables require `.exe` extension. Use `shutil.which()` for cross-platform compatibility:

```suggestion
        # Get the full path to flower-superlink from the current Python environment
        import shutil
        flower_superlink_path = shutil.which(""flower-superlink"")
        if not flower_superlink_path:
            python_bin_dir = os.path.dirname(sys.executable)
            flower_superlink_path = os.path.join(python_bin_dir, ""flower-superlink"")
```",https://github.com/NVIDIA/NVFlare/pull/4039#discussion_r2729835629,,2026-01-27T00:59:58+00:00,runtime,medium,"Greptile correctly identifies that the code constructs executable paths by joining the Python bin directory with the executable name, but on Windows executables have `.exe` extensions. The current code would fail to find `flower-superlink` when it should look for `flower-superlink.exe`, causing runtime errors on Windows. Using `shutil.which()` is the standard cross-platform solution for finding executables.",2026-01-27T03:00:29.885452+00:00
nvidia/nvflare,4039,Fix the rest of the examples,https://github.com/NVIDIA/NVFlare/pull/4039,,"On Windows, executables require `.exe` extension. Use `shutil.which()` for cross-platform compatibility:

```suggestion
        # Get the full path to flwr from the current Python environment
        import shutil
        flwr_path = shutil.which(""flwr"")
        if not flwr_path:
            python_bin_dir = os.path.dirname(sys.executable)
            flwr_path = os.path.join(python_bin_dir, ""flwr"")
```",https://github.com/NVIDIA/NVFlare/pull/4039#discussion_r2729835671,,2026-01-27T00:59:59+00:00,runtime,medium,"Greptile correctly identifies a cross-platform compatibility bug. The code constructs executable paths by joining directory and name (e.g., `os.path.join(python_bin_dir, FLOWER_CLI)`), but on Windows, executables have `.exe` extensions. This would cause FileNotFoundError on Windows systems. Using `shutil.which()` is a standard cross-platform solution for finding executables.",2026-01-27T03:00:29.885497+00:00
greptileai/react-mirror,432,fix(eslint-plugin-react-hooks): allow underscore prefix in component …,https://github.com/greptileai/react-mirror/pull/432,,"<details>
<summary>Additional Comments (1)</summary>

**`packages/eslint-plugin-react-hooks/src/rules/RulesOfHooks.ts`**
inconsistency: `COMPONENT_NAME_PATTERN` in `RunReactCompiler.ts:26` still uses `/^[A-Z]/` and wasn't updated to match this change

```
// packages/eslint-plugin-react-hooks/src/shared/RunReactCompiler.ts:26
const COMPONENT_NAME_PATTERN = /^[A-Z]/;
```

this means underscore-prefixed components like `_PrivateComponent` will pass the hooks rule but might not be recognized by the compiler's React code detection heuristic

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/eslint-plugin-react-hooks/src/rules/RulesOfHooks.ts
Line: 26:26

Comment:
inconsistency: `COMPONENT_NAME_PATTERN` in `RunReactCompiler.ts:26` still uses `/^[A-Z]/` and wasn't updated to match this change

```
// packages/eslint-plugin-react-hooks/src/shared/RunReactCompiler.ts:26
const COMPONENT_NAME_PATTERN = /^[A-Z]/;
```

this means underscore-prefixed components like `_PrivateComponent` will pass the hooks rule but might not be recognized by the compiler's React code detection heuristic

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/greptileai/react-mirror/pull/432#issuecomment-3802817811,,2026-01-27T03:06:01+00:00,logic,medium,"Greptile correctly identified an inconsistency where the PR updates the component name pattern in RulesOfHooks.ts to allow underscore prefixes, but the same pattern in RunReactCompiler.ts wasn't updated. This creates a real behavioral inconsistency where underscore-prefixed components like `_PrivateComponent` will pass hooks linting but won't be recognized by the React compiler, which could lead to unexpected compilation behavior.",2026-01-27T04:32:46.744040+00:00
runanywhereai/runanywhere-sdks,304,[Swift-sdk] VLM Support + Diffusion support,https://github.com/RunanywhereAI/runanywhere-sdks/pull/304,,"Auto-streaming implementation creates a new `Task` but doesn't store a reference to cancel it in `deinit`. While `stopAutoStreaming()` is called in `onDisappear`, if the view is deallocated without `onDisappear` being called, the task could continue running.

Consider storing the task reference and canceling in `deinit`:
```swift
deinit {
    autoStreamTask?.cancel()
    NotificationCenter.default.removeObserver(self)
}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: examples/ios/RunAnywhereAI/RunAnywhereAI/Features/Vision/VLMViewModel.swift
Line: 196:214

Comment:
Auto-streaming implementation creates a new `Task` but doesn't store a reference to cancel it in `deinit`. While `stopAutoStreaming()` is called in `onDisappear`, if the view is deallocated without `onDisappear` being called, the task could continue running.

Consider storing the task reference and canceling in `deinit`:
```swift
deinit {
    autoStreamTask?.cancel()
    NotificationCenter.default.removeObserver(self)
}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RunanywhereAI/runanywhere-sdks/pull/304#discussion_r2730122394,,2026-01-27T03:48:56+00:00,resource_leak,medium,"Greptile correctly identifies that `autoStreamTask` is stored but not cancelled in `deinit`. The code at line 196 creates and stores the task in `autoStreamTask`, but the `deinit` at lines 51-54 only removes the notification observer without cancelling the task. If the view is deallocated without `onDisappear` being called (e.g., during navigation edge cases), the async task could continue running, causing a resource leak and potentially accessing deallocated memory.",2026-01-27T04:32:46.744144+00:00
onyx-dot-app/onyx,7853,feat(license): improve license enforcement middleware,https://github.com/onyx-dot-app/onyx/pull/7853,,"only checks `GATED_ACCESS` but should check all gated statuses (`GRACE_PERIOD`, `PAYMENT_REMINDER`) defined in `_GATED_STATUSES`

```suggestion
        if metadata and metadata.status not in _GATED_STATUSES:
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/ee/onyx/server/settings/api.py
Line: 49:49

Comment:
only checks `GATED_ACCESS` but should check all gated statuses (`GRACE_PERIOD`, `PAYMENT_REMINDER`) defined in `_GATED_STATUSES`

```suggestion
        if metadata and metadata.status not in _GATED_STATUSES:
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7853#discussion_r2730313972,,2026-01-27T05:29:10+00:00,logic,medium,"Greptile correctly identified an inconsistency: `apply_license_status_to_settings` uses `_GATED_STATUSES` (which likely includes multiple statuses like GATED_ACCESS, GRACE_PERIOD, PAYMENT_REMINDER), but `check_ee_features_enabled` only checks for `GATED_ACCESS` specifically. This means users with GRACE_PERIOD or PAYMENT_REMINDER status would still have EE features enabled when they potentially shouldn't. The suggested fix to use `not in _GATED_STATUSES` properly aligns the logic between both functions.",2026-01-27T06:22:10.918265+00:00
onyx-dot-app/onyx,7669,feat(filesys): hierarchynodes carry permission info,https://github.com/onyx-dot-app/onyx/pull/7669,,"`permission_sync_context` always created in slim retrieval mode, causing unnecessary permission fetches

In slim retrieval, permissions aren't typically needed, but this code always creates a `PermissionSyncContext`. This causes `_get_new_ancestors_for_files` to fetch folder permissions even when not required.

```suggestion
            permission_sync_context = (
                PermissionSyncContext(
                    primary_admin_email=self.primary_admin_email,
                    google_domain=self.google_domain,
                )
                if include_permissions
                else None
            )
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/connectors/google_drive/connector.py
Line: 1499:1510

Comment:
`permission_sync_context` always created in slim retrieval mode, causing unnecessary permission fetches

In slim retrieval, permissions aren't typically needed, but this code always creates a `PermissionSyncContext`. This causes `_get_new_ancestors_for_files` to fetch folder permissions even when not required.

```suggestion
            permission_sync_context = (
                PermissionSyncContext(
                    primary_admin_email=self.primary_admin_email,
                    google_domain=self.google_domain,
                )
                if include_permissions
                else None
            )
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7669#discussion_r2730611412,,2026-01-27T07:23:22+00:00,performance,medium,"Greptile correctly identifies that in the slim retrieval function `_yield_slim_batch`, the `permission_sync_context` is always created unconditionally, which causes `_get_new_ancestors_for_files` to fetch folder permissions via `get_external_access_for_folder` (since the context is non-null). This results in unnecessary API calls when permissions aren't needed. The fix suggestion to conditionally create the context based on `include_permissions` matches the pattern used elsewhere in the file (lines 1315-1323).",2026-01-27T08:21:26.872343+00:00
onyx-dot-app/onyx,7669,feat(filesys): hierarchynodes carry permission info,https://github.com/onyx-dot-app/onyx/pull/7669,,"`is_public` can be overwritten by subsequent permissions

If a folder has multiple permissions (e.g., DOMAIN with `allowFileDiscovery=True` followed by ANYONE with `allowFileDiscovery=False`), the last permission processed determines `is_public`. Should use `|=` to make it sticky once true.

```suggestion
        elif permission.type == PermissionType.DOMAIN:
            # Domain permission - check if it matches company domain
            if permission.domain == google_domain:
                # Only public if discoverable (allowFileDiscovery is not False)
                # If allowFileDiscovery is False, it's ""link only"" access
                is_public = is_public or (permission.allow_file_discovery is not False)
            else:
                logger.debug(
                    f""Domain permission for {permission.domain} does not match ""
                    f""company domain {google_domain} for folder {folder_id}""
                )
        elif permission.type == PermissionType.ANYONE:
            # Only public if discoverable (allowFileDiscovery is not False)
            # If allowFileDiscovery is False, it's ""link only"" access
            is_public = is_public or (permission.allow_file_discovery is not False)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/ee/onyx/external_permissions/google_drive/doc_sync.py
Line: 235:249

Comment:
`is_public` can be overwritten by subsequent permissions

If a folder has multiple permissions (e.g., DOMAIN with `allowFileDiscovery=True` followed by ANYONE with `allowFileDiscovery=False`), the last permission processed determines `is_public`. Should use `|=` to make it sticky once true.

```suggestion
        elif permission.type == PermissionType.DOMAIN:
            # Domain permission - check if it matches company domain
            if permission.domain == google_domain:
                # Only public if discoverable (allowFileDiscovery is not False)
                # If allowFileDiscovery is False, it's ""link only"" access
                is_public = is_public or (permission.allow_file_discovery is not False)
            else:
                logger.debug(
                    f""Domain permission for {permission.domain} does not match ""
                    f""company domain {google_domain} for folder {folder_id}""
                )
        elif permission.type == PermissionType.ANYONE:
            # Only public if discoverable (allowFileDiscovery is not False)
            # If allowFileDiscovery is False, it's ""link only"" access
            is_public = is_public or (permission.allow_file_discovery is not False)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7669#discussion_r2730611473,,2026-01-27T07:23:23+00:00,logic,medium,"Greptile correctly identified that the `is_public` variable can be overwritten by subsequent permissions in the loop. Looking at lines 235-249, if a DOMAIN permission with `allow_file_discovery=True` sets `is_public=True`, a later ANYONE permission with `allow_file_discovery=False` would overwrite it to `False`, incorrectly downgrading the folder's public access status. This is a subtle bug that could cause permission sync issues in Google Drive integration.",2026-01-27T08:21:26.872454+00:00
assistant-ui/assistant-ui,3151,refactor: move artifacts example to docs site,https://github.com/assistant-ui/assistant-ui/pull/3151,,"Add `sandbox` attribute to prevent XSS attacks. User-generated HTML should be sandboxed to restrict script execution and access to parent page.

```suggestion
              <iframe
                title=""artifact-preview""
                className=""h-full w-full""
                srcDoc={artifact}
                sandbox=""allow-scripts""
              />
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/docs/components/examples/artifacts-view.tsx
Line: 50:54

Comment:
Add `sandbox` attribute to prevent XSS attacks. User-generated HTML should be sandboxed to restrict script execution and access to parent page.

```suggestion
              <iframe
                title=""artifact-preview""
                className=""h-full w-full""
                srcDoc={artifact}
                sandbox=""allow-scripts""
              />
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3151#discussion_r2730611518,,2026-01-27T07:23:24+00:00,security,high,"The code renders user-generated HTML (from an AI tool call) directly into an iframe using srcDoc without any sandboxing. Without the sandbox attribute, malicious scripts in the artifact could access the parent page, steal cookies, or perform actions on behalf of the user. This is a legitimate XSS vulnerability that's easy to miss during code review.",2026-01-27T08:21:26.872566+00:00
assistant-ui/assistant-ui,3152,fix: avoid polluting :root CSS class,https://github.com/assistant-ui/assistant-ui/pull/3152,,"`:root` cannot be a descendant of any element - this selector will never match. The space creates a descendant combinator, but `:root` is always the document root (`<html>`). All CSS variables defined in this block (lines 63-107) won't be applied, breaking the entire theme system.

Replace with `&` to refer to the parent selector:

```suggestion
  & {
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/styles/src/styles/index.css
Line: 63:63

Comment:
`:root` cannot be a descendant of any element - this selector will never match. The space creates a descendant combinator, but `:root` is always the document root (`<html>`). All CSS variables defined in this block (lines 63-107) won't be applied, breaking the entire theme system.

Replace with `&` to refer to the parent selector:

```suggestion
  & {
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3152#discussion_r2730636571,,2026-01-27T07:32:00+00:00,logic,critical,"Greptile correctly identified that `:where(.aui-thread-root, .aui-modal-content) :root` creates a descendant selector where :root must be inside those elements, which is impossible since :root is always the document root (<html>). This would cause all theme variables (lines 63-107) to never apply, completely breaking the component's styling system. This is a non-obvious CSS specificity/selector bug that a typical reviewer could easily miss.",2026-01-27T08:21:26.872598+00:00
assistant-ui/assistant-ui,3152,fix: avoid polluting :root CSS class,https://github.com/assistant-ui/assistant-ui/pull/3152,,"`.dark` selector needs scoping to avoid global pollution. Should be `& .dark` or wrapped in the parent selector to match the PR's intent.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/styles/src/styles/index.css
Line: 109:109

Comment:
`.dark` selector needs scoping to avoid global pollution. Should be `& .dark` or wrapped in the parent selector to match the PR's intent.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3152#discussion_r2730636613,,2026-01-27T07:32:01+00:00,logic,medium,"Greptile correctly identifies that the `.dark` selector on line 109 is not properly scoped within the `:where(.aui-thread-root, .aui-modal-content)` parent selector, which defeats the PR's explicit purpose of avoiding CSS pollution to :root. While the `.dark` block is nested inside the parent selector block in the code structure, CSS doesn't work that way - `.dark` without `&` prefix would still apply globally, contradicting the PR title 'fix: avoid polluting :root CSS class'.",2026-01-27T08:21:26.872663+00:00
assistant-ui/assistant-ui,3152,fix: avoid polluting :root CSS class,https://github.com/assistant-ui/assistant-ui/pull/3152,,"This creates a redundant nested selector `:where(.aui-thread-root, .aui-modal-content) :where(.aui-thread-root, .aui-modal-content)`. Should be `&` to refer to the parent.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/styles/src/styles/index.css
Line: 154:154

Comment:
This creates a redundant nested selector `:where(.aui-thread-root, .aui-modal-content) :where(.aui-thread-root, .aui-modal-content)`. Should be `&` to refer to the parent.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3152#discussion_r2730636679,,2026-01-27T07:32:02+00:00,logic,medium,"Greptile correctly identified that line 54 creates a redundant nested selector `:where(.aui-thread-root, .aui-modal-content) :where(.aui-thread-root, .aui-modal-content) :root` because the CSS is already inside the `:where(.aui-thread-root, .aui-modal-content)` block (line 5), and then repeats the same selector. This would result in CSS variables not being applied correctly since the selector would never match (requiring nested .aui-thread-root elements). This is a non-obvious CSS scoping bug that could break the entire theme system.",2026-01-27T08:21:26.872689+00:00
democratizedspace/dspace,3159,Stage 1 — build-time docs RAG pack: chunk + MiniSearch index,https://github.com/democratizedspace/dspace/pull/3159,,"if `text.length > maxChars` here, it gets assigned to `current` without splitting, and will remain oversized through subsequent iterations until `pushCurrent()` at line 147 adds it as an oversized chunk

```suggestion
        } else {
            pushCurrent();
            if (text.length > maxChars) {
                let remaining = text;
                while (remaining.length > maxChars) {
                    chunks.push(remaining.slice(0, maxChars));
                    remaining = remaining.slice(maxChars);
                }
                current = remaining;
            } else {
                current = text;
            }
        }
```",https://github.com/democratizedspace/dspace/pull/3159#discussion_r2730750718,,2026-01-27T08:08:24+00:00,logic,medium,"Greptile correctly identified a logic bug: when `current.length + 1 + text.length > maxChars` but `text.length > maxChars`, the code in the else branch (lines 143-145) sets `current = text` without splitting it first. This oversized chunk will then be pushed as-is when `pushCurrent()` is called, defeating the purpose of the MAX_CHUNK_CHARS limit. The fix to apply the same splitting logic used earlier in the function is correct and actionable.",2026-01-27T08:21:26.872715+00:00
democratizedspace/dspace,3157,Build-time docs RAG index: MiniSearch chunking + generated artifacts,https://github.com/democratizedspace/dspace/pull/3157,,"The pattern `'\n---'` will match any line starting with `---`, not just the frontmatter closing delimiter. If a markdown file contains a horizontal rule (`---`) within the content after the frontmatter, it would be incorrectly treated as the end of the frontmatter block.

```suggestion
    const endIndex = content.indexOf('\n---\n', 3);
```

This ensures we only match `---` surrounded by newlines, making it more robust.",https://github.com/democratizedspace/dspace/pull/3157#discussion_r2730728811,,2026-01-27T08:00:58+00:00,logic,medium,"Greptile correctly identifies a real parsing bug: the code uses `content.indexOf('\n---')` which would match horizontal rules (---) anywhere in the document after a newline, not just the frontmatter closing delimiter. This could cause content after a horizontal rule to be incorrectly parsed as frontmatter. The suggested fix to use `'\n---\n'` ensures the delimiter is on its own line, making parsing more robust.",2026-01-27T08:21:26.872747+00:00
posthog/posthog,45983,fix(cdp): bytecode subtraction bug,https://github.com/PostHog/posthog/pull/45983,,"This logic creates false positives for legitimate subtraction operations. For example, `event.properties.count - total` would incorrectly trigger this error (left is a Field with chain length 3, right is Field with chain length 1). The heuristic should be more conservative - perhaps check if the right operand is a bare identifier (no dots) AND the operation would create a syntactically valid property name when concatenated with a hyphen.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/cdp/validation.py
Line: 43:49

Comment:
This logic creates false positives for legitimate subtraction operations. For example, `event.properties.count - total` would incorrectly trigger this error (left is a Field with chain length 3, right is Field with chain length 1). The heuristic should be more conservative - perhaps check if the right operand is a bare identifier (no dots) AND the operation would create a syntactically valid property name when concatenated with a hyphen.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45983#discussion_r2730967203,,2026-01-27T09:01:23+00:00,logic,medium,"Greptile correctly identifies that the heuristic is too broad - it triggers for ANY subtraction where left is a Field with chain >= 2 and right is a Field with chain == 1. Legitimate code like `event.properties.count - total` would falsely trigger this error because it meets all the conditions (left chain length 3, right chain length 1), even though it's valid subtraction, not a hyphenated property name. This is a non-obvious false positive issue in error detection logic.",2026-01-27T10:21:37.363152+00:00
gptme/gptme,1166,feat(cli): add gptme-doctor command for system diagnostics,https://github.com/gptme/gptme/pull/1166,,"Logic issue in lines 121-128: when a provider is in `available_provider_names` but `api_key` is None (line 96 returns None), the code marks it as OK with message ""Configured (via config or env)"". This is misleading - if the key couldn't be retrieved, it means it's NOT actually configured. This path should be unreachable (if provider is available, key should exist), or if it is reachable, it indicates an error state, not OK.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/doctor.py
Line: 121:128

Comment:
Logic issue in lines 121-128: when a provider is in `available_provider_names` but `api_key` is None (line 96 returns None), the code marks it as OK with message ""Configured (via config or env)"". This is misleading - if the key couldn't be retrieved, it means it's NOT actually configured. This path should be unreachable (if provider is available, key should exist), or if it is reachable, it indicates an error state, not OK.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1166#discussion_r2731018654,✅ Fixed in commit 6eea9df - now shows WARNING with message 'Provider available but key not retrievable for validation' instead of misleading OK status,2026-01-27T09:13:55+00:00,logic,medium,Greptile correctly identified a logic bug where a provider could be marked as 'OK' and 'Configured' when the API key couldn't actually be retrieved. The developer confirmed this was a real issue and fixed it to show a WARNING status instead. This is a non-obvious diagnostic logic error that could mislead users about their system configuration.,2026-01-27T10:21:37.363257+00:00
helixdb/helix-db,831,feat (cli): helix sync command,https://github.com/HelixDB/helix-db/pull/831,,"New files from remote aren't added to `differing_files` list, so users won't see them in the confirmation prompt. Only existing files that differ are tracked.

```suggestion
    // Collect files that differ from local
    let mut differing_files: Vec<String> = Vec::new();
    for (filename, content) in &sync_response.hx_files {
        let file_path = queries_dir.join(filename);
        if file_path.exists() {
            if let Ok(local_content) = std::fs::read_to_string(&file_path) {
                if local_content != *content {
                    differing_files.push(filename.clone());
                }
            }
        } else {
            // New file from remote
            differing_files.push(filename.clone());
        }
    }
```",https://github.com/HelixDB/helix-db/pull/831#discussion_r2731023264,,2026-01-27T09:15:08+00:00,logic,medium,"Greptile correctly identified that the code only adds files to `differing_files` when they exist locally AND differ from remote. New files from remote (where `!file_path.exists()`) are silently written without user confirmation, which breaks the intended UX where users should be prompted before any files are modified/created. This is a real bug in the sync confirmation flow.",2026-01-27T10:21:37.363363+00:00
useautumn/autumn,633,fix: invoice created to db,https://github.com/useautumn/autumn/pull/633,,"missing `discounts` field in update - when invoice already exists, discounts won't be updated

the create path (line 102-111) includes discounts via `createInvoiceFromStripe` which calls `getInvoiceDiscounts`, but this update path doesn't include the `discounts` field. this means if an invoice is created before the webhook processes (race condition), the discounts will remain empty even though `updatedStripeInvoice` has the expanded discount data.

```suggestion
		updates: {
			product_ids: productIds,
			internal_product_ids: internalProductIds,
			total: stripeToAtmnAmount({
				amount: updatedStripeInvoice.total,
				currency: updatedStripeInvoice.currency,
			}),
			discounts: getInvoiceDiscounts({
				expandedInvoice: updatedStripeInvoice,
			}),
		},
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: server/src/external/stripe/webhookHandlers/handleStripeInvoiceCreated/tasks/upsertAutumnInvoice.ts
Line: 84:91

Comment:
missing `discounts` field in update - when invoice already exists, discounts won't be updated

the create path (line 102-111) includes discounts via `createInvoiceFromStripe` which calls `getInvoiceDiscounts`, but this update path doesn't include the `discounts` field. this means if an invoice is created before the webhook processes (race condition), the discounts will remain empty even though `updatedStripeInvoice` has the expanded discount data.

```suggestion
		updates: {
			product_ids: productIds,
			internal_product_ids: internalProductIds,
			total: stripeToAtmnAmount({
				amount: updatedStripeInvoice.total,
				currency: updatedStripeInvoice.currency,
			}),
			discounts: getInvoiceDiscounts({
				expandedInvoice: updatedStripeInvoice,
			}),
		},
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/useautumn/autumn/pull/633#discussion_r2731190679,,2026-01-27T09:58:31+00:00,data_integrity,medium,"Greptile correctly identifies an inconsistency between the update and create paths. The code fetches expanded discount data via getStripeInvoice with 'discounts.source.coupon' expansion, but the update path at line 84-91 only updates product_ids, internal_product_ids, and total - omitting discounts. Meanwhile, the create path uses createInvoiceFromStripe which would process the discount data. This creates a data integrity issue where existing invoices won't have their discount information updated correctly.",2026-01-27T10:21:37.363435+00:00
tinyrange/cc,86,Added filesystem snapshot support to the public API,https://github.com/tinyrange/cc/pull/86,,"potential resource leak - previous `currentSnap` isn't closed before reassignment

```suggestion
				snap, err := f.loadFromManifest(ctx, manifest)
				if err == nil {
					if currentSnap != baseSource {
						currentSnap.Close()
					}
					currentSnap = snap
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: internal/api/fs_snapshot_factory.go
Line: 229:231

Comment:
potential resource leak - previous `currentSnap` isn't closed before reassignment

```suggestion
				snap, err := f.loadFromManifest(ctx, manifest)
				if err == nil {
					if currentSnap != baseSource {
						currentSnap.Close()
					}
					currentSnap = snap
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/tinyrange/cc/pull/86#discussion_r2730938369,,2026-01-27T08:54:07+00:00,resource_leak,medium,"Looking at the original code around line 243, when a cached snapshot is found and loaded successfully via `loadFromManifest`, the previous `currentSnap` is reassigned without being closed first (unless it equals `baseSource`). The suggestion correctly adds the missing `Close()` call before reassignment. This is a real resource leak that could accumulate over multiple iterations of the caching loop, and it's non-obvious because the similar cleanup pattern exists elsewhere in the code (around line 281) but was missed in this specific location.",2026-01-27T10:21:37.363558+00:00
