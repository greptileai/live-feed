repo,pr_number,pr_title,pr_url,comment_body,comment_url,reply_body,created_at,bug_category,severity,llm_reasoning,evaluated_at
PostHog/posthog,45413,chore: Use pre-migrated database and update comment when containers start,https://github.com/PostHog/posthog/pull/45413,"**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: bin/hobby-ci.py
Line: 1045:1046

Comment:
**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45413#discussion_r2706357612,"Fixed in aa3a4c5. The function now creates the comment if it doesn't exist yet (similar to `update_smoke_test_comment()`), ensuring users get the early notification that the instance is accessible even when called before the final smoke test comment is created.",2026-01-20T00:08:16+00:00,logic,medium,Greptile correctly identified that update_containers_started_comment() would silently fail because it tries to update a PR comment that doesn't exist yet at that point in the execution flow. The developer confirmed and fixed this bug. This is a real logic bug that would cause the 'early notification' feature to completely not work.,2026-01-28T22:49:05.093404+00:00
PostHog/posthog,45414,chore: Optimize Dagster CI tests: use pre-migrated schema and pytest-xdist,https://github.com/PostHog/posthog/pull/45414,"**logic:** Missing sqlx migrations for `posthog_persons` database. Tests that use `@pytest.mark.django_db(databases=[""default"", ""persons_db_writer""])` will fail because `posthog/conftest.py:run_persons_sqlx_migrations()` requires sqlx-cli to run migrations.

The old workflow ran sqlx commands to create the persons database and run migrations from `rust/persons_migrations/`. The persons migrations are not included in the `migrated-schema` artifact (which only dumps the `posthog` database per `ci-backend.yml:421-422`).

You need to either:
1. Keep the sqlx-cli installation and run the sqlx migrations here, OR
2. Include the persons database in the migrated-schema artifact in ci-backend.yml

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/ci-dagster.yml
Line: 153:161

Comment:
**logic:** Missing sqlx migrations for `posthog_persons` database. Tests that use `@pytest.mark.django_db(databases=[""default"", ""persons_db_writer""])` will fail because `posthog/conftest.py:run_persons_sqlx_migrations()` requires sqlx-cli to run migrations.

The old workflow ran sqlx commands to create the persons database and run migrations from `rust/persons_migrations/`. The persons migrations are not included in the `migrated-schema` artifact (which only dumps the `posthog` database per `ci-backend.yml:421-422`).

You need to either:
1. Keep the sqlx-cli installation and run the sqlx migrations here, OR
2. Include the persons database in the migrated-schema artifact in ci-backend.yml

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45414#discussion_r2706362315,,2026-01-20T00:12:40+00:00,logic,high,"This comment identifies a concrete missing functionality - the persons database migrations are removed but still required by tests. The comment correctly identifies that the old workflow ran sqlx migrations but the new workflow doesn't, and the migrated-schema artifact only contains the posthog database, not the posthog_persons database. This will cause test failures for any tests marked with `databases=[""default"", ""persons_db_writer""]`.",2026-01-28T22:49:05.093447+00:00
posthog/posthog,46208,fix(sig): re-use ongoing video export workflow,https://github.com/PostHog/posthog/pull/46208,"<details>
<summary>Additional Comments (1)</summary>

**`posthog/temporal/ai/session_summary/activities/a1_export_session_video.py`**
when catching `WorkflowAlreadyStartedError`, the code waits for the existing workflow but then returns the NEW `exported_asset.id` that was never processed. The existing workflow is processing a DIFFERENT asset ID (from the first request). This means the returned asset will never have its `content_location` or `content` populated.

need to either:
1. query for the existing asset (checking lines 46-54) BEFORE creating a new one when a workflow is already running, OR
2. look up and return the existing asset's ID that the already-running workflow is processing

the workflow_id is based on `team_id` and `session_id`, but each request creates its own unique `exported_asset`. when the second request catches the exception, it should return the FIRST request's asset ID, not its own.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/ai/session_summary/activities/a1_export_session_video.py
Line: 130:148

Comment:
when catching `WorkflowAlreadyStartedError`, the code waits for the existing workflow but then returns the NEW `exported_asset.id` that was never processed. The existing workflow is processing a DIFFERENT asset ID (from the first request). This means the returned asset will never have its `content_location` or `content` populated.

need to either:
1. query for the existing asset (checking lines 46-54) BEFORE creating a new one when a workflow is already running, OR
2. look up and return the existing asset's ID that the already-running workflow is processing

the workflow_id is based on `team_id` and `session_id`, but each request creates its own unique `exported_asset`. when the second request catches the exception, it should return the FIRST request's asset ID, not its own.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/PostHog/posthog/pull/46208#issuecomment-3814257126,,2026-01-28T22:28:21+00:00,data_integrity,high,"This is a genuine concurrency bug where catching WorkflowAlreadyStartedError leads to returning a newly created asset ID that will never be populated with data, since the existing workflow is processing a different asset. This would cause users to receive an empty/incomplete exported asset in normal concurrent usage scenarios. The analysis is correct and identifies a real data integrity issue.",2026-01-28T23:04:50.907478+00:00
nvidia-nemo/curator,1437,ci: Optimize docker layer for caching and add remote cache,https://github.com/NVIDIA-NeMo/Curator/pull/1437,"`${{ env.REPO_LOWER }}` is empty at line 130-131 - it's set at line 92 but composite actions don't make `env` variables available to subsequent steps. Cache and tag references will be malformed like `nemoci.azurecr.io/:abc123`

Set REPO_LOWER as output from the normalize step and reference `${{ steps.normalize.outputs.repo_lower }}` instead",https://github.com/NVIDIA-NeMo/Curator/pull/1437#discussion_r2738911992,,2026-01-28T22:38:52+00:00,runtime,high,"Comment 9 correctly identifies that `${{ env.REPO_LOWER }}` will be empty when used in the Docker build step because GitHub Actions composite actions don't propagate environment variables set in one step to subsequent steps. This will cause malformed image references like `nemoci.azurecr.io/:abc123` instead of `nemoci.azurecr.io/curator:abc123`, breaking the entire container build and push workflow. The comment also provides a concrete solution.",2026-01-28T23:04:50.907610+00:00
nvidia/numba-cuda,760,Replace legacy wheels-build.yaml with build-wheel.yml in publish workflow,https://github.com/NVIDIA/numba-cuda/pull/760,"The source distribution (sdist) is no longer being built or published. The old workflow downloaded `sdist-py3.13-amd64` artifact and published it to PyPI. The new workflow only publishes wheels.

The PR description states ""Add dedicated build-sdist job for source distribution"", but the actual implementation in commit 521da34 removed sdist building entirely with the note ""sdist not needed"". This contradicts the PR description and removes functionality that was present before.

Verify whether sdist is truly not needed for PyPI publishing, or if it should be restored.",https://github.com/NVIDIA/numba-cuda/pull/760#discussion_r2738854040,Updated initial comment,2026-01-28T22:18:53+00:00,logic,medium,"Greptile caught a discrepancy between the PR description claiming to 'Add dedicated build-sdist job for source distribution' and the actual implementation which removed sdist building entirely. This is a legitimate functionality removal that contradicts the stated PR intent. The developer acknowledged this by saying 'Updated initial comment', confirming the catch was valid.",2026-01-28T23:04:50.907653+00:00
onyx-dot-app/onyx,7962,fix(craft): auto set best model instead of checking for visibility,https://github.com/onyx-dot-app/onyx/pull/7962,"Missing model availability check - the function returns hardcoded model names without verifying they exist in the provider's `model_configurations`. The old implementation checked `matchingProvider.model_configurations.some((m) => m.name === model)` before selecting a model. This will cause users to be assigned non-existent models, breaking LLM functionality.

```suggestion
  // Try each priority provider in order
  for (const { provider, modelName } of LLM_SELECTION_PRIORITY) {
    const matchingProvider = llmProviders.find((p) => p.provider === provider);
    if (matchingProvider) {
      // Check if the model exists in the provider's configurations
      const hasModel = matchingProvider.model_configurations.some(
        (m) => m.name === modelName
      );
      if (hasModel) {
        return {
          providerName: matchingProvider.name,
          provider: matchingProvider.provider,
          modelName,
        };
      }
    }
  }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/craft/onboarding/constants.ts
Line: 35:44

Comment:
Missing model availability check - the function returns hardcoded model names without verifying they exist in the provider's `model_configurations`. The old implementation checked `matchingProvider.model_configurations.some((m) => m.name === model)` before selecting a model. This will cause users to be assigned non-existent models, breaking LLM functionality.

```suggestion
  // Try each priority provider in order
  for (const { provider, modelName } of LLM_SELECTION_PRIORITY) {
    const matchingProvider = llmProviders.find((p) => p.provider === provider);
    if (matchingProvider) {
      // Check if the model exists in the provider's configurations
      const hasModel = matchingProvider.model_configurations.some(
        (m) => m.name === modelName
      );
      if (hasModel) {
        return {
          providerName: matchingProvider.name,
          provider: matchingProvider.provider,
          modelName,
        };
      }
    }
  }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7962#discussion_r2739148674,,2026-01-29T00:07:29+00:00,logic,high,"The comment correctly identifies that the new implementation removed a critical validation check - the code returns hardcoded model names (like 'claude-opus-4-5', 'gpt-5.2') without verifying these models actually exist in the provider's configuration. This is a regression from the old implementation that checked model availability. Users could be assigned non-existent models, breaking LLM functionality during normal usage of the onboarding flow.",2026-01-29T00:48:55.408962+00:00
credal-ai/actions-sdk,482,Create action that reads comments from docs,https://github.com/Credal-ai/actions-sdk/pull/482,"Google Drive API returns `id` not `commentId`. This will cause all comment IDs to be undefined.

```suggestion
      res.data.comments?.map((c: any) => ({
        commentId: c.id,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/actions/providers/google-oauth/readCommentsOnDoc.ts
Line: 40:41

Comment:
Google Drive API returns `id` not `commentId`. This will cause all comment IDs to be undefined.

```suggestion
      res.data.comments?.map((c: any) => ({
        commentId: c.id,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Credal-ai/actions-sdk/pull/482#discussion_r2739225309,,2026-01-29T00:33:49+00:00,logic,high,"Greptile correctly identifies that the Google Drive API returns 'id' not 'commentId' for comments, which means all comment IDs would be undefined in the output. This is a real bug that would affect every user of this function in normal usage, making comment identification impossible. The catch demonstrates knowledge of the external API contract.",2026-01-29T00:48:55.409045+00:00
davis7dotsh/better-context,134,"update: use ""better result"" for handling errors, instead of try catch spam",https://github.com/davis7dotsh/better-context/pull/134,"The result is matched but not returned, causing `gitClone` to return `undefined` instead of throwing on error

```suggestion
	return result.match({
		ok: () => undefined,
		err: (error) => {
			throw error;
		}
	});
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/server/src/resources/impls/git.ts
Line: 306:311

Comment:
The result is matched but not returned, causing `gitClone` to return `undefined` instead of throwing on error

```suggestion
	return result.match({
		ok: () => undefined,
		err: (error) => {
			throw error;
		}
	});
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/davis7dotsh/better-context/pull/134#discussion_r2739888680,,2026-01-29T04:21:28+00:00,logic,high,"Greptile correctly identifies that the `result.match()` call's return value is not being returned from the function, causing `gitClone` to silently return `undefined` instead of throwing errors on failure. This is a real bug introduced during the refactoring from try-catch to Result pattern - errors will be swallowed and git clone failures won't be properly propagated to callers, affecting normal usage.",2026-01-29T04:44:12.144348+00:00
raycast/extensions,24954,Add markdown-docs extension,https://github.com/raycast/extensions/pull/24954,"This PR adds a Git submodule reference instead of the actual extension files. Extensions should be submitted directly to the repository, not as submodules. Copy the extension files directly into `extensions/markdown-docs/` instead of adding a submodule pointer.",https://github.com/raycast/extensions/pull/24954#discussion_r2740056067,,2026-01-29T05:44:11+00:00,data_integrity,critical,"This catch identifies that the PR adds a Git submodule reference instead of actual extension files, which means the extension won't work at all when the PR is merged. This is a fundamental submission error that would completely break the extension in the Raycast extensions repository. The diff clearly shows only a submodule commit hash being added rather than actual code files.",2026-01-29T06:28:57.078799+00:00
nvidia/matx,1125,Avoid warning about unused variables,https://github.com/NVIDIA/MatX/pull/1125,"`unfused_range` is now undefined. `MATX_NVTX_START_RANGE` was changed to not assign to a variable, but line 36 still references `unfused_range` in `MATX_NVTX_END_RANGE(unfused_range)`.

```suggestion
    [[maybe_unused]] int unfused_range = MATX_NVTX_START_RANGE(""Unfused Kernels"");
    (result = cos(C)).run(exec);
    (result = result / D).run(exec);
    (result = result * B).run(exec);
    MATX_NVTX_END_RANGE(unfused_range);
```",https://github.com/NVIDIA/MatX/pull/1125#discussion_r2740872358,,2026-01-29T10:05:25+00:00,runtime,high,"Greptile correctly identified that removing the variable assignment from MATX_NVTX_START_RANGE creates undefined variable references. The code still uses `unfused_range` and `fused_range` in MATX_NVTX_END_RANGE calls, but these variables no longer exist after the PR changes. This will cause a compilation error, making it a clear bug introduced by the PR.",2026-01-29T10:25:30.134993+00:00
twentyhq/twenty,17530,Refactor `delete` `objectMetadata` action type and handler to be workspace agnostic,https://github.com/twentyhq/twenty/pull/17530,"inconsistent error handling: `update-field-action-handler.service.ts` uses `WorkspaceMigrationActionExecutionException` for this same check (line 104-107), but this throws a generic `Error`

```suggestion
    if (!isDefined(objectMetadataUniversalIdentifier)) {
      throw new WorkspaceMigrationActionExecutionException({
        message: `objectMetadataUniversalIdentifier is not defined for field metadata ${universalIdentifier}`,
        code: WorkspaceMigrationActionExecutionExceptionCode.FLAT_ENTITY_NOT_FOUND,
      });
    }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/workspace-manager/workspace-migration/workspace-migration-runner/action-handlers/field/services/delete-field-action-handler.service.ts
Line: 72:76

Comment:
inconsistent error handling: `update-field-action-handler.service.ts` uses `WorkspaceMigrationActionExecutionException` for this same check (line 104-107), but this throws a generic `Error`

```suggestion
    if (!isDefined(objectMetadataUniversalIdentifier)) {
      throw new WorkspaceMigrationActionExecutionException({
        message: `objectMetadataUniversalIdentifier is not defined for field metadata ${universalIdentifier}`,
        code: WorkspaceMigrationActionExecutionExceptionCode.FLAT_ENTITY_NOT_FOUND,
      });
    }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17530#discussion_r2741102919,Nice catch,2026-01-29T11:04:56+00:00,logic,low,"Greptile caught an inconsistency in error handling where the delete handler throws a generic Error while the update handler uses the proper WorkspaceMigrationActionExecutionException for the same check. This was confirmed by the developer with 'Nice catch'. While this is a code consistency issue rather than a bug that would cause incorrect behavior, the developer confirmation makes it a valid catch.",2026-01-29T12:27:12.487687+00:00
gptme/gptme,1182,fix(llm): handle OpenRouter subprovider suffix in model lookup,https://github.com/gptme/gptme/pull/1182,"Suffix not preserved when model found via dynamic fetch. When `model_meta.model` matches `lookup_model_name`, returning `model_meta` directly loses the original suffix. Should create new ModelMeta with original `model_name` like the static path does:

```suggestion
                        # Check both full name (with suffix) and base name (without suffix)
                        if (
                            model_meta.model == model_name
                            or model_meta.model == lookup_model_name
                        ):
                            return ModelMeta(
                                provider, model_name, **model_meta.to_dict()
                            )
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/llm/models.py
Line: 566:570

Comment:
Suffix not preserved when model found via dynamic fetch. When `model_meta.model` matches `lookup_model_name`, returning `model_meta` directly loses the original suffix. Should create new ModelMeta with original `model_name` like the static path does:

```suggestion
                        # Check both full name (with suffix) and base name (without suffix)
                        if (
                            model_meta.model == model_name
                            or model_meta.model == lookup_model_name
                        ):
                            return ModelMeta(
                                provider, model_name, **model_meta.to_dict()
                            )
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1182#discussion_r2741612984,"✅ Fixed in commit 6bc865aa - Now creates new ModelMeta with original `model_name` preserving the suffix, similar to the static path.",2026-01-29T13:19:20+00:00,logic,medium,"Greptile correctly identified that when a model is found via dynamic fetch using the base name (lookup_model_name), returning model_meta directly would lose the original suffix (e.g., @moonshotai) that the user requested. This is a real bug that would cause incorrect model name preservation in API calls. The developer confirmed and fixed it.",2026-01-29T14:29:03.640787+00:00
posthog/posthog,46271,feat: multiselect for statuses and priorities,https://github.com/PostHog/posthog/pull/46271,"test expects `NULL` priority tickets to be included when filtering by specific priorities, but the backend filter logic in `tickets.py:115-116` uses `priority__in` which won't match `NULL` values. Either fix the backend to include `NULL` priorities with `Q(priority__in=priorities) | Q(priority__isnull=True)`, or update the test expectation to exclude the `NULL` priority ticket

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/conversations/backend/api/tests/test_tickets.py
Line: 307:333

Comment:
test expects `NULL` priority tickets to be included when filtering by specific priorities, but the backend filter logic in `tickets.py:115-116` uses `priority__in` which won't match `NULL` values. Either fix the backend to include `NULL` priorities with `Q(priority__in=priorities) | Q(priority__isnull=True)`, or update the test expectation to exclude the `NULL` priority ticket

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46271#discussion_r2742332062,Fixed,2026-01-29T15:51:11+00:00,logic,medium,"Greptile correctly identified that the test expects NULL priority tickets to be included when filtering by specific priorities, but the backend's `priority__in` filter won't match NULL values in SQL. This is a real logic bug that was confirmed by the developer with 'Fixed'. The catch demonstrates understanding of both Django ORM behavior and SQL NULL semantics.",2026-01-29T16:28:21.841777+00:00
posthog/posthog,46262,feat(logs): json parse log lines in log-ingestion based on team settings,https://github.com/PostHog/posthog/pull/46262,"Double stringification - `value` is already a string from `flattenJson` (line 117-145 converts everything to strings), so `value.toString()` returns the same string, then `JSON.stringify()` adds extra quotes around it. This means a value like `""info""` becomes `""\""info\""""`.

```suggestion
        newAttributes[key] = value
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nodejs/src/logs-ingestion/log-record-avro.ts
Line: 176:176

Comment:
Double stringification - `value` is already a string from `flattenJson` (line 117-145 converts everything to strings), so `value.toString()` returns the same string, then `JSON.stringify()` adds extra quotes around it. This means a value like `""info""` becomes `""\""info\""""`.

```suggestion
        newAttributes[key] = value
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46262#discussion_r2742153502,,2026-01-29T15:14:36+00:00,data_integrity,high,"This catch identifies a double stringification bug where values already converted to strings get wrapped in extra quotes via JSON.stringify(), causing data corruption (e.g., 'info' becomes '""info""'). This would affect all users in normal usage and corrupt log attribute values in production. The analysis is correct - flattenJson already converts values to strings, so the additional JSON.stringify is incorrect.",2026-01-29T16:28:21.841900+00:00
nvidia/numba-cuda,762,test(refactor): clean up `run_in_subprocess`,https://github.com/NVIDIA/numba-cuda/pull/762,"Logic error: `check=True` (line 206) raises `CalledProcessError` on non-zero exit, making the `if proc.returncode:` check (line 212) unreachable for failures. Remove `check=True` to preserve the original `AssertionError` behavior.

```suggestion
    proc = subprocess.run(
        [sys.executable, *flags, ""-c"", code],
        env=env,
        capture_output=True,
        text=True,
        timeout=timeout,
    )
    out, err = proc.stdout, proc.stderr
    if proc.returncode:
        raise AssertionError(
            f""process failed with code {proc.returncode:d}: stderr:\n{err}\n""
        )
```",https://github.com/NVIDIA/numba-cuda/pull/762#discussion_r2742143212,"This review comment seems legitimate, but I imagine we'd be better off catching the Python exception instead of removing the `check=True`?
---
Seems legit.",2026-01-29T15:12:21+00:00,logic,medium,"Greptile correctly identified that `check=True` makes the subsequent `if proc.returncode:` check unreachable dead code, since `subprocess.run` will raise `CalledProcessError` before reaching that line. This changes the error behavior from raising `AssertionError` to raising `CalledProcessError`, which could break test expectations. The developer confirmed this was legitimate.",2026-01-29T16:28:21.841987+00:00
onyx-dot-app/onyx,7977,fix: input masking,https://github.com/onyx-dot-app/onyx/pull/7977,"<details>
<summary>Additional Comments (1)</summary>

**`web/src/app/admin/embeddings/modals/ChangeCredentialsModal.tsx`**
Missing `data-ph-no-capture` attribute. Since PostHog's `maskAllInputs` is now set to `false` (in `providers.tsx:21`), this API key input needs the explicit `data-ph-no-capture` attribute to prevent session recordings from capturing sensitive credentials.

```suggestion
                    <input
                      type=""password""
                      data-ph-no-capture
                      className=""border border-border rounded w-full py-2 px-3 bg-background-emphasis""
                      value={apiKey}
                      onChange={(e: any) => setApiKey(e.target.value)}
                      placeholder=""Paste your API key here""
                    />
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/admin/embeddings/modals/ChangeCredentialsModal.tsx
Line: 206:212

Comment:
Missing `data-ph-no-capture` attribute. Since PostHog's `maskAllInputs` is now set to `false` (in `providers.tsx:21`), this API key input needs the explicit `data-ph-no-capture` attribute to prevent session recordings from capturing sensitive credentials.

```suggestion
                    <input
                      type=""password""
                      data-ph-no-capture
                      className=""border border-border rounded w-full py-2 px-3 bg-background-emphasis""
                      value={apiKey}
                      onChange={(e: any) => setApiKey(e.target.value)}
                      placeholder=""Paste your API key here""
                    />
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/onyx-dot-app/onyx/pull/7977#issuecomment-3819293947,,2026-01-29T17:57:22+00:00,security,critical,"This is a legitimate security vulnerability catch. The PR disabled global input masking (maskAllInputs: false) for PostHog session recordings, but failed to add the data-ph-no-capture attribute to an API key input field. This means sensitive credentials would be captured in session recordings and sent to PostHog, exposing them to anyone with access to those recordings. This is a real data exposure issue that directly contradicts the PR's intent to fix input masking.",2026-01-29T18:29:27.407222+00:00
openreplay/openreplay,4265,refactor(chalice): changed displayName&description handling refactor(DB): changed displayName&description handling,https://github.com/openreplay/openreplay/pull/4265,"Missing data migration - existing `display_name`, `description`, and `status` values should be migrated to the new `_customized` tables before dropping columns

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/scripts/schema/db/init_dbs/clickhouse/1.25.0/1.25.0.sql
Line: 148:158

Comment:
Missing data migration - existing `display_name`, `description`, and `status` values should be migrated to the new `_customized` tables before dropping columns

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4265#discussion_r2742737361,,2026-01-29T17:25:46+00:00,data_integrity,critical,"This catch identifies a critical data loss issue - the migration script drops columns containing existing user data (display_name, description, status) before migrating that data to the new _customized tables. Running this migration in production would permanently destroy user customizations. This is a guaranteed data loss bug that would affect all existing users.",2026-01-29T18:29:27.407387+00:00
gptme/gptme-contrib,225,feat(gptodo): add effective command to show computed task state,https://github.com/gptme/gptme-contrib/pull/225,"`blocked` state missing from `STATE_STYLES` dict

The virtual `blocked` state returned by `compute_effective_state()` isn't defined in the `STATE_STYLES` dictionary (utils.py:157-181), so it falls back to `white` styling. Consider adding an entry like:

```
""blocked"": (""red"", ""blocked""),
```

to utils.py STATE_STYLES for consistent styling of blocked tasks.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/cli.py
Line: 263:263

Comment:
`blocked` state missing from `STATE_STYLES` dict

The virtual `blocked` state returned by `compute_effective_state()` isn't defined in the `STATE_STYLES` dictionary (utils.py:157-181), so it falls back to `white` styling. Consider adding an entry like:

```
""blocked"": (""red"", ""blocked""),
```

to utils.py STATE_STYLES for consistent styling of blocked tasks.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/225#discussion_r2742594440,"✅ Fixed in commit 93c0ba0 - added `blocked` virtual state with red styling to STATE_STYLES.

Good catch! The comment distinguishes it from stored frontmatter states.",2026-01-29T16:50:36+00:00,logic,low,"Greptile correctly identified that the new 'blocked' virtual state returned by compute_effective_state() was missing from STATE_STYLES, causing it to fall back to default white styling instead of appropriate red styling. The developer confirmed this with '✅ Fixed' and 'Good catch!'. While not a critical bug, it's a valid issue that would affect user experience by making blocked tasks visually indistinct.",2026-01-29T18:29:27.407441+00:00
greptileai/react-mirror,335,[Flight] Forward early debug info for lazy chunks,https://github.com/greptileai/react-mirror/pull/335,"Field name mismatch - initialized as `forwardedDebugInfos` but type definition at line 617 declares `partialDebugInfoProgress`. This will cause the WeakMap to never be initialized, breaking the progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react-server/src/ReactFlightServer.js
Line: 742:742

Comment:
Field name mismatch - initialized as `forwardedDebugInfos` but type definition at line 617 declares `partialDebugInfoProgress`. This will cause the WeakMap to never be initialized, breaking the progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/335#discussion_r2744122368,,2026-01-30T00:39:33+00:00,type_error,high,Greptile correctly identified a field name mismatch where the type definition declares `partialDebugInfoProgress` but the initialization uses `forwardedDebugInfos`. This is a clear bug that would cause the progressive debug info tracking feature to completely fail since the WeakMap would never be properly initialized under the expected field name. The code would attempt to access `partialDebugInfoProgress` but find null/undefined instead of the WeakMap.,2026-01-30T00:50:50.252520+00:00
gptme/gptme-contrib,230,feat(plugins): add gptme-retrieval plugin for automatic context retrieval,https://github.com/gptme/gptme-contrib/pull/230,"Shell injection vulnerability - user-controlled `backend` value passed directly to shell. If backend comes from untrusted config, this allows arbitrary command execution.

```suggestion
    cmd = [backend, query, ""--mode"", mode, ""-n"", str(max_results)]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: plugins/gptme-retrieval/src/gptme_retrieval/__init__.py
Line: 201:201

Comment:
Shell injection vulnerability - user-controlled `backend` value passed directly to shell. If backend comes from untrusted config, this allows arbitrary command execution.

```suggestion
    cmd = [backend, query, ""--mode"", mode, ""-n"", str(max_results)]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/230#discussion_r2744032789,,2026-01-30T00:02:55+00:00,security,critical,"This is a legitimate shell injection vulnerability where user-controlled config values (backend) are passed to subprocess execution. The suggestion to use a list format instead of shell=True prevents command injection attacks. While the config comes from toml files (somewhat trusted), allowing arbitrary custom commands makes this a real security concern.",2026-01-30T00:50:50.252618+00:00
gptme/gptme-contrib,229,fix(gptodo): ensure tmux sessions inherit environment variables,https://github.com/gptme/gptme-contrib/pull/229,"`workspace` is not shell-escaped, creating command injection vulnerability if workspace path contains special characters

```suggestion
        full_cmd = f""bash -l -c {shlex.quote(f'cd {shlex.quote(str(workspace))} && {env_setup}{shell_cmd}')}""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/subagent.py
Line: 173:173

Comment:
`workspace` is not shell-escaped, creating command injection vulnerability if workspace path contains special characters

```suggestion
        full_cmd = f""bash -l -c {shlex.quote(f'cd {shlex.quote(str(workspace))} && {env_setup}{shell_cmd}')}""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/229#discussion_r2743888735,✅ Fixed in commit ee90d31 - added `safe_workspace = shlex.quote(str(workspace))` before using it in the command.,2026-01-29T23:01:24+00:00,security,high,"Greptile correctly identified a command injection vulnerability where the workspace path was not shell-escaped when used inside the nested shell command. The developer confirmed and fixed the issue. While workspace paths containing special characters aren't common, this is a real security bug that could cause incorrect behavior or exploitation in production environments.",2026-01-30T00:50:50.252678+00:00
posthog/posthog,46345,feat(duckling): auto-create events and persons tables if missing,https://github.com/PostHog/posthog/pull/46345,"SQL injection risk: f-string interpolation in SQL query allows potential injection if parameters aren't controlled. While `catalog_alias`, `schema`, and `table` are currently hardcoded as ""duckling""/""main""/""events""/""persons"", the function signature accepts arbitrary strings.

```suggestion
def table_exists(
    conn: duckdb.DuckDBPyConnection,
    catalog_alias: str,
    schema: str,
    table: str,
) -> bool:
    """"""Check if a table exists in the DuckLake catalog.""""""
    try:
        # Use parameterized query to prevent SQL injection
        # DuckDB doesn't support parameterized identifiers in DESCRIBE,
        # so validate inputs are safe identifiers
        for identifier in [catalog_alias, schema, table]:
            if not identifier.replace(""_"", """").isalnum():
                raise ValueError(f""Invalid identifier: {identifier}"")
        conn.execute(f""DESCRIBE {catalog_alias}.{schema}.{table}"")
        return True
    except duckdb.CatalogException:
        return False
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/dags/events_backfill_to_duckling.py
Line: 295:306

Comment:
SQL injection risk: f-string interpolation in SQL query allows potential injection if parameters aren't controlled. While `catalog_alias`, `schema`, and `table` are currently hardcoded as ""duckling""/""main""/""events""/""persons"", the function signature accepts arbitrary strings.

```suggestion
def table_exists(
    conn: duckdb.DuckDBPyConnection,
    catalog_alias: str,
    schema: str,
    table: str,
) -> bool:
    """"""Check if a table exists in the DuckLake catalog.""""""
    try:
        # Use parameterized query to prevent SQL injection
        # DuckDB doesn't support parameterized identifiers in DESCRIBE,
        # so validate inputs are safe identifiers
        for identifier in [catalog_alias, schema, table]:
            if not identifier.replace(""_"", """").isalnum():
                raise ValueError(f""Invalid identifier: {identifier}"")
        conn.execute(f""DESCRIBE {catalog_alias}.{schema}.{table}"")
        return True
    except duckdb.CatalogException:
        return False
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46345#discussion_r2744290999,"Already addressed in commit 726837e7. Added `_validate_identifier()` function that validates all identifiers are alphanumeric/underscore only before interpolation, with comprehensive tests for SQL injection attempts.",2026-01-30T01:57:49+00:00,security,medium,"Greptile correctly identified a SQL injection vulnerability where f-string interpolation was used with function parameters that could accept arbitrary strings. The developer confirmed this was a valid catch and fixed it by adding identifier validation. While the current usage was hardcoded safe values, the function signature allowed arbitrary input, making this a real security concern worth fixing.",2026-01-30T03:10:50.718850+00:00
greptileai/react-mirror,335,[Flight] Forward early debug info for lazy chunks,https://github.com/greptileai/react-mirror/pull/335,"Field name mismatch - should be `partialDebugInfoProgress` not `forwardedDebugInfos` (type at line 617 declares `partialDebugInfoProgress`). This breaks progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react-server/src/ReactFlightServer.js
Line: 742:742

Comment:
Field name mismatch - should be `partialDebugInfoProgress` not `forwardedDebugInfos` (type at line 617 declares `partialDebugInfoProgress`). This breaks progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/335#discussion_r2744270097,,2026-01-30T01:47:00+00:00,type_error,high,This is a clear field name mismatch bug where the type declaration uses `partialDebugInfoProgress` but the initialization uses `forwardedDebugInfos`. This would cause runtime errors or broken functionality when the code tries to access the correctly-named field that was never initialized. The type system would not catch this at runtime since JavaScript doesn't enforce types.,2026-01-30T03:10:50.719005+00:00
posthog/posthog,46349,chore(ducklake): simplify schema and table naming,https://github.com/PostHog/posthog/pull/46349,"removing `_{schema.id.hex[:8]}` from table names will cause naming conflicts when a team has multiple `ExternalDataSchema` objects with the same `source_type` and `normalized_name`. The codebase explicitly acknowledges this possibility (see `external_data_schema.py:368`), and the schema's unique ID is used in `folder_path()` to avoid conflicts. Without the schema ID suffix, `CREATE OR REPLACE TABLE` will overwrite data from different schemas.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/ducklake/ducklake_copy_data_imports_workflow.py
Line: 197:199

Comment:
removing `_{schema.id.hex[:8]}` from table names will cause naming conflicts when a team has multiple `ExternalDataSchema` objects with the same `source_type` and `normalized_name`. The codebase explicitly acknowledges this possibility (see `external_data_schema.py:368`), and the schema's unique ID is used in `folder_path()` to avoid conflicts. Without the schema ID suffix, `CREATE OR REPLACE TABLE` will overwrite data from different schemas.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46349#discussion_r2744668903,,2026-01-30T05:28:21+00:00,data_integrity,high,"This is a legitimate data integrity bug. Removing the schema ID suffix from table names will cause data overwrites when a team has multiple ExternalDataSchema objects with the same source_type and normalized_name - a scenario the codebase explicitly acknowledges is possible. The reviewer correctly identified that the unique ID is used elsewhere (folder_path) to prevent this exact conflict, and without it, CREATE OR REPLACE TABLE will silently overwrite different schemas' data.",2026-01-30T06:29:30.424979+00:00
helixdb/helix-db,843,Add HNSWDiagnostics endpoint for graph health checks,https://github.com/HelixDB/helix-db/pull/843,"`get_all_vector_ids` method doesn't exist on `VectorCore`

`VectorCore` doesn't have a `get_all_vector_ids` method. Need to iterate over `vector_properties_db` instead:

```suggestion
    let vector_ids: Vec<u128> = db
        .vectors
        .vector_properties_db
        .iter(&txn)
        .map_err(|e| GraphError::New(format!(""Failed to iterate vector IDs: {}"", e)))?
        .filter_map(|r| r.ok())
        .map(|(id, _)| id)
        .collect();
```",https://github.com/HelixDB/helix-db/pull/843#discussion_r2744671602,,2026-01-30T05:29:18+00:00,runtime,high,"Greptile correctly identified that the code calls a method `get_all_vector_ids` that doesn't exist on the `VectorCore` type. This would cause a compilation error, preventing the feature from working at all. The reviewer provided a concrete fix showing how to iterate over the underlying database instead.",2026-01-30T06:29:30.425041+00:00
green-coding-solutions/green-metrics-tool,1529,(fix): Hardware Info Root Sudoers entry was possible to hijack,https://github.com/green-coding-solutions/green-metrics-tool/pull/1529,"**CRITICAL: Sudoers entry doesn't match actual code invocations**

The sudoers entries now require direct path format:
- `/usr/bin/python3 /path/to/lib/hardware_info_root.py`

But the actual code still uses module format:
- `lib/scenario_runner.py:797`: `sudo /usr/bin/python3 -m lib.hardware_info_root`
- `lib/utils.py:216`: `sudo python3 -m lib.hardware_info_root --read-rapl-energy-filtering`

These won't match the sudoers entry and sudo will prompt for password or fail.

**Fix:** Either update `scenario_runner.py` and `utils.py` to use direct path invocation, OR keep the sudoers entry using `-m lib.hardware_info_root` format (which was the original vulnerable format that this PR aims to fix).

The secure solution is to update the code invocations to match the new sudoers format.",https://github.com/green-coding-solutions/green-metrics-tool/pull/1529#discussion_r2745104559,,2026-01-30T08:04:48+00:00,runtime,critical,"This catch identifies a critical mismatch where the sudoers file is being updated to use direct path format, but the actual code invocations in scenario_runner.py and utils.py still use the `-m lib.hardware_info_root` module format. This would cause sudo commands to fail or prompt for password in production, completely breaking the functionality. The comment correctly identifies both the problem and the solution path.",2026-01-30T08:23:21.146349+00:00
posthog/posthog,46351,feat(devex): nodejs composable capability-groups,https://github.com/PostHog/posthog/pull/46351,"Dependency creates circular issue with capability group mapping

`nodejs_realtime_cohorts` requires `nodejs_cdp_workflows`, but in `generator.py:227-234`, the capability mapping is flat - it doesn't handle transitive dependencies. When a user selects only `nodejs_realtime_cohorts`, the Python code will only add `realtime_cohorts` to `NODEJS_CAPABILITY_GROUPS`, but the TypeScript code in `capabilities.ts:28-30` only enables `cdpPrecalculatedFilters` and `cdpCohortMembership` without the CDP workflow capabilities that are required by the intent-map dependency.

Either:
1. Change the requires to `[event_ingestion]` instead of `[nodejs_cdp_workflows]`, OR
2. Update `generator.py` to resolve transitive dependencies when building the capability groups string

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: devenv/intent-map.yaml
Line: 104:104

Comment:
Dependency creates circular issue with capability group mapping

`nodejs_realtime_cohorts` requires `nodejs_cdp_workflows`, but in `generator.py:227-234`, the capability mapping is flat - it doesn't handle transitive dependencies. When a user selects only `nodejs_realtime_cohorts`, the Python code will only add `realtime_cohorts` to `NODEJS_CAPABILITY_GROUPS`, but the TypeScript code in `capabilities.ts:28-30` only enables `cdpPrecalculatedFilters` and `cdpCohortMembership` without the CDP workflow capabilities that are required by the intent-map dependency.

Either:
1. Change the requires to `[event_ingestion]` instead of `[nodejs_cdp_workflows]`, OR
2. Update `generator.py` to resolve transitive dependencies when building the capability groups string

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46351#discussion_r2745298413,"  Fixed - changed nodejs_realtime_cohorts to require event_ingestion instead of nodejs_cdp_workflows. The intent-map resolver handles transitive dependencies at resolution time, so the consumers are correctly independent.                                                ",2026-01-30T09:04:41+00:00,logic,medium,"This catch identified a real dependency resolution bug where nodejs_realtime_cohorts required nodejs_cdp_workflows, but the capability mapping was flat and wouldn't resolve transitive dependencies correctly. The developer confirmed and fixed it by changing the dependency to event_ingestion. This is a genuine architectural issue that could cause missing capabilities at runtime.",2026-01-30T10:22:36.136078+00:00
green-coding-solutions/green-metrics-tool,1529,(fix): Hardware Info Root Sudoers entry was possible to hijack,https://github.com/green-coding-solutions/green-metrics-tool/pull/1529,"`readlink -f` requires GNU coreutils on macOS (BSD readlink doesn't support `-f`). Script installs coreutils only if `stdbuf` is missing, but these commands will fail if coreutils wasn't installed.

```suggestion
    powermetrics_path=$(grealpath ""/usr/bin/powermetrics"" 2>/dev/null || readlink -f ""/usr/bin/powermetrics"" 2>/dev/null || realpath ""/usr/bin/powermetrics"")
    killall_path=$(grealpath ""/usr/bin/killall"" 2>/dev/null || readlink -f ""/usr/bin/killall"" 2>/dev/null || realpath ""/usr/bin/killall"")
```",https://github.com/green-coding-solutions/green-metrics-tool/pull/1529#discussion_r2745872648,,2026-01-30T11:46:09+00:00,runtime,high,"This is a valid catch that will cause the macOS install script to fail. The script uses `readlink -f` which is a GNU coreutils feature not available in BSD readlink on macOS by default. The script only conditionally installs coreutils if stdbuf is missing, so this command will fail for users who have stdbuf but not full coreutils. This is a real bug that will break installation for a significant portion of macOS users.",2026-01-30T12:26:54.956779+00:00
helixdb/helix-db,844,"fix(hql+core): fixing nested ID, math ops",https://github.com/HelixDB/helix-db/pull/844,"missing method implementations that are called by generated code in `computed_expr.rs`:
- `pow(&Value) -> Value`
- `abs() -> Value` 
- `sqrt() -> Value`
- `min(&Value) -> Value`
- `max(&Value) -> Value`
- `Rem` trait implementation for modulo

also need `to_f64()` to be public or use alternative conversion for pow/sqrt",https://github.com/HelixDB/helix-db/pull/844#discussion_r2745924528,,2026-01-30T11:58:15+00:00,runtime,high,"Comment 6 consolidates all the missing method issues (pow, abs, sqrt, min, max, Rem trait) that comments 0-5 identify individually. The generated code in computed_expr.rs calls methods that don't exist on the Value type, which will cause compilation failures when users try to use these math operations. This is a comprehensive catch that identifies a real bug where the code generator produces invalid Rust code.",2026-01-30T12:26:54.956894+00:00
gptme/gptme-contrib,233,"feat(gptodo): add checker pattern, dependency tree, and ready_for_review state",https://github.com/gptme/gptme-contrib/pull/233,"`details[""total""]` should exclude URL dependencies since they're skipped at line 144-146. Currently if URL deps exist, `len(resolved) + len(unresolved)` won't equal `total`, causing confusing output.

```suggestion
    details[""total""] = len(resolved) + len(unresolved)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/checker.py
Line: 157:157

Comment:
`details[""total""]` should exclude URL dependencies since they're skipped at line 144-146. Currently if URL deps exist, `len(resolved) + len(unresolved)` won't equal `total`, causing confusing output.

```suggestion
    details[""total""] = len(resolved) + len(unresolved)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/233#discussion_r2745847919,✅ Fixed in bf34c25 - now excluding URL deps from total count,2026-01-30T11:39:07+00:00,logic,medium,"This is a verified logic bug where URL dependencies are skipped during processing but the total count still includes them, causing the math to not add up (resolved + unresolved != total). The developer confirmed and fixed it. It's a clear data inconsistency issue that would produce confusing output for users.",2026-01-30T12:26:54.956946+00:00
posthog/posthog,46368,feat: add NodeJS Recording API to support encryption of recording data,https://github.com/PostHog/posthog/pull/46368,"The `key` parameter accepts arbitrary S3 paths without validation. Consider validating the key format matches expected patterns (e.g., starts with expected prefix) to prevent potential path traversal or access to unintended S3 objects.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nodejs/src/recording-api/recording-api.ts
Line: 164:164

Comment:
The `key` parameter accepts arbitrary S3 paths without validation. Consider validating the key format matches expected patterns (e.g., starts with expected prefix) to prevent potential path traversal or access to unintended S3 objects.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46368#discussion_r2746164321,Fixed in latest commit,2026-01-30T13:02:17+00:00,security,medium,"Comment 2 identifies a path traversal vulnerability where arbitrary S3 paths could be accessed without validation. The developer confirmed it was a valid issue and fixed it. While Comments 0 and 1 were false positives (internal API), this S3 key validation issue is a legitimate security concern that was addressed.",2026-01-30T14:25:02.873463+00:00
openreplay/openreplay,4268,fix(chalice): fixed idp_group_prefixes default value,https://github.com/openreplay/openreplay/pull/4268,"Migration drops columns with existing data but doesn't migrate that data to the new `all_events_customized` table. Any existing `display_name`, `description`, or `status` values will be permanently lost.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/scripts/schema/db/init_dbs/clickhouse/1.25.0/1.25.0.sql
Line: 147:156

Comment:
Migration drops columns with existing data but doesn't migrate that data to the new `all_events_customized` table. Any existing `display_name`, `description`, or `status` values will be permanently lost.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4268#discussion_r2746479945,,2026-01-30T14:05:07+00:00,data_integrity,critical,"The migration script drops columns (display_name, description, status) containing existing user data before creating the new tables to hold that data, but includes no data migration step. This will cause permanent data loss in production for all existing customizations. This is a critical data integrity issue that would affect all users who have customized their events.",2026-01-30T14:25:02.873523+00:00
helixdb/helix-db,845,impr (hql+core): nested ids and math ops,https://github.com/HelixDB/helix-db/pull/845,"Division by zero not handled - `wrapping_div` panics on zero divisor

```suggestion
            // Same-type signed integer divisions
            (Value::I8(a), Value::I8(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I8(a.wrapping_div(b))
            }
            (Value::I16(a), Value::I16(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I16(a.wrapping_div(b))
            }
            (Value::I32(a), Value::I32(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I32(a.wrapping_div(b))
            }
            (Value::I64(a), Value::I64(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I64(a.wrapping_div(b))
            }
```",https://github.com/HelixDB/helix-db/pull/845#discussion_r2746255502,,2026-01-30T13:22:47+00:00,runtime,high,"This is a genuine bug catch - division by zero with wrapping_div will cause a panic in Rust, crashing the program. This affects normal usage whenever a user performs division with a zero divisor through the Value type's arithmetic operations. The comment correctly identifies the issue and provides a concrete fix.",2026-01-30T14:25:02.873554+00:00
posthog/posthog,46391,feat(customers): Add revenue data to person profiles,https://github.com/PostHog/posthog/pull/46391,"the `loadRevenueData` action is never called, so revenue data will never load. Add an `afterMount` that calls `actions.loadRevenueData()` (similar to how `groupLogic` calls `actions.loadGroupRevenueAnalyticsData()` in its `afterMount` at groupLogic.ts:316)

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/persons/personLogic.tsx
Line: 137:150

Comment:
the `loadRevenueData` action is never called, so revenue data will never load. Add an `afterMount` that calls `actions.loadRevenueData()` (similar to how `groupLogic` calls `actions.loadGroupRevenueAnalyticsData()` in its `afterMount` at groupLogic.ts:316)

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46391#discussion_r2746560402,,2026-01-30T14:24:44+00:00,logic,high,"This is a clear functional bug - the PR adds a `loadRevenueData` loader to fetch revenue data for person profiles, but never actually calls it anywhere. The revenue data feature simply won't work as implemented. The reviewer correctly identifies the pattern used elsewhere (groupLogic's afterMount) that should be followed here.",2026-01-30T16:25:13.000671+00:00
raycast/extensions,25000,Add okta-app-manager extension,https://github.com/raycast/extensions/pull/25000,`confirm()` is not available in Raycast. Use `Action` with confirmation or `Alert` API instead.,https://github.com/raycast/extensions/pull/25000#discussion_r2746966008,,2026-01-30T16:08:56+00:00,runtime,high,"The comment correctly identifies that `confirm()` is a browser API that doesn't exist in Raycast's environment. This will cause a runtime error when users try to delete an environment, making it a real bug that affects normal usage. The fix is straightforward - use Raycast's Alert API instead.",2026-01-30T16:25:13.000772+00:00
infisical/infisical,5325,feat(ui): add app connection id permission condition to secret syncs,https://github.com/Infisical/infisical/pull/5325,"<details>
<summary>Additional Comments (1)</summary>

**`backend/src/services/secret-sync/secret-sync-service.ts`**
**Critical Security Issue: Permission bypass when updating `connectionId`**

The permission checks use the OLD `connectionId` from the existing sync, but when `params.connectionId` is provided (line 502-511), the update allows changing to a NEW connection without verifying permission against that new `connectionId`.

**Attack scenario:**
1. User has Read/Edit on syncs with `connectionId=conn-1`  
2. User does NOT have access to `connectionId=conn-2`
3. User calls update with `params.connectionId=conn-2`
4. Permission check at line 436-443 passes (checks OLD `conn-1`)
5. Line 502-511 only validates the connection exists
6. User successfully updates sync to use `conn-2`, bypassing permission restrictions

**Fix required:**
When `params.connectionId` is provided, add a permission check against the NEW connection:

```typescript
if (params.connectionId && params.connectionId !== connectionId) {
  // Check permission against NEW connectionId
  ForbiddenError.from(permission).throwUnlessCan(
    ProjectPermissionSecretSyncActions.Edit,
    secretSync.environment?.slug && secretSync.folder?.path
      ? subject(ProjectPermissionSub.SecretSyncs, {
          environment: secretSync.environment.slug,
          secretPath: secretSync.folder.path,
          connectionId: params.connectionId
        })
      : ProjectPermissionSub.SecretSyncs
  );
}
```

Note: Secret rotation correctly prevents this by excluding `connectionId` from `TUpdateSecretRotationV2DTO`.
</details>",https://github.com/Infisical/infisical/pull/5325#issuecomment-3824534353,,2026-01-30T16:16:14+00:00,security,critical,"This identifies a permission bypass vulnerability where a user can change a sync's connectionId to one they don't have access to. The permission check only validates against the OLD connectionId, allowing users to escalate privileges by updating to a connection they shouldn't have access to. This is a clear security flaw in the authorization logic that could allow unauthorized access to connections.",2026-01-30T16:25:13.000803+00:00
onyx-dot-app/onyx,8013,"feat(opensearch): Migration 0 - Introduce db tables, alembic migration, db model utils",https://github.com/onyx-dot-app/onyx/pull/8013,"enum values must be lowercase to match Python enum definitions

In `backend/onyx/db/enums.py`, `OpenSearchTenantMigrationStatus` uses lowercase values (`""pending""`, `""completed""`), but this migration uses uppercase (`""PENDING""`, `""COMPLETED""`). Same issue exists on lines 40-46 and 72-78.

```suggestion
            sa.Enum(
                ""pending"",
                ""completed"",
                name=""opensearchtenantmigrationstatus"",
                native_enum=False,
            ),
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/alembic/versions/d7d881a02917_add_opensearch_migration_tables.py
Line: 26:30

Comment:
enum values must be lowercase to match Python enum definitions

In `backend/onyx/db/enums.py`, `OpenSearchTenantMigrationStatus` uses lowercase values (`""pending""`, `""completed""`), but this migration uses uppercase (`""PENDING""`, `""COMPLETED""`). Same issue exists on lines 40-46 and 72-78.

```suggestion
            sa.Enum(
                ""pending"",
                ""completed"",
                name=""opensearchtenantmigrationstatus"",
                native_enum=False,
            ),
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/8013#discussion_r2747186530,,2026-01-30T17:04:41+00:00,data_integrity,high,"This catch identifies a real mismatch between the Python enum definitions (lowercase values like 'pending', 'completed') and the database migration (uppercase values like 'PENDING', 'COMPLETED'). This would cause database operations to fail when the ORM tries to map between Python enums and database values. The comment correctly identifies the issue and provides the fix with cross-reference to the actual enum definitions.",2026-01-30T18:27:08.877934+00:00
posthog/posthog,46426,chore(feature flags): client-side search/filter,https://github.com/PostHog/posthog/pull/46426,"pagination disabled whenever client-side filtering reduces results below total count, but pagination should still work on the filtered subset. users can't navigate through filtered results if there are more than 100 matches

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/feature-flags/FeatureFlags.tsx
Line: 495:495

Comment:
pagination disabled whenever client-side filtering reduces results below total count, but pagination should still work on the filtered subset. users can't navigate through filtered results if there are more than 100 matches

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46426#discussion_r2747522938,"Agreed. Additionally, this also prevents loading more pages from the server - if the user searches for ""test"" and only 15 cached flags match, they can't fetch page 2 which might have more matches.",2026-01-30T18:45:30+00:00,logic,medium,"This comment correctly identifies that pagination is disabled when client-side filtering reduces results, preventing users from navigating through filtered results or fetching additional pages from the server. The developer confirmed this is a real issue affecting user experience. This is the most impactful catch as it breaks core functionality of browsing filtered feature flags.",2026-01-30T20:20:29.552473+00:00
getarcaneapp/arcane,1602,refactor: use docker dhi images for runtimes and builds,https://github.com/getarcaneapp/arcane/pull/1602,"`apk` is Alpine's package manager, but `dhi.io/static:20250419-glibc` appears to be a glibc-based image (not Alpine/musl-based). This will fail at build time with ""apk: command not found"" or similar error.

Check whether `dhi.io/static:20250419-glibc` is Alpine-based. If not Alpine, either:
- Use a different base image that's Alpine-based, or
- Remove the `apk add` line if `curl`, `ca-certificates`, and `tzdata` are already included in the base image (which they likely are, given the diff shows they were removed from the install line)

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: docker/next-builds/Dockerfile-static
Line: 7:7

Comment:
`apk` is Alpine's package manager, but `dhi.io/static:20250419-glibc` appears to be a glibc-based image (not Alpine/musl-based). This will fail at build time with ""apk: command not found"" or similar error.

Check whether `dhi.io/static:20250419-glibc` is Alpine-based. If not Alpine, either:
- Use a different base image that's Alpine-based, or
- Remove the `apk add` line if `curl`, `ca-certificates`, and `tzdata` are already included in the base image (which they likely are, given the diff shows they were removed from the install line)

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/getarcaneapp/arcane/pull/1602#discussion_r2747512213,,2026-01-30T18:41:32+00:00,runtime,high,"This is a valid catch - the Dockerfile switches from Alpine to a glibc-based image but still uses `apk` (Alpine's package manager), which will cause the Docker build to fail immediately. This is a guaranteed build failure that would block deployment. Comment 0 and 1 are duplicates for different files with the same issue.",2026-01-30T20:20:29.552577+00:00
openucx/ucc,1260,TL/NCCL: add user buffer registration via memmap,https://github.com/openucx/ucc/pull/1260,"buffer size calculation is incorrect for variable-size collectives (ALLTOALLV, ALLGATHERV, GATHERV, SCATTERV). For these collectives, `coll_args->args.src` uses the `info_v` union member with `counts*` array, not `info.count` scalar. Accessing `.info.count` reads the `counts` pointer as an integer, resulting in incorrect buffer size. Need to check `coll_args->args.coll_type` and use appropriate buffer size calculation for each collective type.

```suggestion
            // TODO: Need to calculate correct buffer size based on coll_type
            // For variable-size collectives (ALLTOALLV, ALLGATHERV, etc), 
            // must sum counts array instead of using single count value
            status = ucc_tl_nccl_lazy_register_memh(
                coll_args->args.src.info.buffer,
                coll_args->args.src.info.count *
                    ucc_dt_size(coll_args->args.src.info.datatype),
```",https://github.com/openucx/ucc/pull/1260#discussion_r2747554746,,2026-01-30T18:55:12+00:00,logic,high,"Comment 0 identifies a real bug where variable-size collectives (ALLTOALLV, ALLGATHERV, etc.) use a union with `counts*` array in the `info_v` member, but the code incorrectly accesses `.info.count` which would interpret the pointer as an integer. This would cause incorrect buffer size calculations for these collective operations, leading to wrong memory registration bounds.",2026-01-30T20:20:29.552655+00:00
nvidia-nemo/curator,1446,cp: `ci: Optimize docker layer and uv with no cache (1444)` into `r1.1.0`,https://github.com/NVIDIA-NeMo/Curator/pull/1446,"`nemo_curator/__init__.py` imports `cosmos_xenna.ray_utils.cluster` on line 34, which won't be available until after `uv sync` runs. This will cause the build to fail when uv tries to read the version from `nemo_curator.package_info.__version__`.

```suggestion
COPY nemo_curator/package_info.py /opt/Curator/nemo_curator/
```

Remove `__init__.py` from this COPY command and only copy `package_info.py` which is actually needed for version resolution.",https://github.com/NVIDIA-NeMo/Curator/pull/1446#discussion_r2748082814,,2026-01-30T21:46:34+00:00,runtime,high,Greptile correctly identified that copying `__init__.py` before running `uv sync` will cause a build failure because `__init__.py` imports `cosmos_xenna.ray_utils.cluster` which isn't installed yet. This is a concrete build-breaking bug that would affect anyone trying to build this Docker image. The suggestion to only copy `package_info.py` (which is what's actually needed for version resolution) is actionable and correct.,2026-01-30T22:17:33.961712+00:00
requestnetwork/requestnetwork,1693,build(deps-dev): Bump eslint from 7.22.0 to 9.26.0,https://github.com/RequestNetwork/requestNetwork/pull/1693,"ESLint 9 introduces breaking changes: `@typescript-eslint/eslint-plugin` v4.18.0 and `@typescript-eslint/parser` v4.18.0 are incompatible with ESLint 9. These TypeScript ESLint packages require upgrade to v8.x or v9.x to work with ESLint 9.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: package.json
Line: 42:43

Comment:
ESLint 9 introduces breaking changes: `@typescript-eslint/eslint-plugin` v4.18.0 and `@typescript-eslint/parser` v4.18.0 are incompatible with ESLint 9. These TypeScript ESLint packages require upgrade to v8.x or v9.x to work with ESLint 9.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RequestNetwork/requestNetwork/pull/1693#discussion_r2747918958,,2026-01-30T20:40:50+00:00,runtime,high,This catch correctly identifies that ESLint 9 has breaking changes that make it incompatible with the TypeScript ESLint packages at version 4.18.0. This would cause the build/lint process to fail in production since the packages are fundamentally incompatible. This is a significant catch for a dependency bump PR that could break CI/CD pipelines.,2026-01-30T22:17:33.961775+00:00
