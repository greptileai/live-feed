repo,pr_number,pr_title,pr_url,comment_body,comment_url,reply_body,created_at,bug_category,severity,llm_reasoning,evaluated_at
PostHog/posthog,45413,chore: Use pre-migrated database and update comment when containers start,https://github.com/PostHog/posthog/pull/45413,"**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: bin/hobby-ci.py
Line: 1045:1046

Comment:
**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45413#discussion_r2706357612,"Fixed in aa3a4c5. The function now creates the comment if it doesn't exist yet (similar to `update_smoke_test_comment()`), ensuring users get the early notification that the instance is accessible even when called before the final smoke test comment is created.",2026-01-20T00:08:16+00:00,logic,medium,Greptile correctly identified that update_containers_started_comment() would silently fail because it tries to update a PR comment that doesn't exist yet at that point in the execution flow. The developer confirmed and fixed this bug. This is a real logic bug that would cause the 'early notification' feature to completely not work.,2026-01-28T22:49:05.093404+00:00
PostHog/posthog,45414,chore: Optimize Dagster CI tests: use pre-migrated schema and pytest-xdist,https://github.com/PostHog/posthog/pull/45414,"**logic:** Missing sqlx migrations for `posthog_persons` database. Tests that use `@pytest.mark.django_db(databases=[""default"", ""persons_db_writer""])` will fail because `posthog/conftest.py:run_persons_sqlx_migrations()` requires sqlx-cli to run migrations.

The old workflow ran sqlx commands to create the persons database and run migrations from `rust/persons_migrations/`. The persons migrations are not included in the `migrated-schema` artifact (which only dumps the `posthog` database per `ci-backend.yml:421-422`).

You need to either:
1. Keep the sqlx-cli installation and run the sqlx migrations here, OR
2. Include the persons database in the migrated-schema artifact in ci-backend.yml

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/ci-dagster.yml
Line: 153:161

Comment:
**logic:** Missing sqlx migrations for `posthog_persons` database. Tests that use `@pytest.mark.django_db(databases=[""default"", ""persons_db_writer""])` will fail because `posthog/conftest.py:run_persons_sqlx_migrations()` requires sqlx-cli to run migrations.

The old workflow ran sqlx commands to create the persons database and run migrations from `rust/persons_migrations/`. The persons migrations are not included in the `migrated-schema` artifact (which only dumps the `posthog` database per `ci-backend.yml:421-422`).

You need to either:
1. Keep the sqlx-cli installation and run the sqlx migrations here, OR
2. Include the persons database in the migrated-schema artifact in ci-backend.yml

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45414#discussion_r2706362315,,2026-01-20T00:12:40+00:00,logic,high,"This comment identifies a concrete missing functionality - the persons database migrations are removed but still required by tests. The comment correctly identifies that the old workflow ran sqlx migrations but the new workflow doesn't, and the migrated-schema artifact only contains the posthog database, not the posthog_persons database. This will cause test failures for any tests marked with `databases=[""default"", ""persons_db_writer""]`.",2026-01-28T22:49:05.093447+00:00
posthog/posthog,46208,fix(sig): re-use ongoing video export workflow,https://github.com/PostHog/posthog/pull/46208,"<details>
<summary>Additional Comments (1)</summary>

**`posthog/temporal/ai/session_summary/activities/a1_export_session_video.py`**
when catching `WorkflowAlreadyStartedError`, the code waits for the existing workflow but then returns the NEW `exported_asset.id` that was never processed. The existing workflow is processing a DIFFERENT asset ID (from the first request). This means the returned asset will never have its `content_location` or `content` populated.

need to either:
1. query for the existing asset (checking lines 46-54) BEFORE creating a new one when a workflow is already running, OR
2. look up and return the existing asset's ID that the already-running workflow is processing

the workflow_id is based on `team_id` and `session_id`, but each request creates its own unique `exported_asset`. when the second request catches the exception, it should return the FIRST request's asset ID, not its own.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/ai/session_summary/activities/a1_export_session_video.py
Line: 130:148

Comment:
when catching `WorkflowAlreadyStartedError`, the code waits for the existing workflow but then returns the NEW `exported_asset.id` that was never processed. The existing workflow is processing a DIFFERENT asset ID (from the first request). This means the returned asset will never have its `content_location` or `content` populated.

need to either:
1. query for the existing asset (checking lines 46-54) BEFORE creating a new one when a workflow is already running, OR
2. look up and return the existing asset's ID that the already-running workflow is processing

the workflow_id is based on `team_id` and `session_id`, but each request creates its own unique `exported_asset`. when the second request catches the exception, it should return the FIRST request's asset ID, not its own.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/PostHog/posthog/pull/46208#issuecomment-3814257126,,2026-01-28T22:28:21+00:00,data_integrity,high,"This is a genuine concurrency bug where catching WorkflowAlreadyStartedError leads to returning a newly created asset ID that will never be populated with data, since the existing workflow is processing a different asset. This would cause users to receive an empty/incomplete exported asset in normal concurrent usage scenarios. The analysis is correct and identifies a real data integrity issue.",2026-01-28T23:04:50.907478+00:00
nvidia-nemo/curator,1437,ci: Optimize docker layer for caching and add remote cache,https://github.com/NVIDIA-NeMo/Curator/pull/1437,"`${{ env.REPO_LOWER }}` is empty at line 130-131 - it's set at line 92 but composite actions don't make `env` variables available to subsequent steps. Cache and tag references will be malformed like `nemoci.azurecr.io/:abc123`

Set REPO_LOWER as output from the normalize step and reference `${{ steps.normalize.outputs.repo_lower }}` instead",https://github.com/NVIDIA-NeMo/Curator/pull/1437#discussion_r2738911992,,2026-01-28T22:38:52+00:00,runtime,high,"Comment 9 correctly identifies that `${{ env.REPO_LOWER }}` will be empty when used in the Docker build step because GitHub Actions composite actions don't propagate environment variables set in one step to subsequent steps. This will cause malformed image references like `nemoci.azurecr.io/:abc123` instead of `nemoci.azurecr.io/curator:abc123`, breaking the entire container build and push workflow. The comment also provides a concrete solution.",2026-01-28T23:04:50.907610+00:00
nvidia/numba-cuda,760,Replace legacy wheels-build.yaml with build-wheel.yml in publish workflow,https://github.com/NVIDIA/numba-cuda/pull/760,"The source distribution (sdist) is no longer being built or published. The old workflow downloaded `sdist-py3.13-amd64` artifact and published it to PyPI. The new workflow only publishes wheels.

The PR description states ""Add dedicated build-sdist job for source distribution"", but the actual implementation in commit 521da34 removed sdist building entirely with the note ""sdist not needed"". This contradicts the PR description and removes functionality that was present before.

Verify whether sdist is truly not needed for PyPI publishing, or if it should be restored.",https://github.com/NVIDIA/numba-cuda/pull/760#discussion_r2738854040,Updated initial comment,2026-01-28T22:18:53+00:00,logic,medium,"Greptile caught a discrepancy between the PR description claiming to 'Add dedicated build-sdist job for source distribution' and the actual implementation which removed sdist building entirely. This is a legitimate functionality removal that contradicts the stated PR intent. The developer acknowledged this by saying 'Updated initial comment', confirming the catch was valid.",2026-01-28T23:04:50.907653+00:00
onyx-dot-app/onyx,7962,fix(craft): auto set best model instead of checking for visibility,https://github.com/onyx-dot-app/onyx/pull/7962,"Missing model availability check - the function returns hardcoded model names without verifying they exist in the provider's `model_configurations`. The old implementation checked `matchingProvider.model_configurations.some((m) => m.name === model)` before selecting a model. This will cause users to be assigned non-existent models, breaking LLM functionality.

```suggestion
  // Try each priority provider in order
  for (const { provider, modelName } of LLM_SELECTION_PRIORITY) {
    const matchingProvider = llmProviders.find((p) => p.provider === provider);
    if (matchingProvider) {
      // Check if the model exists in the provider's configurations
      const hasModel = matchingProvider.model_configurations.some(
        (m) => m.name === modelName
      );
      if (hasModel) {
        return {
          providerName: matchingProvider.name,
          provider: matchingProvider.provider,
          modelName,
        };
      }
    }
  }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/craft/onboarding/constants.ts
Line: 35:44

Comment:
Missing model availability check - the function returns hardcoded model names without verifying they exist in the provider's `model_configurations`. The old implementation checked `matchingProvider.model_configurations.some((m) => m.name === model)` before selecting a model. This will cause users to be assigned non-existent models, breaking LLM functionality.

```suggestion
  // Try each priority provider in order
  for (const { provider, modelName } of LLM_SELECTION_PRIORITY) {
    const matchingProvider = llmProviders.find((p) => p.provider === provider);
    if (matchingProvider) {
      // Check if the model exists in the provider's configurations
      const hasModel = matchingProvider.model_configurations.some(
        (m) => m.name === modelName
      );
      if (hasModel) {
        return {
          providerName: matchingProvider.name,
          provider: matchingProvider.provider,
          modelName,
        };
      }
    }
  }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7962#discussion_r2739148674,,2026-01-29T00:07:29+00:00,logic,high,"The comment correctly identifies that the new implementation removed a critical validation check - the code returns hardcoded model names (like 'claude-opus-4-5', 'gpt-5.2') without verifying these models actually exist in the provider's configuration. This is a regression from the old implementation that checked model availability. Users could be assigned non-existent models, breaking LLM functionality during normal usage of the onboarding flow.",2026-01-29T00:48:55.408962+00:00
credal-ai/actions-sdk,482,Create action that reads comments from docs,https://github.com/Credal-ai/actions-sdk/pull/482,"Google Drive API returns `id` not `commentId`. This will cause all comment IDs to be undefined.

```suggestion
      res.data.comments?.map((c: any) => ({
        commentId: c.id,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/actions/providers/google-oauth/readCommentsOnDoc.ts
Line: 40:41

Comment:
Google Drive API returns `id` not `commentId`. This will cause all comment IDs to be undefined.

```suggestion
      res.data.comments?.map((c: any) => ({
        commentId: c.id,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Credal-ai/actions-sdk/pull/482#discussion_r2739225309,,2026-01-29T00:33:49+00:00,logic,high,"Greptile correctly identifies that the Google Drive API returns 'id' not 'commentId' for comments, which means all comment IDs would be undefined in the output. This is a real bug that would affect every user of this function in normal usage, making comment identification impossible. The catch demonstrates knowledge of the external API contract.",2026-01-29T00:48:55.409045+00:00
davis7dotsh/better-context,134,"update: use ""better result"" for handling errors, instead of try catch spam",https://github.com/davis7dotsh/better-context/pull/134,"The result is matched but not returned, causing `gitClone` to return `undefined` instead of throwing on error

```suggestion
	return result.match({
		ok: () => undefined,
		err: (error) => {
			throw error;
		}
	});
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/server/src/resources/impls/git.ts
Line: 306:311

Comment:
The result is matched but not returned, causing `gitClone` to return `undefined` instead of throwing on error

```suggestion
	return result.match({
		ok: () => undefined,
		err: (error) => {
			throw error;
		}
	});
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/davis7dotsh/better-context/pull/134#discussion_r2739888680,,2026-01-29T04:21:28+00:00,logic,high,"Greptile correctly identifies that the `result.match()` call's return value is not being returned from the function, causing `gitClone` to silently return `undefined` instead of throwing errors on failure. This is a real bug introduced during the refactoring from try-catch to Result pattern - errors will be swallowed and git clone failures won't be properly propagated to callers, affecting normal usage.",2026-01-29T04:44:12.144348+00:00
raycast/extensions,24954,Add markdown-docs extension,https://github.com/raycast/extensions/pull/24954,"This PR adds a Git submodule reference instead of the actual extension files. Extensions should be submitted directly to the repository, not as submodules. Copy the extension files directly into `extensions/markdown-docs/` instead of adding a submodule pointer.",https://github.com/raycast/extensions/pull/24954#discussion_r2740056067,,2026-01-29T05:44:11+00:00,data_integrity,critical,"This catch identifies that the PR adds a Git submodule reference instead of actual extension files, which means the extension won't work at all when the PR is merged. This is a fundamental submission error that would completely break the extension in the Raycast extensions repository. The diff clearly shows only a submodule commit hash being added rather than actual code files.",2026-01-29T06:28:57.078799+00:00
nvidia/matx,1125,Avoid warning about unused variables,https://github.com/NVIDIA/MatX/pull/1125,"`unfused_range` is now undefined. `MATX_NVTX_START_RANGE` was changed to not assign to a variable, but line 36 still references `unfused_range` in `MATX_NVTX_END_RANGE(unfused_range)`.

```suggestion
    [[maybe_unused]] int unfused_range = MATX_NVTX_START_RANGE(""Unfused Kernels"");
    (result = cos(C)).run(exec);
    (result = result / D).run(exec);
    (result = result * B).run(exec);
    MATX_NVTX_END_RANGE(unfused_range);
```",https://github.com/NVIDIA/MatX/pull/1125#discussion_r2740872358,,2026-01-29T10:05:25+00:00,runtime,high,"Greptile correctly identified that removing the variable assignment from MATX_NVTX_START_RANGE creates undefined variable references. The code still uses `unfused_range` and `fused_range` in MATX_NVTX_END_RANGE calls, but these variables no longer exist after the PR changes. This will cause a compilation error, making it a clear bug introduced by the PR.",2026-01-29T10:25:30.134993+00:00
twentyhq/twenty,17530,Refactor `delete` `objectMetadata` action type and handler to be workspace agnostic,https://github.com/twentyhq/twenty/pull/17530,"inconsistent error handling: `update-field-action-handler.service.ts` uses `WorkspaceMigrationActionExecutionException` for this same check (line 104-107), but this throws a generic `Error`

```suggestion
    if (!isDefined(objectMetadataUniversalIdentifier)) {
      throw new WorkspaceMigrationActionExecutionException({
        message: `objectMetadataUniversalIdentifier is not defined for field metadata ${universalIdentifier}`,
        code: WorkspaceMigrationActionExecutionExceptionCode.FLAT_ENTITY_NOT_FOUND,
      });
    }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/workspace-manager/workspace-migration/workspace-migration-runner/action-handlers/field/services/delete-field-action-handler.service.ts
Line: 72:76

Comment:
inconsistent error handling: `update-field-action-handler.service.ts` uses `WorkspaceMigrationActionExecutionException` for this same check (line 104-107), but this throws a generic `Error`

```suggestion
    if (!isDefined(objectMetadataUniversalIdentifier)) {
      throw new WorkspaceMigrationActionExecutionException({
        message: `objectMetadataUniversalIdentifier is not defined for field metadata ${universalIdentifier}`,
        code: WorkspaceMigrationActionExecutionExceptionCode.FLAT_ENTITY_NOT_FOUND,
      });
    }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17530#discussion_r2741102919,Nice catch,2026-01-29T11:04:56+00:00,logic,low,"Greptile caught an inconsistency in error handling where the delete handler throws a generic Error while the update handler uses the proper WorkspaceMigrationActionExecutionException for the same check. This was confirmed by the developer with 'Nice catch'. While this is a code consistency issue rather than a bug that would cause incorrect behavior, the developer confirmation makes it a valid catch.",2026-01-29T12:27:12.487687+00:00
gptme/gptme,1182,fix(llm): handle OpenRouter subprovider suffix in model lookup,https://github.com/gptme/gptme/pull/1182,"Suffix not preserved when model found via dynamic fetch. When `model_meta.model` matches `lookup_model_name`, returning `model_meta` directly loses the original suffix. Should create new ModelMeta with original `model_name` like the static path does:

```suggestion
                        # Check both full name (with suffix) and base name (without suffix)
                        if (
                            model_meta.model == model_name
                            or model_meta.model == lookup_model_name
                        ):
                            return ModelMeta(
                                provider, model_name, **model_meta.to_dict()
                            )
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/llm/models.py
Line: 566:570

Comment:
Suffix not preserved when model found via dynamic fetch. When `model_meta.model` matches `lookup_model_name`, returning `model_meta` directly loses the original suffix. Should create new ModelMeta with original `model_name` like the static path does:

```suggestion
                        # Check both full name (with suffix) and base name (without suffix)
                        if (
                            model_meta.model == model_name
                            or model_meta.model == lookup_model_name
                        ):
                            return ModelMeta(
                                provider, model_name, **model_meta.to_dict()
                            )
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1182#discussion_r2741612984,"✅ Fixed in commit 6bc865aa - Now creates new ModelMeta with original `model_name` preserving the suffix, similar to the static path.",2026-01-29T13:19:20+00:00,logic,medium,"Greptile correctly identified that when a model is found via dynamic fetch using the base name (lookup_model_name), returning model_meta directly would lose the original suffix (e.g., @moonshotai) that the user requested. This is a real bug that would cause incorrect model name preservation in API calls. The developer confirmed and fixed it.",2026-01-29T14:29:03.640787+00:00
posthog/posthog,46271,feat: multiselect for statuses and priorities,https://github.com/PostHog/posthog/pull/46271,"test expects `NULL` priority tickets to be included when filtering by specific priorities, but the backend filter logic in `tickets.py:115-116` uses `priority__in` which won't match `NULL` values. Either fix the backend to include `NULL` priorities with `Q(priority__in=priorities) | Q(priority__isnull=True)`, or update the test expectation to exclude the `NULL` priority ticket

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/conversations/backend/api/tests/test_tickets.py
Line: 307:333

Comment:
test expects `NULL` priority tickets to be included when filtering by specific priorities, but the backend filter logic in `tickets.py:115-116` uses `priority__in` which won't match `NULL` values. Either fix the backend to include `NULL` priorities with `Q(priority__in=priorities) | Q(priority__isnull=True)`, or update the test expectation to exclude the `NULL` priority ticket

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46271#discussion_r2742332062,Fixed,2026-01-29T15:51:11+00:00,logic,medium,"Greptile correctly identified that the test expects NULL priority tickets to be included when filtering by specific priorities, but the backend's `priority__in` filter won't match NULL values in SQL. This is a real logic bug that was confirmed by the developer with 'Fixed'. The catch demonstrates understanding of both Django ORM behavior and SQL NULL semantics.",2026-01-29T16:28:21.841777+00:00
posthog/posthog,46262,feat(logs): json parse log lines in log-ingestion based on team settings,https://github.com/PostHog/posthog/pull/46262,"Double stringification - `value` is already a string from `flattenJson` (line 117-145 converts everything to strings), so `value.toString()` returns the same string, then `JSON.stringify()` adds extra quotes around it. This means a value like `""info""` becomes `""\""info\""""`.

```suggestion
        newAttributes[key] = value
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nodejs/src/logs-ingestion/log-record-avro.ts
Line: 176:176

Comment:
Double stringification - `value` is already a string from `flattenJson` (line 117-145 converts everything to strings), so `value.toString()` returns the same string, then `JSON.stringify()` adds extra quotes around it. This means a value like `""info""` becomes `""\""info\""""`.

```suggestion
        newAttributes[key] = value
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46262#discussion_r2742153502,,2026-01-29T15:14:36+00:00,data_integrity,high,"This catch identifies a double stringification bug where values already converted to strings get wrapped in extra quotes via JSON.stringify(), causing data corruption (e.g., 'info' becomes '""info""'). This would affect all users in normal usage and corrupt log attribute values in production. The analysis is correct - flattenJson already converts values to strings, so the additional JSON.stringify is incorrect.",2026-01-29T16:28:21.841900+00:00
nvidia/numba-cuda,762,test(refactor): clean up `run_in_subprocess`,https://github.com/NVIDIA/numba-cuda/pull/762,"Logic error: `check=True` (line 206) raises `CalledProcessError` on non-zero exit, making the `if proc.returncode:` check (line 212) unreachable for failures. Remove `check=True` to preserve the original `AssertionError` behavior.

```suggestion
    proc = subprocess.run(
        [sys.executable, *flags, ""-c"", code],
        env=env,
        capture_output=True,
        text=True,
        timeout=timeout,
    )
    out, err = proc.stdout, proc.stderr
    if proc.returncode:
        raise AssertionError(
            f""process failed with code {proc.returncode:d}: stderr:\n{err}\n""
        )
```",https://github.com/NVIDIA/numba-cuda/pull/762#discussion_r2742143212,"This review comment seems legitimate, but I imagine we'd be better off catching the Python exception instead of removing the `check=True`?
---
Seems legit.",2026-01-29T15:12:21+00:00,logic,medium,"Greptile correctly identified that `check=True` makes the subsequent `if proc.returncode:` check unreachable dead code, since `subprocess.run` will raise `CalledProcessError` before reaching that line. This changes the error behavior from raising `AssertionError` to raising `CalledProcessError`, which could break test expectations. The developer confirmed this was legitimate.",2026-01-29T16:28:21.841987+00:00
onyx-dot-app/onyx,7977,fix: input masking,https://github.com/onyx-dot-app/onyx/pull/7977,"<details>
<summary>Additional Comments (1)</summary>

**`web/src/app/admin/embeddings/modals/ChangeCredentialsModal.tsx`**
Missing `data-ph-no-capture` attribute. Since PostHog's `maskAllInputs` is now set to `false` (in `providers.tsx:21`), this API key input needs the explicit `data-ph-no-capture` attribute to prevent session recordings from capturing sensitive credentials.

```suggestion
                    <input
                      type=""password""
                      data-ph-no-capture
                      className=""border border-border rounded w-full py-2 px-3 bg-background-emphasis""
                      value={apiKey}
                      onChange={(e: any) => setApiKey(e.target.value)}
                      placeholder=""Paste your API key here""
                    />
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/admin/embeddings/modals/ChangeCredentialsModal.tsx
Line: 206:212

Comment:
Missing `data-ph-no-capture` attribute. Since PostHog's `maskAllInputs` is now set to `false` (in `providers.tsx:21`), this API key input needs the explicit `data-ph-no-capture` attribute to prevent session recordings from capturing sensitive credentials.

```suggestion
                    <input
                      type=""password""
                      data-ph-no-capture
                      className=""border border-border rounded w-full py-2 px-3 bg-background-emphasis""
                      value={apiKey}
                      onChange={(e: any) => setApiKey(e.target.value)}
                      placeholder=""Paste your API key here""
                    />
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/onyx-dot-app/onyx/pull/7977#issuecomment-3819293947,,2026-01-29T17:57:22+00:00,security,critical,"This is a legitimate security vulnerability catch. The PR disabled global input masking (maskAllInputs: false) for PostHog session recordings, but failed to add the data-ph-no-capture attribute to an API key input field. This means sensitive credentials would be captured in session recordings and sent to PostHog, exposing them to anyone with access to those recordings. This is a real data exposure issue that directly contradicts the PR's intent to fix input masking.",2026-01-29T18:29:27.407222+00:00
openreplay/openreplay,4265,refactor(chalice): changed displayName&description handling refactor(DB): changed displayName&description handling,https://github.com/openreplay/openreplay/pull/4265,"Missing data migration - existing `display_name`, `description`, and `status` values should be migrated to the new `_customized` tables before dropping columns

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/scripts/schema/db/init_dbs/clickhouse/1.25.0/1.25.0.sql
Line: 148:158

Comment:
Missing data migration - existing `display_name`, `description`, and `status` values should be migrated to the new `_customized` tables before dropping columns

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4265#discussion_r2742737361,,2026-01-29T17:25:46+00:00,data_integrity,critical,"This catch identifies a critical data loss issue - the migration script drops columns containing existing user data (display_name, description, status) before migrating that data to the new _customized tables. Running this migration in production would permanently destroy user customizations. This is a guaranteed data loss bug that would affect all existing users.",2026-01-29T18:29:27.407387+00:00
gptme/gptme-contrib,225,feat(gptodo): add effective command to show computed task state,https://github.com/gptme/gptme-contrib/pull/225,"`blocked` state missing from `STATE_STYLES` dict

The virtual `blocked` state returned by `compute_effective_state()` isn't defined in the `STATE_STYLES` dictionary (utils.py:157-181), so it falls back to `white` styling. Consider adding an entry like:

```
""blocked"": (""red"", ""blocked""),
```

to utils.py STATE_STYLES for consistent styling of blocked tasks.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/cli.py
Line: 263:263

Comment:
`blocked` state missing from `STATE_STYLES` dict

The virtual `blocked` state returned by `compute_effective_state()` isn't defined in the `STATE_STYLES` dictionary (utils.py:157-181), so it falls back to `white` styling. Consider adding an entry like:

```
""blocked"": (""red"", ""blocked""),
```

to utils.py STATE_STYLES for consistent styling of blocked tasks.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/225#discussion_r2742594440,"✅ Fixed in commit 93c0ba0 - added `blocked` virtual state with red styling to STATE_STYLES.

Good catch! The comment distinguishes it from stored frontmatter states.",2026-01-29T16:50:36+00:00,logic,low,"Greptile correctly identified that the new 'blocked' virtual state returned by compute_effective_state() was missing from STATE_STYLES, causing it to fall back to default white styling instead of appropriate red styling. The developer confirmed this with '✅ Fixed' and 'Good catch!'. While not a critical bug, it's a valid issue that would affect user experience by making blocked tasks visually indistinct.",2026-01-29T18:29:27.407441+00:00
greptileai/react-mirror,335,[Flight] Forward early debug info for lazy chunks,https://github.com/greptileai/react-mirror/pull/335,"Field name mismatch - initialized as `forwardedDebugInfos` but type definition at line 617 declares `partialDebugInfoProgress`. This will cause the WeakMap to never be initialized, breaking the progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react-server/src/ReactFlightServer.js
Line: 742:742

Comment:
Field name mismatch - initialized as `forwardedDebugInfos` but type definition at line 617 declares `partialDebugInfoProgress`. This will cause the WeakMap to never be initialized, breaking the progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/335#discussion_r2744122368,,2026-01-30T00:39:33+00:00,type_error,high,Greptile correctly identified a field name mismatch where the type definition declares `partialDebugInfoProgress` but the initialization uses `forwardedDebugInfos`. This is a clear bug that would cause the progressive debug info tracking feature to completely fail since the WeakMap would never be properly initialized under the expected field name. The code would attempt to access `partialDebugInfoProgress` but find null/undefined instead of the WeakMap.,2026-01-30T00:50:50.252520+00:00
gptme/gptme-contrib,230,feat(plugins): add gptme-retrieval plugin for automatic context retrieval,https://github.com/gptme/gptme-contrib/pull/230,"Shell injection vulnerability - user-controlled `backend` value passed directly to shell. If backend comes from untrusted config, this allows arbitrary command execution.

```suggestion
    cmd = [backend, query, ""--mode"", mode, ""-n"", str(max_results)]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: plugins/gptme-retrieval/src/gptme_retrieval/__init__.py
Line: 201:201

Comment:
Shell injection vulnerability - user-controlled `backend` value passed directly to shell. If backend comes from untrusted config, this allows arbitrary command execution.

```suggestion
    cmd = [backend, query, ""--mode"", mode, ""-n"", str(max_results)]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/230#discussion_r2744032789,,2026-01-30T00:02:55+00:00,security,critical,"This is a legitimate shell injection vulnerability where user-controlled config values (backend) are passed to subprocess execution. The suggestion to use a list format instead of shell=True prevents command injection attacks. While the config comes from toml files (somewhat trusted), allowing arbitrary custom commands makes this a real security concern.",2026-01-30T00:50:50.252618+00:00
gptme/gptme-contrib,229,fix(gptodo): ensure tmux sessions inherit environment variables,https://github.com/gptme/gptme-contrib/pull/229,"`workspace` is not shell-escaped, creating command injection vulnerability if workspace path contains special characters

```suggestion
        full_cmd = f""bash -l -c {shlex.quote(f'cd {shlex.quote(str(workspace))} && {env_setup}{shell_cmd}')}""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/subagent.py
Line: 173:173

Comment:
`workspace` is not shell-escaped, creating command injection vulnerability if workspace path contains special characters

```suggestion
        full_cmd = f""bash -l -c {shlex.quote(f'cd {shlex.quote(str(workspace))} && {env_setup}{shell_cmd}')}""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/229#discussion_r2743888735,✅ Fixed in commit ee90d31 - added `safe_workspace = shlex.quote(str(workspace))` before using it in the command.,2026-01-29T23:01:24+00:00,security,high,"Greptile correctly identified a command injection vulnerability where the workspace path was not shell-escaped when used inside the nested shell command. The developer confirmed and fixed the issue. While workspace paths containing special characters aren't common, this is a real security bug that could cause incorrect behavior or exploitation in production environments.",2026-01-30T00:50:50.252678+00:00
posthog/posthog,46345,feat(duckling): auto-create events and persons tables if missing,https://github.com/PostHog/posthog/pull/46345,"SQL injection risk: f-string interpolation in SQL query allows potential injection if parameters aren't controlled. While `catalog_alias`, `schema`, and `table` are currently hardcoded as ""duckling""/""main""/""events""/""persons"", the function signature accepts arbitrary strings.

```suggestion
def table_exists(
    conn: duckdb.DuckDBPyConnection,
    catalog_alias: str,
    schema: str,
    table: str,
) -> bool:
    """"""Check if a table exists in the DuckLake catalog.""""""
    try:
        # Use parameterized query to prevent SQL injection
        # DuckDB doesn't support parameterized identifiers in DESCRIBE,
        # so validate inputs are safe identifiers
        for identifier in [catalog_alias, schema, table]:
            if not identifier.replace(""_"", """").isalnum():
                raise ValueError(f""Invalid identifier: {identifier}"")
        conn.execute(f""DESCRIBE {catalog_alias}.{schema}.{table}"")
        return True
    except duckdb.CatalogException:
        return False
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/dags/events_backfill_to_duckling.py
Line: 295:306

Comment:
SQL injection risk: f-string interpolation in SQL query allows potential injection if parameters aren't controlled. While `catalog_alias`, `schema`, and `table` are currently hardcoded as ""duckling""/""main""/""events""/""persons"", the function signature accepts arbitrary strings.

```suggestion
def table_exists(
    conn: duckdb.DuckDBPyConnection,
    catalog_alias: str,
    schema: str,
    table: str,
) -> bool:
    """"""Check if a table exists in the DuckLake catalog.""""""
    try:
        # Use parameterized query to prevent SQL injection
        # DuckDB doesn't support parameterized identifiers in DESCRIBE,
        # so validate inputs are safe identifiers
        for identifier in [catalog_alias, schema, table]:
            if not identifier.replace(""_"", """").isalnum():
                raise ValueError(f""Invalid identifier: {identifier}"")
        conn.execute(f""DESCRIBE {catalog_alias}.{schema}.{table}"")
        return True
    except duckdb.CatalogException:
        return False
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46345#discussion_r2744290999,"Already addressed in commit 726837e7. Added `_validate_identifier()` function that validates all identifiers are alphanumeric/underscore only before interpolation, with comprehensive tests for SQL injection attempts.",2026-01-30T01:57:49+00:00,security,medium,"Greptile correctly identified a SQL injection vulnerability where f-string interpolation was used with function parameters that could accept arbitrary strings. The developer confirmed this was a valid catch and fixed it by adding identifier validation. While the current usage was hardcoded safe values, the function signature allowed arbitrary input, making this a real security concern worth fixing.",2026-01-30T03:10:50.718850+00:00
greptileai/react-mirror,335,[Flight] Forward early debug info for lazy chunks,https://github.com/greptileai/react-mirror/pull/335,"Field name mismatch - should be `partialDebugInfoProgress` not `forwardedDebugInfos` (type at line 617 declares `partialDebugInfoProgress`). This breaks progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react-server/src/ReactFlightServer.js
Line: 742:742

Comment:
Field name mismatch - should be `partialDebugInfoProgress` not `forwardedDebugInfos` (type at line 617 declares `partialDebugInfoProgress`). This breaks progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/335#discussion_r2744270097,,2026-01-30T01:47:00+00:00,type_error,high,This is a clear field name mismatch bug where the type declaration uses `partialDebugInfoProgress` but the initialization uses `forwardedDebugInfos`. This would cause runtime errors or broken functionality when the code tries to access the correctly-named field that was never initialized. The type system would not catch this at runtime since JavaScript doesn't enforce types.,2026-01-30T03:10:50.719005+00:00
posthog/posthog,46349,chore(ducklake): simplify schema and table naming,https://github.com/PostHog/posthog/pull/46349,"removing `_{schema.id.hex[:8]}` from table names will cause naming conflicts when a team has multiple `ExternalDataSchema` objects with the same `source_type` and `normalized_name`. The codebase explicitly acknowledges this possibility (see `external_data_schema.py:368`), and the schema's unique ID is used in `folder_path()` to avoid conflicts. Without the schema ID suffix, `CREATE OR REPLACE TABLE` will overwrite data from different schemas.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/ducklake/ducklake_copy_data_imports_workflow.py
Line: 197:199

Comment:
removing `_{schema.id.hex[:8]}` from table names will cause naming conflicts when a team has multiple `ExternalDataSchema` objects with the same `source_type` and `normalized_name`. The codebase explicitly acknowledges this possibility (see `external_data_schema.py:368`), and the schema's unique ID is used in `folder_path()` to avoid conflicts. Without the schema ID suffix, `CREATE OR REPLACE TABLE` will overwrite data from different schemas.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46349#discussion_r2744668903,,2026-01-30T05:28:21+00:00,data_integrity,high,"This is a legitimate data integrity bug. Removing the schema ID suffix from table names will cause data overwrites when a team has multiple ExternalDataSchema objects with the same source_type and normalized_name - a scenario the codebase explicitly acknowledges is possible. The reviewer correctly identified that the unique ID is used elsewhere (folder_path) to prevent this exact conflict, and without it, CREATE OR REPLACE TABLE will silently overwrite different schemas' data.",2026-01-30T06:29:30.424979+00:00
helixdb/helix-db,843,Add HNSWDiagnostics endpoint for graph health checks,https://github.com/HelixDB/helix-db/pull/843,"`get_all_vector_ids` method doesn't exist on `VectorCore`

`VectorCore` doesn't have a `get_all_vector_ids` method. Need to iterate over `vector_properties_db` instead:

```suggestion
    let vector_ids: Vec<u128> = db
        .vectors
        .vector_properties_db
        .iter(&txn)
        .map_err(|e| GraphError::New(format!(""Failed to iterate vector IDs: {}"", e)))?
        .filter_map(|r| r.ok())
        .map(|(id, _)| id)
        .collect();
```",https://github.com/HelixDB/helix-db/pull/843#discussion_r2744671602,,2026-01-30T05:29:18+00:00,runtime,high,"Greptile correctly identified that the code calls a method `get_all_vector_ids` that doesn't exist on the `VectorCore` type. This would cause a compilation error, preventing the feature from working at all. The reviewer provided a concrete fix showing how to iterate over the underlying database instead.",2026-01-30T06:29:30.425041+00:00
green-coding-solutions/green-metrics-tool,1529,(fix): Hardware Info Root Sudoers entry was possible to hijack,https://github.com/green-coding-solutions/green-metrics-tool/pull/1529,"**CRITICAL: Sudoers entry doesn't match actual code invocations**

The sudoers entries now require direct path format:
- `/usr/bin/python3 /path/to/lib/hardware_info_root.py`

But the actual code still uses module format:
- `lib/scenario_runner.py:797`: `sudo /usr/bin/python3 -m lib.hardware_info_root`
- `lib/utils.py:216`: `sudo python3 -m lib.hardware_info_root --read-rapl-energy-filtering`

These won't match the sudoers entry and sudo will prompt for password or fail.

**Fix:** Either update `scenario_runner.py` and `utils.py` to use direct path invocation, OR keep the sudoers entry using `-m lib.hardware_info_root` format (which was the original vulnerable format that this PR aims to fix).

The secure solution is to update the code invocations to match the new sudoers format.",https://github.com/green-coding-solutions/green-metrics-tool/pull/1529#discussion_r2745104559,,2026-01-30T08:04:48+00:00,runtime,critical,"This catch identifies a critical mismatch where the sudoers file is being updated to use direct path format, but the actual code invocations in scenario_runner.py and utils.py still use the `-m lib.hardware_info_root` module format. This would cause sudo commands to fail or prompt for password in production, completely breaking the functionality. The comment correctly identifies both the problem and the solution path.",2026-01-30T08:23:21.146349+00:00
posthog/posthog,46351,feat(devex): nodejs composable capability-groups,https://github.com/PostHog/posthog/pull/46351,"Dependency creates circular issue with capability group mapping

`nodejs_realtime_cohorts` requires `nodejs_cdp_workflows`, but in `generator.py:227-234`, the capability mapping is flat - it doesn't handle transitive dependencies. When a user selects only `nodejs_realtime_cohorts`, the Python code will only add `realtime_cohorts` to `NODEJS_CAPABILITY_GROUPS`, but the TypeScript code in `capabilities.ts:28-30` only enables `cdpPrecalculatedFilters` and `cdpCohortMembership` without the CDP workflow capabilities that are required by the intent-map dependency.

Either:
1. Change the requires to `[event_ingestion]` instead of `[nodejs_cdp_workflows]`, OR
2. Update `generator.py` to resolve transitive dependencies when building the capability groups string

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: devenv/intent-map.yaml
Line: 104:104

Comment:
Dependency creates circular issue with capability group mapping

`nodejs_realtime_cohorts` requires `nodejs_cdp_workflows`, but in `generator.py:227-234`, the capability mapping is flat - it doesn't handle transitive dependencies. When a user selects only `nodejs_realtime_cohorts`, the Python code will only add `realtime_cohorts` to `NODEJS_CAPABILITY_GROUPS`, but the TypeScript code in `capabilities.ts:28-30` only enables `cdpPrecalculatedFilters` and `cdpCohortMembership` without the CDP workflow capabilities that are required by the intent-map dependency.

Either:
1. Change the requires to `[event_ingestion]` instead of `[nodejs_cdp_workflows]`, OR
2. Update `generator.py` to resolve transitive dependencies when building the capability groups string

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46351#discussion_r2745298413,"  Fixed - changed nodejs_realtime_cohorts to require event_ingestion instead of nodejs_cdp_workflows. The intent-map resolver handles transitive dependencies at resolution time, so the consumers are correctly independent.                                                ",2026-01-30T09:04:41+00:00,logic,medium,"This catch identified a real dependency resolution bug where nodejs_realtime_cohorts required nodejs_cdp_workflows, but the capability mapping was flat and wouldn't resolve transitive dependencies correctly. The developer confirmed and fixed it by changing the dependency to event_ingestion. This is a genuine architectural issue that could cause missing capabilities at runtime.",2026-01-30T10:22:36.136078+00:00
green-coding-solutions/green-metrics-tool,1529,(fix): Hardware Info Root Sudoers entry was possible to hijack,https://github.com/green-coding-solutions/green-metrics-tool/pull/1529,"`readlink -f` requires GNU coreutils on macOS (BSD readlink doesn't support `-f`). Script installs coreutils only if `stdbuf` is missing, but these commands will fail if coreutils wasn't installed.

```suggestion
    powermetrics_path=$(grealpath ""/usr/bin/powermetrics"" 2>/dev/null || readlink -f ""/usr/bin/powermetrics"" 2>/dev/null || realpath ""/usr/bin/powermetrics"")
    killall_path=$(grealpath ""/usr/bin/killall"" 2>/dev/null || readlink -f ""/usr/bin/killall"" 2>/dev/null || realpath ""/usr/bin/killall"")
```",https://github.com/green-coding-solutions/green-metrics-tool/pull/1529#discussion_r2745872648,,2026-01-30T11:46:09+00:00,runtime,high,"This is a valid catch that will cause the macOS install script to fail. The script uses `readlink -f` which is a GNU coreutils feature not available in BSD readlink on macOS by default. The script only conditionally installs coreutils if stdbuf is missing, so this command will fail for users who have stdbuf but not full coreutils. This is a real bug that will break installation for a significant portion of macOS users.",2026-01-30T12:26:54.956779+00:00
helixdb/helix-db,844,"fix(hql+core): fixing nested ID, math ops",https://github.com/HelixDB/helix-db/pull/844,"missing method implementations that are called by generated code in `computed_expr.rs`:
- `pow(&Value) -> Value`
- `abs() -> Value` 
- `sqrt() -> Value`
- `min(&Value) -> Value`
- `max(&Value) -> Value`
- `Rem` trait implementation for modulo

also need `to_f64()` to be public or use alternative conversion for pow/sqrt",https://github.com/HelixDB/helix-db/pull/844#discussion_r2745924528,,2026-01-30T11:58:15+00:00,runtime,high,"Comment 6 consolidates all the missing method issues (pow, abs, sqrt, min, max, Rem trait) that comments 0-5 identify individually. The generated code in computed_expr.rs calls methods that don't exist on the Value type, which will cause compilation failures when users try to use these math operations. This is a comprehensive catch that identifies a real bug where the code generator produces invalid Rust code.",2026-01-30T12:26:54.956894+00:00
gptme/gptme-contrib,233,"feat(gptodo): add checker pattern, dependency tree, and ready_for_review state",https://github.com/gptme/gptme-contrib/pull/233,"`details[""total""]` should exclude URL dependencies since they're skipped at line 144-146. Currently if URL deps exist, `len(resolved) + len(unresolved)` won't equal `total`, causing confusing output.

```suggestion
    details[""total""] = len(resolved) + len(unresolved)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/checker.py
Line: 157:157

Comment:
`details[""total""]` should exclude URL dependencies since they're skipped at line 144-146. Currently if URL deps exist, `len(resolved) + len(unresolved)` won't equal `total`, causing confusing output.

```suggestion
    details[""total""] = len(resolved) + len(unresolved)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/233#discussion_r2745847919,✅ Fixed in bf34c25 - now excluding URL deps from total count,2026-01-30T11:39:07+00:00,logic,medium,"This is a verified logic bug where URL dependencies are skipped during processing but the total count still includes them, causing the math to not add up (resolved + unresolved != total). The developer confirmed and fixed it. It's a clear data inconsistency issue that would produce confusing output for users.",2026-01-30T12:26:54.956946+00:00
posthog/posthog,46368,feat: add NodeJS Recording API to support encryption of recording data,https://github.com/PostHog/posthog/pull/46368,"The `key` parameter accepts arbitrary S3 paths without validation. Consider validating the key format matches expected patterns (e.g., starts with expected prefix) to prevent potential path traversal or access to unintended S3 objects.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nodejs/src/recording-api/recording-api.ts
Line: 164:164

Comment:
The `key` parameter accepts arbitrary S3 paths without validation. Consider validating the key format matches expected patterns (e.g., starts with expected prefix) to prevent potential path traversal or access to unintended S3 objects.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46368#discussion_r2746164321,Fixed in latest commit,2026-01-30T13:02:17+00:00,security,medium,"Comment 2 identifies a path traversal vulnerability where arbitrary S3 paths could be accessed without validation. The developer confirmed it was a valid issue and fixed it. While Comments 0 and 1 were false positives (internal API), this S3 key validation issue is a legitimate security concern that was addressed.",2026-01-30T14:25:02.873463+00:00
openreplay/openreplay,4268,fix(chalice): fixed idp_group_prefixes default value,https://github.com/openreplay/openreplay/pull/4268,"Migration drops columns with existing data but doesn't migrate that data to the new `all_events_customized` table. Any existing `display_name`, `description`, or `status` values will be permanently lost.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/scripts/schema/db/init_dbs/clickhouse/1.25.0/1.25.0.sql
Line: 147:156

Comment:
Migration drops columns with existing data but doesn't migrate that data to the new `all_events_customized` table. Any existing `display_name`, `description`, or `status` values will be permanently lost.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4268#discussion_r2746479945,,2026-01-30T14:05:07+00:00,data_integrity,critical,"The migration script drops columns (display_name, description, status) containing existing user data before creating the new tables to hold that data, but includes no data migration step. This will cause permanent data loss in production for all existing customizations. This is a critical data integrity issue that would affect all users who have customized their events.",2026-01-30T14:25:02.873523+00:00
helixdb/helix-db,845,impr (hql+core): nested ids and math ops,https://github.com/HelixDB/helix-db/pull/845,"Division by zero not handled - `wrapping_div` panics on zero divisor

```suggestion
            // Same-type signed integer divisions
            (Value::I8(a), Value::I8(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I8(a.wrapping_div(b))
            }
            (Value::I16(a), Value::I16(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I16(a.wrapping_div(b))
            }
            (Value::I32(a), Value::I32(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I32(a.wrapping_div(b))
            }
            (Value::I64(a), Value::I64(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I64(a.wrapping_div(b))
            }
```",https://github.com/HelixDB/helix-db/pull/845#discussion_r2746255502,,2026-01-30T13:22:47+00:00,runtime,high,"This is a genuine bug catch - division by zero with wrapping_div will cause a panic in Rust, crashing the program. This affects normal usage whenever a user performs division with a zero divisor through the Value type's arithmetic operations. The comment correctly identifies the issue and provides a concrete fix.",2026-01-30T14:25:02.873554+00:00
posthog/posthog,46391,feat(customers): Add revenue data to person profiles,https://github.com/PostHog/posthog/pull/46391,"the `loadRevenueData` action is never called, so revenue data will never load. Add an `afterMount` that calls `actions.loadRevenueData()` (similar to how `groupLogic` calls `actions.loadGroupRevenueAnalyticsData()` in its `afterMount` at groupLogic.ts:316)

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/persons/personLogic.tsx
Line: 137:150

Comment:
the `loadRevenueData` action is never called, so revenue data will never load. Add an `afterMount` that calls `actions.loadRevenueData()` (similar to how `groupLogic` calls `actions.loadGroupRevenueAnalyticsData()` in its `afterMount` at groupLogic.ts:316)

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46391#discussion_r2746560402,,2026-01-30T14:24:44+00:00,logic,high,"This is a clear functional bug - the PR adds a `loadRevenueData` loader to fetch revenue data for person profiles, but never actually calls it anywhere. The revenue data feature simply won't work as implemented. The reviewer correctly identifies the pattern used elsewhere (groupLogic's afterMount) that should be followed here.",2026-01-30T16:25:13.000671+00:00
raycast/extensions,25000,Add okta-app-manager extension,https://github.com/raycast/extensions/pull/25000,`confirm()` is not available in Raycast. Use `Action` with confirmation or `Alert` API instead.,https://github.com/raycast/extensions/pull/25000#discussion_r2746966008,,2026-01-30T16:08:56+00:00,runtime,high,"The comment correctly identifies that `confirm()` is a browser API that doesn't exist in Raycast's environment. This will cause a runtime error when users try to delete an environment, making it a real bug that affects normal usage. The fix is straightforward - use Raycast's Alert API instead.",2026-01-30T16:25:13.000772+00:00
infisical/infisical,5325,feat(ui): add app connection id permission condition to secret syncs,https://github.com/Infisical/infisical/pull/5325,"<details>
<summary>Additional Comments (1)</summary>

**`backend/src/services/secret-sync/secret-sync-service.ts`**
**Critical Security Issue: Permission bypass when updating `connectionId`**

The permission checks use the OLD `connectionId` from the existing sync, but when `params.connectionId` is provided (line 502-511), the update allows changing to a NEW connection without verifying permission against that new `connectionId`.

**Attack scenario:**
1. User has Read/Edit on syncs with `connectionId=conn-1`  
2. User does NOT have access to `connectionId=conn-2`
3. User calls update with `params.connectionId=conn-2`
4. Permission check at line 436-443 passes (checks OLD `conn-1`)
5. Line 502-511 only validates the connection exists
6. User successfully updates sync to use `conn-2`, bypassing permission restrictions

**Fix required:**
When `params.connectionId` is provided, add a permission check against the NEW connection:

```typescript
if (params.connectionId && params.connectionId !== connectionId) {
  // Check permission against NEW connectionId
  ForbiddenError.from(permission).throwUnlessCan(
    ProjectPermissionSecretSyncActions.Edit,
    secretSync.environment?.slug && secretSync.folder?.path
      ? subject(ProjectPermissionSub.SecretSyncs, {
          environment: secretSync.environment.slug,
          secretPath: secretSync.folder.path,
          connectionId: params.connectionId
        })
      : ProjectPermissionSub.SecretSyncs
  );
}
```

Note: Secret rotation correctly prevents this by excluding `connectionId` from `TUpdateSecretRotationV2DTO`.
</details>",https://github.com/Infisical/infisical/pull/5325#issuecomment-3824534353,,2026-01-30T16:16:14+00:00,security,critical,"This identifies a permission bypass vulnerability where a user can change a sync's connectionId to one they don't have access to. The permission check only validates against the OLD connectionId, allowing users to escalate privileges by updating to a connection they shouldn't have access to. This is a clear security flaw in the authorization logic that could allow unauthorized access to connections.",2026-01-30T16:25:13.000803+00:00
onyx-dot-app/onyx,8013,"feat(opensearch): Migration 0 - Introduce db tables, alembic migration, db model utils",https://github.com/onyx-dot-app/onyx/pull/8013,"enum values must be lowercase to match Python enum definitions

In `backend/onyx/db/enums.py`, `OpenSearchTenantMigrationStatus` uses lowercase values (`""pending""`, `""completed""`), but this migration uses uppercase (`""PENDING""`, `""COMPLETED""`). Same issue exists on lines 40-46 and 72-78.

```suggestion
            sa.Enum(
                ""pending"",
                ""completed"",
                name=""opensearchtenantmigrationstatus"",
                native_enum=False,
            ),
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/alembic/versions/d7d881a02917_add_opensearch_migration_tables.py
Line: 26:30

Comment:
enum values must be lowercase to match Python enum definitions

In `backend/onyx/db/enums.py`, `OpenSearchTenantMigrationStatus` uses lowercase values (`""pending""`, `""completed""`), but this migration uses uppercase (`""PENDING""`, `""COMPLETED""`). Same issue exists on lines 40-46 and 72-78.

```suggestion
            sa.Enum(
                ""pending"",
                ""completed"",
                name=""opensearchtenantmigrationstatus"",
                native_enum=False,
            ),
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/8013#discussion_r2747186530,,2026-01-30T17:04:41+00:00,data_integrity,high,"This catch identifies a real mismatch between the Python enum definitions (lowercase values like 'pending', 'completed') and the database migration (uppercase values like 'PENDING', 'COMPLETED'). This would cause database operations to fail when the ORM tries to map between Python enums and database values. The comment correctly identifies the issue and provides the fix with cross-reference to the actual enum definitions.",2026-01-30T18:27:08.877934+00:00
posthog/posthog,46426,chore(feature flags): client-side search/filter,https://github.com/PostHog/posthog/pull/46426,"pagination disabled whenever client-side filtering reduces results below total count, but pagination should still work on the filtered subset. users can't navigate through filtered results if there are more than 100 matches

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/feature-flags/FeatureFlags.tsx
Line: 495:495

Comment:
pagination disabled whenever client-side filtering reduces results below total count, but pagination should still work on the filtered subset. users can't navigate through filtered results if there are more than 100 matches

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46426#discussion_r2747522938,"Agreed. Additionally, this also prevents loading more pages from the server - if the user searches for ""test"" and only 15 cached flags match, they can't fetch page 2 which might have more matches.",2026-01-30T18:45:30+00:00,logic,medium,"This comment correctly identifies that pagination is disabled when client-side filtering reduces results, preventing users from navigating through filtered results or fetching additional pages from the server. The developer confirmed this is a real issue affecting user experience. This is the most impactful catch as it breaks core functionality of browsing filtered feature flags.",2026-01-30T20:20:29.552473+00:00
getarcaneapp/arcane,1602,refactor: use docker dhi images for runtimes and builds,https://github.com/getarcaneapp/arcane/pull/1602,"`apk` is Alpine's package manager, but `dhi.io/static:20250419-glibc` appears to be a glibc-based image (not Alpine/musl-based). This will fail at build time with ""apk: command not found"" or similar error.

Check whether `dhi.io/static:20250419-glibc` is Alpine-based. If not Alpine, either:
- Use a different base image that's Alpine-based, or
- Remove the `apk add` line if `curl`, `ca-certificates`, and `tzdata` are already included in the base image (which they likely are, given the diff shows they were removed from the install line)

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: docker/next-builds/Dockerfile-static
Line: 7:7

Comment:
`apk` is Alpine's package manager, but `dhi.io/static:20250419-glibc` appears to be a glibc-based image (not Alpine/musl-based). This will fail at build time with ""apk: command not found"" or similar error.

Check whether `dhi.io/static:20250419-glibc` is Alpine-based. If not Alpine, either:
- Use a different base image that's Alpine-based, or
- Remove the `apk add` line if `curl`, `ca-certificates`, and `tzdata` are already included in the base image (which they likely are, given the diff shows they were removed from the install line)

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/getarcaneapp/arcane/pull/1602#discussion_r2747512213,,2026-01-30T18:41:32+00:00,runtime,high,"This is a valid catch - the Dockerfile switches from Alpine to a glibc-based image but still uses `apk` (Alpine's package manager), which will cause the Docker build to fail immediately. This is a guaranteed build failure that would block deployment. Comment 0 and 1 are duplicates for different files with the same issue.",2026-01-30T20:20:29.552577+00:00
openucx/ucc,1260,TL/NCCL: add user buffer registration via memmap,https://github.com/openucx/ucc/pull/1260,"buffer size calculation is incorrect for variable-size collectives (ALLTOALLV, ALLGATHERV, GATHERV, SCATTERV). For these collectives, `coll_args->args.src` uses the `info_v` union member with `counts*` array, not `info.count` scalar. Accessing `.info.count` reads the `counts` pointer as an integer, resulting in incorrect buffer size. Need to check `coll_args->args.coll_type` and use appropriate buffer size calculation for each collective type.

```suggestion
            // TODO: Need to calculate correct buffer size based on coll_type
            // For variable-size collectives (ALLTOALLV, ALLGATHERV, etc), 
            // must sum counts array instead of using single count value
            status = ucc_tl_nccl_lazy_register_memh(
                coll_args->args.src.info.buffer,
                coll_args->args.src.info.count *
                    ucc_dt_size(coll_args->args.src.info.datatype),
```",https://github.com/openucx/ucc/pull/1260#discussion_r2747554746,,2026-01-30T18:55:12+00:00,logic,high,"Comment 0 identifies a real bug where variable-size collectives (ALLTOALLV, ALLGATHERV, etc.) use a union with `counts*` array in the `info_v` member, but the code incorrectly accesses `.info.count` which would interpret the pointer as an integer. This would cause incorrect buffer size calculations for these collective operations, leading to wrong memory registration bounds.",2026-01-30T20:20:29.552655+00:00
nvidia-nemo/curator,1446,cp: `ci: Optimize docker layer and uv with no cache (1444)` into `r1.1.0`,https://github.com/NVIDIA-NeMo/Curator/pull/1446,"`nemo_curator/__init__.py` imports `cosmos_xenna.ray_utils.cluster` on line 34, which won't be available until after `uv sync` runs. This will cause the build to fail when uv tries to read the version from `nemo_curator.package_info.__version__`.

```suggestion
COPY nemo_curator/package_info.py /opt/Curator/nemo_curator/
```

Remove `__init__.py` from this COPY command and only copy `package_info.py` which is actually needed for version resolution.",https://github.com/NVIDIA-NeMo/Curator/pull/1446#discussion_r2748082814,,2026-01-30T21:46:34+00:00,runtime,high,Greptile correctly identified that copying `__init__.py` before running `uv sync` will cause a build failure because `__init__.py` imports `cosmos_xenna.ray_utils.cluster` which isn't installed yet. This is a concrete build-breaking bug that would affect anyone trying to build this Docker image. The suggestion to only copy `package_info.py` (which is what's actually needed for version resolution) is actionable and correct.,2026-01-30T22:17:33.961712+00:00
requestnetwork/requestnetwork,1693,build(deps-dev): Bump eslint from 7.22.0 to 9.26.0,https://github.com/RequestNetwork/requestNetwork/pull/1693,"ESLint 9 introduces breaking changes: `@typescript-eslint/eslint-plugin` v4.18.0 and `@typescript-eslint/parser` v4.18.0 are incompatible with ESLint 9. These TypeScript ESLint packages require upgrade to v8.x or v9.x to work with ESLint 9.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: package.json
Line: 42:43

Comment:
ESLint 9 introduces breaking changes: `@typescript-eslint/eslint-plugin` v4.18.0 and `@typescript-eslint/parser` v4.18.0 are incompatible with ESLint 9. These TypeScript ESLint packages require upgrade to v8.x or v9.x to work with ESLint 9.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RequestNetwork/requestNetwork/pull/1693#discussion_r2747918958,,2026-01-30T20:40:50+00:00,runtime,high,This catch correctly identifies that ESLint 9 has breaking changes that make it incompatible with the TypeScript ESLint packages at version 4.18.0. This would cause the build/lint process to fail in production since the packages are fundamentally incompatible. This is a significant catch for a dependency bump PR that could break CI/CD pipelines.,2026-01-30T22:17:33.961775+00:00
alpaca-network/gatewayz-frontend,940,feat: integrate /models/unique endpoint with many-to-many providers,https://github.com/Alpaca-Network/gatewayz-frontend/pull/940,"<details>
<summary>Additional Comments (4)</summary>

**`src/app/models/models-client.tsx`**
`GatewayPricing` type is used but not defined. The local interface was removed when switching to `UniqueModel`, but `MobileProviderSubRow` component still references it.

```suggestion
  pricing: { prompt: string; completion: string };
```

Either add the type definition back or use the inline type.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/app/models/models-client.tsx
Line: 240:240

Comment:
`GatewayPricing` type is used but not defined. The local interface was removed when switching to `UniqueModel`, but `MobileProviderSubRow` component still references it.

```suggestion
  pricing: { prompt: string; completion: string };
```

Either add the type definition back or use the inline type.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`src/app/models/models-client.tsx`**
`gateway_pricing` property doesn't exist on `UniqueModel` type. The new endpoint uses `providers` array instead. This will cause TypeScript errors.

The code should access provider pricing through `model.providers` array instead:

```
const gatewayPricing: Record<string, { prompt: string; completion: string }> = {};
model.providers.forEach(p => {
  gatewayPricing[p.slug] = p.pricing;
});
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/app/models/models-client.tsx
Line: 284:284

Comment:
`gateway_pricing` property doesn't exist on `UniqueModel` type. The new endpoint uses `providers` array instead. This will cause TypeScript errors.

The code should access provider pricing through `model.providers` array instead:

```
const gatewayPricing: Record<string, { prompt: string; completion: string }> = {};
model.providers.forEach(p => {
  gatewayPricing[p.slug] = p.pricing;
});
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`src/app/models/models-client.tsx`**
`MobileModelRow` component uses old `Model` properties that don't exist on `UniqueModel`:
- `model.pricing` (line 278) - doesn't exist, should use `providers[0].pricing`
- `model.provider_slug` (line 281) - doesn't exist, should use `providers[0].slug`
- `model.gateway_pricing` (line 284) - doesn't exist, should build from `providers` array

This component needs to be updated to use the `UniqueModel` structure like the desktop `ExpandableModelRow` component does (lines 398-406).

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/app/models/models-client.tsx
Line: 278:286

Comment:
`MobileModelRow` component uses old `Model` properties that don't exist on `UniqueModel`:
- `model.pricing` (line 278) - doesn't exist, should use `providers[0].pricing`
- `model.provider_slug` (line 281) - doesn't exist, should use `providers[0].slug`
- `model.gateway_pricing` (line 284) - doesn't exist, should build from `providers` array

This component needs to be updated to use the `UniqueModel` structure like the desktop `ExpandableModelRow` component does (lines 398-406).

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`src/app/models/models-client.tsx`**
`ModelTableRow` component uses old properties that don't exist on `UniqueModel`:
- `model.pricing` (lines 81, 84-85) - should use cheapest provider's pricing
- `model.provider_slug` (lines 86, 95) - should use `providers[0].slug`

Similar to `ExpandableModelRow`, this should be updated to:
```
const cheapestProvider = model.providers.find(p => p.slug === model.cheapest_provider) || model.providers[0];
const hasPricing = cheapestProvider && cheapestProvider.pricing;
const inputCost = hasPricing ? formatPricingForDisplay(cheapestProvider.pricing.prompt, cheapestProvider.slug) : null;
const modelUrl = getModelUrl(model.id, model.providers[0]?.slug || 'unknown');
const providerDisplay = model.providers[0]?.name || 'Unknown';
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/app/models/models-client.tsx
Line: 81:86

Comment:
`ModelTableRow` component uses old properties that don't exist on `UniqueModel`:
- `model.pricing` (lines 81, 84-85) - should use cheapest provider's pricing
- `model.provider_slug` (lines 86, 95) - should use `providers[0].slug`

Similar to `ExpandableModelRow`, this should be updated to:
```
const cheapestProvider = model.providers.find(p => p.slug === model.cheapest_provider) || model.providers[0];
const hasPricing = cheapestProvider && cheapestProvider.pricing;
const inputCost = hasPricing ? formatPricingForDisplay(cheapestProvider.pricing.prompt, cheapestProvider.slug) : null;
const modelUrl = getModelUrl(model.id, model.providers[0]?.slug || 'unknown');
const providerDisplay = model.providers[0]?.name || 'Unknown';
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/940#issuecomment-3827244325,,2026-01-31T02:41:59+00:00,type_error,high,"Comment 1 (the consolidated comments) identifies multiple instances where the code accesses properties that don't exist on the new `UniqueModel` type - `model.pricing`, `model.provider_slug`, and `model.gateway_pricing` are all accessed but don't exist after the type change. This will cause TypeScript compilation errors and runtime failures if somehow bypassed. The issue affects multiple components (ModelTableRow, MobileModelRow) that weren't properly updated during the API migration.",2026-01-31T03:07:20.308070+00:00
davis7dotsh/better-context,149,fix(cli): wire chat + cursor auth,https://github.com/davis7dotsh/better-context/pull/149,"<details>
<summary>Additional Comments (1)</summary>

**`apps/server/src/providers/registry.ts`**
Cursor provider removed from registry but `apps/server/src/providers/cursor.ts` still exists and auth code in `apps/server/src/providers/auth.ts` still reads Cursor tokens. Users with `provider = ""cursor""` in their config will get ""Provider not supported"" errors at runtime.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/server/src/providers/registry.ts
Line: 39:71

Comment:
Cursor provider removed from registry but `apps/server/src/providers/cursor.ts` still exists and auth code in `apps/server/src/providers/auth.ts` still reads Cursor tokens. Users with `provider = ""cursor""` in their config will get ""Provider not supported"" errors at runtime.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/davis7dotsh/better-context/pull/149#issuecomment-3827584452,,2026-01-31T05:47:05+00:00,runtime,high,"This catch identifies a clear breaking change: the Cursor provider was removed from the registry while the auth code and provider file still exist. Users with 'provider = ""cursor""' in their config will get runtime errors. This is a real bug that will affect users in normal usage when they try to use Cursor as a provider.",2026-01-31T06:23:03.226573+00:00
huskit/website-agent,28,Feat/improve template flow,https://github.com/HuskIT/website-agent/pull/28,"path traversal check uses simple string matching which doesn't catch all encoding variants

The current `isPathSafe` implementation checks for `..` using `includes('...')` which misses encoded variants like:
- URL encoding: `%2e%2e/` or `..%2f`
- Unicode normalization issues
- Mixed separators: `..\` on Windows

Consider using `path.normalize()` and checking if the resolved path stays within the expected directory:

```suggestion
function isPathSafe(filePath: string): boolean {
  // Reject absolute paths
  if (filePath.startsWith('/') || filePath.startsWith('\\')) {
    return false;
  }

  // Reject Windows absolute paths
  if (/^[a-zA-Z]:/.test(filePath)) {
    return false;
  }

  // Normalize and check for path traversal
  const normalized = path.normalize(filePath);
  if (normalized.startsWith('..') || normalized.includes(`${path.sep}..`)) {
    return false;
  }

  return true;
}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: app/lib/.server/templates/zip-template-fetcher.ts
Line: 166:183

Comment:
path traversal check uses simple string matching which doesn't catch all encoding variants

The current `isPathSafe` implementation checks for `..` using `includes('...')` which misses encoded variants like:
- URL encoding: `%2e%2e/` or `..%2f`
- Unicode normalization issues
- Mixed separators: `..\` on Windows

Consider using `path.normalize()` and checking if the resolved path stays within the expected directory:

```suggestion
function isPathSafe(filePath: string): boolean {
  // Reject absolute paths
  if (filePath.startsWith('/') || filePath.startsWith('\\')) {
    return false;
  }

  // Reject Windows absolute paths
  if (/^[a-zA-Z]:/.test(filePath)) {
    return false;
  }

  // Normalize and check for path traversal
  const normalized = path.normalize(filePath);
  if (normalized.startsWith('..') || normalized.includes(`${path.sep}..`)) {
    return false;
  }

  return true;
}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/HuskIT/website-agent/pull/28#discussion_r2749161211,,2026-01-31T07:32:56+00:00,security,critical,"This is a security vulnerability in path traversal validation for zip file extraction. The simple string matching with `includes('..')` can be bypassed using URL encoding (%2e%2e/), unicode normalization, or mixed path separators. This could allow malicious zip files to write files outside the intended directory, which is a well-known zip slip vulnerability.",2026-01-31T08:17:01.571261+00:00
mrmps/smry,57,"Add Gravity micro/inline ads, AI copy menu, and buy-click device metadata",https://github.com/mrmps/SMRY/pull/57,"`setState` called during render (lines 73-74) will cause infinite re-renders. Use `useEffect` instead.

```suggestion
  const [prevSourcesLen, setPrevSourcesLen] = useState(sources.length);
  
  useEffect(() => {
    if (sources.length !== prevSourcesLen) {
      setPrevSourcesLen(sources.length);
      if (deselectedSources.size > 0) setDeselectedSources(new Set());
    }
  }, [sources.length, prevSourcesLen, deselectedSources.size]);
```",https://github.com/mrmps/SMRY/pull/57#discussion_r2749168539,,2026-01-31T07:46:34+00:00,runtime,high,"Calling setState during render (outside of useEffect) is a React anti-pattern that will cause infinite re-renders. The code sets `setPrevSourcesLen` and `setDeselectedSources` directly in the component body during render, which triggers more renders. This is a real bug that would cause performance issues or crashes in production.",2026-01-31T08:17:01.571420+00:00
twentyhq/twenty,17602,feat(imap): Enterprise-Grade IDLE + Chunked Sync,https://github.com/twentyhq/twenty/pull/17602,"[P0] `startIdle` awaits `client.idle()`, which typically only resolves when IDLE ends. This means `startIdle` likely never returns while the connection is healthy, and `activeClients.set(accountId, client)` / the ""IDLE started"" log may never execute. In practice this can block whatever code path calls `startIdle` and prevent tracking the active client.

If you want IDLE to run in the background, it usually needs to be started without awaiting completion (or managed in a long-lived task), and `activeClients` should be set before entering the long wait.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/modules/messaging/message-import-manager/drivers/imap/services/imap-idle.service.ts
Line: 15:18

Comment:
[P0] `startIdle` awaits `client.idle()`, which typically only resolves when IDLE ends. This means `startIdle` likely never returns while the connection is healthy, and `activeClients.set(accountId, client)` / the ""IDLE started"" log may never execute. In practice this can block whatever code path calls `startIdle` and prevent tracking the active client.

If you want IDLE to run in the background, it usually needs to be started without awaiting completion (or managed in a long-lived task), and `activeClients` should be set before entering the long wait.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17602#discussion_r2749733668,,2026-01-31T16:48:54+00:00,logic,high,"Comment 0 correctly identifies that `await client.idle()` is a blocking call that only resolves when IDLE ends, meaning the code after it (`activeClients.set()` and the log) will never execute while the connection is healthy. This is a fundamental bug that will cause the service to block indefinitely and fail to track active clients, breaking the entire IDLE functionality. The analysis is technically accurate for the imapflow library.",2026-01-31T18:17:48.114665+00:00
emerge-lab/adaptive_driving_agent,21,"Revert ""Sync upstream docs evals ci (#20)""",https://github.com/Emerge-Lab/Adaptive_Driving_Agent/pull/21,"matplotlib removed from dependencies but still imported in `pufferlib/ocean/benchmark/evaluator.py:9` and `pufferlib/ocean/benchmark/estimators.py`. This will cause ImportError when running WOSAC evaluations.

```suggestion
        ""heavyball<2.0.0"",
        ""neptune"",
        ""wandb"",
        ""matplotlib"",
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: setup.py
Line: 326:328

Comment:
matplotlib removed from dependencies but still imported in `pufferlib/ocean/benchmark/evaluator.py:9` and `pufferlib/ocean/benchmark/estimators.py`. This will cause ImportError when running WOSAC evaluations.

```suggestion
        ""heavyball<2.0.0"",
        ""neptune"",
        ""wandb"",
        ""matplotlib"",
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Emerge-Lab/Adaptive_Driving_Agent/pull/21#discussion_r2749748168,,2026-01-31T17:05:08+00:00,runtime,high,"Greptile correctly identified that removing matplotlib from dependencies while it's still imported in evaluator.py and estimators.py will cause an ImportError at runtime. This is a concrete, verifiable bug that will break WOSAC evaluations in production. The reviewer traced the dependency chain and provided specific file locations where the import exists.",2026-01-31T18:17:48.114765+00:00
nvidia/nvflare,3876,Feature Election contribution,https://github.com/NVIDIA/NVFlare/pull/3876,"**CRITICAL SYNTAX ERROR**: Lines 156-170 contain broken try-except blocks with duplicate code and incomplete control flow.

Line 162 ends with `if match:` but is never completed. Lines 163-164 start a duplicate `if site_name.startswith(""site-""):` block. This code will not execute correctly.

The correct structure should be:

```suggestion
        try:
            if site_name.startswith(""site-""):
                client_id = int(site_name.split(""-"")[1]) - 1
            else:
                match = re.search(r""\d+"", site_name)
                if match:
                    client_id = int(match.group()) - 1
                else:
                    client_id = 0
            
            # Validate range
            if not (0 <= client_id < self.num_clients):
                raise ValueError(f""Client ID {client_id} from '{site_name}' out of range [0, {self.num_clients-1}]"")
            
        except (ValueError, IndexError) as e:
            logger.error(f""Failed to parse client_id from '{site_name}': {e}. Defaulting to client_id=0"")
            client_id = 0
```",https://github.com/NVIDIA/NVFlare/pull/3876#discussion_r2749930177,"correct, solved",2026-01-31T19:46:48+00:00,runtime,high,"This comment identifies a critical syntax error with broken try-except blocks, duplicate code, and incomplete control flow that would cause the code to fail at runtime. The developer confirmed it was correct and fixed it. This is a clear, impactful bug that would break functionality in production.",2026-01-31T20:15:02.088225+00:00
sqlfluff/sqlfluff,7414,Support globs in COMMA_SEPARATED_PATH_KEYS,https://github.com/sqlfluff/sqlfluff/pull/7414,"<details>
<summary>Additional Comments (1)</summary>

**`src/sqlfluff/core/config/file.py`**
Breaking change: `_resolve_path()` now returns a list, but this code expects a string for single path keys (like `library_path`). This will cause type errors when code tries to use these paths with string operations (e.g., `os.path.join(library_path, ""__init__.py"")` in jinja.py:266).

The fix requires handling the list return value. For single path keys, take the first match or original value if no matches:

```suggestion
        # It it's a single path key, resolve it.
        elif key.lower().endswith(RESOLVE_PATH_SUFFIXES):
            assert isinstance(
                val, str
            ), f""Value for {key} in {log_filename} must be a string not {type(val)}.""
            matches = _resolve_path(filepath, val)
            config[key] = matches[0] if matches else val
```
</details>",https://github.com/sqlfluff/sqlfluff/pull/7414#issuecomment-3829312171,,2026-01-31T20:51:10+00:00,type_error,high,"Comment 1 correctly identifies a breaking change where `_resolve_path()` now returns a list instead of a string, but the code for single path keys (like `library_path`) still expects a string. This would cause runtime type errors when these paths are used in string operations like `os.path.join()`. The comment provides a concrete fix and references the actual downstream code that would break.",2026-01-31T22:14:27.502763+00:00
raycast/extensions,25035,Update moneybird extension: paginating contacts/lists (#24390) and repeat time entry (#23438),https://github.com/raycast/extensions/pull/25035,"infinite loop if `links.next` is not present - `nextUrl` is never set to null

```suggestion
  while (nextUrl) {
    const response = await fetchWithAuth(nextUrl.toString());
    const data = await response.json();
    if (Array.isArray(data)) results.push(...(data as T[]));

    const links = parseLinkHeader(response.headers.get(""Link""));
    if (links.next) {
      nextUrl = new URL(links.next, apiBaseUrl);
    } else {
      nextUrl = null as unknown as URL;
      break;
    }
  }
```",https://github.com/raycast/extensions/pull/25035#discussion_r2750134117,,2026-01-31T22:41:40+00:00,logic,high,"This is a valid catch of an infinite loop bug. The pagination logic continues while `nextUrl` exists, but if `links.next` is not present (i.e., we've reached the last page), `nextUrl` is never set to null, causing an infinite loop that would hang the application. This would affect users in normal usage when paginating through contacts/lists.",2026-02-01T00:58:32.807123+00:00
onyx-dot-app/onyx,8056,refactor(craft): make usage limit overrides feature flags instead of env vars,https://github.com/onyx-dot-app/onyx/pull/8056,"`_should_skip_rate_limiting` unconditionally uses `get_default_feature_flag_provider().feature_enabled(...)`, but unlike `is_onyx_craft_enabled` it doesn’t fall back when PostHog isn’t configured (NoOp provider). If NoOp returns False by default, this will make `not has_rate_limit` = True and effectively grant unlimited usage to *all* cloud users whenever PostHog is unavailable/misconfigured.

Consider mirroring the existing pattern:
- if provider is `NoOpFeatureFlagProvider`, default to applying limits (return False), or gate this behind an env/config that’s explicitly opt-in.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/api/rate_limit.py
Line: 33:36

Comment:
`_should_skip_rate_limiting` unconditionally uses `get_default_feature_flag_provider().feature_enabled(...)`, but unlike `is_onyx_craft_enabled` it doesn’t fall back when PostHog isn’t configured (NoOp provider). If NoOp returns False by default, this will make `not has_rate_limit` = True and effectively grant unlimited usage to *all* cloud users whenever PostHog is unavailable/misconfigured.

Consider mirroring the existing pattern:
- if provider is `NoOpFeatureFlagProvider`, default to applying limits (return False), or gate this behind an env/config that’s explicitly opt-in.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/8056#discussion_r2750167381,,2026-01-31T23:19:36+00:00,security,high,"This is a solid catch identifying that when PostHog is unavailable or misconfigured (NoOp provider returns False), the logic `not has_rate_limit` would evaluate to True, bypassing rate limiting for all cloud users. This is a security issue that could allow unlimited usage in production when the feature flag system fails. The analysis correctly identifies the missing fallback pattern that exists elsewhere in the codebase.",2026-02-01T00:58:32.807185+00:00
traceroot-ai/traceroot,397,Support org and project management #373,https://github.com/traceroot-ai/traceroot/pull/397,"[P0] API key endpoints no longer enforce any project existence or user authorization.

In this PR, `ProjectAccess` and the explicit `get_active_project_by_id` check were removed from `create_api_key_endpoint`, `list_api_keys_endpoint`, and `delete_api_key_endpoint`, meaning anyone who can reach the API can create/list/delete keys for any `project_id` (including IDs they don’t own), and `create` will also succeed for non-existent projects unless DB constraints reject it. If these routes are intended to be protected (even in MVP), they need to reintroduce an auth/access dependency or at least verify the project exists + caller has access.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: rest/routers/api_keys.py
Line: 24:29

Comment:
[P0] API key endpoints no longer enforce any project existence or user authorization.

In this PR, `ProjectAccess` and the explicit `get_active_project_by_id` check were removed from `create_api_key_endpoint`, `list_api_keys_endpoint`, and `delete_api_key_endpoint`, meaning anyone who can reach the API can create/list/delete keys for any `project_id` (including IDs they don’t own), and `create` will also succeed for non-existent projects unless DB constraints reject it. If these routes are intended to be protected (even in MVP), they need to reintroduce an auth/access dependency or at least verify the project exists + caller has access.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/397#discussion_r2750210955,,2026-01-31T23:51:03+00:00,security,critical,"This comment identifies a critical security vulnerability where authentication and authorization checks (ProjectAccess dependency and project existence verification) were completely removed from API key management endpoints. This allows any user who can reach the API to create, list, or delete API keys for any project_id, including projects they don't own. This is a clear auth bypass that would affect production security.",2026-02-01T00:58:32.807232+00:00
synnaxlabs/synnax,1914,SY-3659: Update `stylelint` to v17 and `*.ts` files,https://github.com/synnaxlabs/synnax/pull/1914,"[P0] `configs/stylelint/eslint.config.ts` appears to be importing the wrong package for a stylelint config (it imports `eslint-config-synnaxlabs`). If this file is intended to be the stylelint config (it was renamed from `console/stylelint.config.js` per diff), stylelint will either fail to load it or load an eslint config object, causing linting to break/misbehave.",https://github.com/synnaxlabs/synnax/pull/1914#discussion_r2750332548,,2026-02-01T01:12:40+00:00,logic,high,"This is a clear misconfiguration where a stylelint config file is importing eslint-config instead of stylelint-config. The file is in the stylelint directory and is meant to be a stylelint configuration, but it's importing the wrong package entirely. This will cause stylelint to either fail or behave incorrectly when linting CSS/style files.",2026-02-01T03:17:20.278043+00:00
getarcaneapp/arcane,1614,fix: container exec sessions not being cleaned up after websockets close,https://github.com/getarcaneapp/arcane/pull/1614,"using cancelled context for timeout won't work

When this cleanup is called from `watchExecContext` (line 489), the `ctx` will already be cancelled. `context.WithTimeout(ctx, 5*time.Second)` inherits the cancellation from the parent, so `cleanupCtx` will be immediately cancelled.

```suggestion
		cleanupCtx, cleanupCancel := context.WithTimeout(context.Background(), 5*time.Second)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/internal/api/ws_handler.go
Line: 478:479

Comment:
using cancelled context for timeout won't work

When this cleanup is called from `watchExecContext` (line 489), the `ctx` will already be cancelled. `context.WithTimeout(ctx, 5*time.Second)` inherits the cancellation from the parent, so `cleanupCtx` will be immediately cancelled.

```suggestion
		cleanupCtx, cleanupCancel := context.WithTimeout(context.Background(), 5*time.Second)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/getarcaneapp/arcane/pull/1614#discussion_r2750334846,,2026-02-01T01:14:07+00:00,logic,high,"Comment 0 identifies a real bug where using a cancelled context as the parent for a timeout context will cause the cleanup operation to fail immediately. When watchExecContext triggers cleanup after context cancellation, the cleanup will not be able to complete its 5-second timeout work because the parent context is already cancelled. This would cause exec sessions to not be properly cleaned up, which is exactly what this PR is trying to fix.",2026-02-01T03:17:20.278100+00:00
workos/workos-node,1457,Update Node SDK to ensure that all audit-log api's are in it ,https://github.com/workos/workos-node/pull/1457,"[P0] Self-dependency added (`@workos-inc/node` depends on itself)

`package.json` declares `""@workos-inc/node"": ""^8.0.0""` as a dependency of the `@workos-inc/node` package. This creates a circular/self dependency that can break installs (npm will refuse/behave unexpectedly) and will also bloat/complicate the published package graph. This looks accidental and should be removed.

Also appears in `package-lock.json` (self package entry present).

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: package.json
Line: 41:46

Comment:
[P0] Self-dependency added (`@workos-inc/node` depends on itself)

`package.json` declares `""@workos-inc/node"": ""^8.0.0""` as a dependency of the `@workos-inc/node` package. This creates a circular/self dependency that can break installs (npm will refuse/behave unexpectedly) and will also bloat/complicate the published package graph. This looks accidental and should be removed.

Also appears in `package-lock.json` (self package entry present).

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/workos/workos-node/pull/1457#discussion_r2750428257,,2026-02-01T02:41:59+00:00,runtime,high,This is a legitimate catch of a self-dependency bug where `@workos-inc/node` is accidentally listing itself as a dependency. This will cause npm install failures or unexpected behavior and would break the package for all users. The issue is clearly visible in the diff and is a real error that would affect production usage.,2026-02-01T03:17:20.278139+00:00
nvidia/warp,1206,Fix:Automatic workaround for CUDA compiler crash with local matrices …,https://github.com/NVIDIA/warp/pull/1206,"Pattern check happens before `kernel.adj.build(self)` on line 2099, but the detection flags are only populated during the build process, so this check will always evaluate to `False`

Move this check to after line 2099:
```python
kernel.adj.build(self)

# Check for known compiler bugs after building (Issue #1200)
if warp.config.auto_detect_cuda_compiler_bugs:
    if kernel.adj.detect_issue_1200_pattern() and self.options.get(""optimization_level"", 3) == 3:
        # Warning and fix logic here
        ...
```

However, this creates a new problem: by the time we detect the pattern, the kernel has already been built with -O3. The architecture needs redesign to either:
1. Build the kernel, check the pattern, then rebuild if needed (inefficient)
2. Do a pre-scan pass before building to detect patterns (complex)
3. Check the pattern after build and mark kernel for rebuild on next use (better)",https://github.com/NVIDIA/warp/pull/1206#discussion_r2750574738,,2026-02-01T05:37:25+00:00,logic,high,"Comment 2 identifies a critical ordering bug where the pattern detection check happens BEFORE the build() call that populates the detection flags. This means the workaround for the CUDA compiler bug will never activate, completely defeating the purpose of the entire PR. The comment also thoughtfully outlines the architectural challenge this creates.",2026-02-01T06:29:20.005906+00:00
traceroot-ai/traceroot,401,[AgentOps] Backend support for org and project management,https://github.com/traceroot-ai/traceroot/pull/401,"[P1] Org-scoping bug in delete: the handler authorizes against `orgId`, but deletes by `where: { id: invitationId }` without verifying the invitation belongs to that org. An OWNER/ADMIN in Org A could delete an invitation in Org B if they know/guess its ID.

Consider deleting with an org-scoped predicate (e.g., `id` + `org_id`) or fetching and validating `invitation.org_id === orgId` first.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/app/api/organizations/[orgId]/invitations/[invitationId]/route.ts
Line: 28:30

Comment:
[P1] Org-scoping bug in delete: the handler authorizes against `orgId`, but deletes by `where: { id: invitationId }` without verifying the invitation belongs to that org. An OWNER/ADMIN in Org A could delete an invitation in Org B if they know/guess its ID.

Consider deleting with an org-scoped predicate (e.g., `id` + `org_id`) or fetching and validating `invitation.org_id === orgId` first.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/401#discussion_r2750674454,,2026-02-01T07:35:05+00:00,security,critical,"This is a classic authorization bypass vulnerability where an admin in Organization A can delete invitations belonging to Organization B by guessing invitation IDs. The code checks org membership but then deletes by invitation ID alone without verifying the invitation belongs to that org. This is a clear, exploitable security flaw that would affect production.",2026-02-01T08:19:53.512402+00:00
davis7dotsh/better-context,149,fix(cli): wire chat + cursor auth,https://github.com/davis7dotsh/better-context/pull/149,"Removed many providers (cursor, groq, mistral, xai, cohere, deepinfra, cerebras, perplexity, togetherai, amazon-bedrock, azure, google-vertex) and all provider aliases. This breaks existing configs referencing these providers or aliases like `claude`, `gpt-4`, `gemini`, etc.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/server/src/providers/registry.ts
Line: 35:42

Comment:
Removed many providers (cursor, groq, mistral, xai, cohere, deepinfra, cerebras, perplexity, togetherai, amazon-bedrock, azure, google-vertex) and all provider aliases. This breaks existing configs referencing these providers or aliases like `claude`, `gpt-4`, `gemini`, etc.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/davis7dotsh/better-context/pull/149#discussion_r2750747570,,2026-02-01T08:29:33+00:00,logic,high,"This comment identifies a significant breaking change where multiple providers (cursor, groq, mistral, xai, cohere, deepinfra, cerebras, perplexity, togetherai, amazon-bedrock, azure, google-vertex) and their aliases were removed from the registry. This will cause existing user configurations referencing these providers to fail in production, affecting normal usage. The PR title mentions 'cursor auth' but actually removes cursor support entirely.",2026-02-01T10:16:09.088380+00:00
ever-co/ever-gauzy,9392,Feat/optimize2,https://github.com/ever-co/ever-gauzy/pull/9392,"GitHub Actions matrix syntax error: `os: [self-hosted, Windows, X64]` will be interpreted as 3 separate string values (""self-hosted"", ""Windows"", ""X64""), not as a single array of labels. This will cause the workflow to fail. Either remove the matrix and use `runs-on: [self-hosted, Windows, X64]` directly, or wrap in nested array: `os: [[self-hosted, Windows, X64]]`

```suggestion
        os: [[self-hosted, Windows, X64]]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/agent-prod.yml
Line: 285:285

Comment:
GitHub Actions matrix syntax error: `os: [self-hosted, Windows, X64]` will be interpreted as 3 separate string values (""self-hosted"", ""Windows"", ""X64""), not as a single array of labels. This will cause the workflow to fail. Either remove the matrix and use `runs-on: [self-hosted, Windows, X64]` directly, or wrap in nested array: `os: [[self-hosted, Windows, X64]]`

```suggestion
        os: [[self-hosted, Windows, X64]]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-gauzy/pull/9392#discussion_r2751090807,,2026-02-01T11:27:18+00:00,runtime,high,"Greptile correctly identified a GitHub Actions matrix syntax error that would cause CI/CD workflows to fail. When `os: [self-hosted, Windows, X64]` is used in a matrix, GitHub interprets this as 3 separate values instead of a single array of runner labels, which would break the build. This affects 12 production and staging workflow files and would be immediately broken upon merge.",2026-02-01T12:21:26.433586+00:00
pdfme/pdfme,1336,build(deps): bump react and @types/react,https://github.com/pdfme/pdfme/pull/1336,"`react` upgraded to 19.2.4 but `react-dom` still at 16.14.0 - this version mismatch will cause runtime errors. Both must be the same major version.

```suggestion
    ""react"": ""^19.2.4"",
    ""react-dom"": ""^19.2.4"",
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/ui/package.json
Line: 49:50

Comment:
`react` upgraded to 19.2.4 but `react-dom` still at 16.14.0 - this version mismatch will cause runtime errors. Both must be the same major version.

```suggestion
    ""react"": ""^19.2.4"",
    ""react-dom"": ""^19.2.4"",
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/pdfme/pdfme/pull/1336#discussion_r2751375033,,2026-02-01T15:19:14+00:00,runtime,critical,"This catch identifies a critical version mismatch where react is upgraded to 19.2.4 but react-dom remains at 16.14.0. This is a guaranteed runtime failure - React and ReactDOM must always be the same major version, and mixing React 19 with ReactDOM 16 will cause immediate crashes when trying to render anything. This is a fundamental incompatibility that would completely break the application.",2026-02-01T16:17:48.016997+00:00
pdfme/pdfme,1335,build(deps): bump react-dom and @types/react-dom,https://github.com/pdfme/pdfme/pull/1335,"**[P0] React/ReactDOM major version mismatch (likely runtime breakage)**

`react` is still pinned to `^16.14.0` while `react-dom` is bumped to `^19.2.4`. `react-dom@19` has a peer dep on `react@^19.2.4`, so installs will typically produce peer warnings and can break at runtime/build (render API + internals expect matching majors). This PR should bump `react` in the same package (and likely align `@types/react` too) rather than only `react-dom`.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/ui/package.json
Line: 47:51

Comment:
**[P0] React/ReactDOM major version mismatch (likely runtime breakage)**

`react` is still pinned to `^16.14.0` while `react-dom` is bumped to `^19.2.4`. `react-dom@19` has a peer dep on `react@^19.2.4`, so installs will typically produce peer warnings and can break at runtime/build (render API + internals expect matching majors). This PR should bump `react` in the same package (and likely align `@types/react` too) rather than only `react-dom`.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/pdfme/pdfme/pull/1335#discussion_r2751353446,,2026-02-01T15:12:25+00:00,runtime,high,This catch identifies a critical React version mismatch where react-dom is being upgraded from v16 to v19 while react remains at v16. This will definitely cause runtime breakage as React and ReactDOM must be the same major version - ReactDOM 19 expects React 19's internal APIs and rendering model. This is a real bug that would break the application in production.,2026-02-01T16:17:48.017115+00:00
pdfme/pdfme,1329,build(deps): bump antd from 5.28.0 to 6.2.2,https://github.com/pdfme/pdfme/pull/1329,"antd 6.x cannot be installed due to peer dependency conflict with `form-render@^2.5.5`. The `form-render` package has a peer dependency on `antd: ""4.x || 5.x""`, which doesn't include version 6.x. As a result, npm resolved antd to version 5.29.3 instead of 6.2.2 and marked it as a peer dependency (see `package-lock.json` where antd has `""peer"": true`).

To upgrade to antd 6.x, you must either:
1. Wait for `form-render` to add antd 6.x support and update it
2. Use `--force` or `--legacy-peer-deps` flags (not recommended as it may cause runtime issues)
3. Find an alternative to `form-render` that supports antd 6.x

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/ui/package.json
Line: 43:43

Comment:
antd 6.x cannot be installed due to peer dependency conflict with `form-render@^2.5.5`. The `form-render` package has a peer dependency on `antd: ""4.x || 5.x""`, which doesn't include version 6.x. As a result, npm resolved antd to version 5.29.3 instead of 6.2.2 and marked it as a peer dependency (see `package-lock.json` where antd has `""peer"": true`).

To upgrade to antd 6.x, you must either:
1. Wait for `form-render` to add antd 6.x support and update it
2. Use `--force` or `--legacy-peer-deps` flags (not recommended as it may cause runtime issues)
3. Find an alternative to `form-render` that supports antd 6.x

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/pdfme/pdfme/pull/1329#discussion_r2751356995,,2026-02-01T15:13:30+00:00,runtime,high,"Greptile correctly identified that the antd 6.x upgrade cannot actually succeed due to peer dependency conflicts with form-render, which only supports antd 4.x/5.x. This is a substantive catch that would prevent a broken dependency state from being merged - the PR claims to upgrade to antd 6.2.2 but npm actually resolved it to 5.29.3 due to the conflict.",2026-02-01T16:17:48.017197+00:00
pdfme/pdfme,1320,build(deps): bump react from 16.14.0 to 19.2.4 in /packages/ui,https://github.com/pdfme/pdfme/pull/1320,"`ReactDOM.render` removed in React 18+. The codebase uses `ReactDOM.render` in `Designer.tsx:67`, `Form.tsx:66`, and `Viewer.tsx:35`, which will break with React 19. Must migrate to `createRoot` API from `react-dom/client`.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/ui/package.json
Line: 49:50

Comment:
`ReactDOM.render` removed in React 18+. The codebase uses `ReactDOM.render` in `Designer.tsx:67`, `Form.tsx:66`, and `Viewer.tsx:35`, which will break with React 19. Must migrate to `createRoot` API from `react-dom/client`.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/pdfme/pdfme/pull/1320#discussion_r2751349485,,2026-02-01T15:10:51+00:00,runtime,critical,"This catch identifies that React 19 removes ReactDOM.render entirely, and the codebase uses it in multiple critical files (Designer.tsx, Form.tsx, Viewer.tsx). This would cause the entire UI package to completely break at runtime - not just a warning, but a hard failure. This is a genuine breaking change that would prevent the application from working.",2026-02-01T16:17:48.017284+00:00
pdfme/pdfme,1330,build(deps-dev): bump @testing-library/react from 12.1.5 to 16.3.2,https://github.com/pdfme/pdfme/pull/1330,"Critical peer dependency mismatch: `@testing-library/react` v16.3.2 requires React 18+ or 19+ (peer dependency: `react: ^18.0.0 || ^19.0.0`), but this package uses React 16.14.0 (line 49). This will cause installation warnings and tests will likely fail.

Either:
1. Downgrade to `@testing-library/react` v12.x (compatible with React 16)
2. Upgrade React to v18+ (breaking change requiring code updates)

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/ui/package.json
Line: 59:59

Comment:
Critical peer dependency mismatch: `@testing-library/react` v16.3.2 requires React 18+ or 19+ (peer dependency: `react: ^18.0.0 || ^19.0.0`), but this package uses React 16.14.0 (line 49). This will cause installation warnings and tests will likely fail.

Either:
1. Downgrade to `@testing-library/react` v12.x (compatible with React 16)
2. Upgrade React to v18+ (breaking change requiring code updates)

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/pdfme/pdfme/pull/1330#discussion_r2751353949,,2026-02-01T15:12:36+00:00,runtime,high,Greptile correctly identified a critical peer dependency mismatch where @testing-library/react v16.3.2 requires React 18+ but the package uses React 16.14.0. This will cause tests to fail in normal CI/development workflows. This is a legitimate catch that would break the test suite.,2026-02-01T16:17:48.017333+00:00
pdfme/pdfme,1324,build(deps): bump react-dom from 16.14.0 to 19.2.4 in /packages/ui,https://github.com/pdfme/pdfme/pull/1324,"Critical version mismatch: `react` is at `^16.14.0` but `react-dom` is being upgraded to `^19.2.4`. React and ReactDOM must have matching major versions.

```suggestion
    ""react"": ""^19.2.4"",
    ""react-dom"": ""^19.2.4"",
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/ui/package.json
Line: 49:50

Comment:
Critical version mismatch: `react` is at `^16.14.0` but `react-dom` is being upgraded to `^19.2.4`. React and ReactDOM must have matching major versions.

```suggestion
    ""react"": ""^19.2.4"",
    ""react-dom"": ""^19.2.4"",
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/pdfme/pdfme/pull/1324#discussion_r2751351839,,2026-02-01T15:11:52+00:00,runtime,high,"This catch identifies a critical version mismatch where react-dom is being upgraded to v19 while react remains at v16. React and ReactDOM must have matching major versions - this mismatch will cause runtime failures as the packages are tightly coupled. This is a clear, verifiable bug that would break the application in production.",2026-02-01T16:17:48.017387+00:00
pdfme/pdfme,1326,build(deps): bump pdfjs-dist from 3.11.174 to 5.4.530,https://github.com/pdfme/pdfme/pull/1326,"<details>
<summary>Additional Comments (1)</summary>

**`packages/converter/src/index.node.ts`**
legacy import paths may not exist in pdfjs-dist v5. The `/legacy/build/` paths were specific to v3 and earlier. Check if these imports still work with v5.4.530, as this could cause runtime errors.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/converter/src/index.node.ts
Line: 1:3

Comment:
legacy import paths may not exist in pdfjs-dist v5. The `/legacy/build/` paths were specific to v3 and earlier. Check if these imports still work with v5.4.530, as this could cause runtime errors.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/pdfme/pdfme/pull/1326#issuecomment-3831179678,,2026-02-01T15:11:24+00:00,runtime,high,"This comment identifies a real compatibility issue - the `/legacy/build/` import paths used in the codebase were specific to pdfjs-dist v3 and may not exist in v5.4.530, which would cause runtime import errors when the code tries to load the PDF.js library. This is a breaking change that would affect all users of the converter package.",2026-02-01T16:17:48.017479+00:00
ever-co/ever-gauzy,9401,Apps,https://github.com/ever-co/ever-gauzy/pull/9401,"[P0] TypeORM ManyToOne decorator can receive the *options object* as the inverse-side argument.

In `MultiORMManyToOne`, when `inverseSideOrOptions` is an object it is treated as `options` and `inverseSideProperty` remains unset, but the TypeORM branch still calls `TypeOrmManyToOne(..., inverseSideOrOptions as TypeORMInverseSide<T>, ...)`. Under TypeORM, calls like `MultiORMManyToOne(User, { nullable: true })` will pass that options object as the inverse side and likely break relation metadata registration.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/core/src/lib/core/decorators/entity/relations/many-to-one.decorator.ts
Line: 73:79

Comment:
[P0] TypeORM ManyToOne decorator can receive the *options object* as the inverse-side argument.

In `MultiORMManyToOne`, when `inverseSideOrOptions` is an object it is treated as `options` and `inverseSideProperty` remains unset, but the TypeORM branch still calls `TypeOrmManyToOne(..., inverseSideOrOptions as TypeORMInverseSide<T>, ...)`. Under TypeORM, calls like `MultiORMManyToOne(User, { nullable: true })` will pass that options object as the inverse side and likely break relation metadata registration.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-gauzy/pull/9401#discussion_r2751743497,,2026-02-01T18:08:56+00:00,logic,high,"Greptile correctly identified a real bug where the parameter normalization logic sets `inverseSideProperty` when `inverseSideOrOptions` is an object (treating it as options), but the TypeORM branch still incorrectly passes the original `inverseSideOrOptions` (which is now the options object) as the inverse-side argument. This will break relation metadata registration for any ManyToOne relation called with options but no inverse side function, which is a common usage pattern.",2026-02-01T18:19:55.549943+00:00
twentyhq/twenty,17610,Disable workspace creation by non-admin users,https://github.com/twentyhq/twenty/pull/17610,"Removing the first workspace check contradicts the configuration documentation and description.

The config description in `config-variables.ts:393` states: ""Ignored during initial setup when no workspace exists"", but this change now blocks **all** workspace creation for non-admins, including their first workspace created via `signUpInNewWorkspace`.

The documentation at `packages/twenty-docs/developers/self-host/capabilities/setup.mdx:115` also states: ""Users can still create their first workspace during initial signup.""

If this is intentional, both the config description and documentation need updates.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/core-modules/auth/services/sign-in-up.service.ts
Line: 406:410

Comment:
Removing the first workspace check contradicts the configuration documentation and description.

The config description in `config-variables.ts:393` states: ""Ignored during initial setup when no workspace exists"", but this change now blocks **all** workspace creation for non-admins, including their first workspace created via `signUpInNewWorkspace`.

The documentation at `packages/twenty-docs/developers/self-host/capabilities/setup.mdx:115` also states: ""Users can still create their first workspace during initial signup.""

If this is intentional, both the config description and documentation need updates.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17610#discussion_r2751958736,,2026-02-01T21:05:01+00:00,logic,high,"Greptile correctly identified that the code change removes the 'first workspace' exception, which contradicts both the configuration documentation and user-facing docs that explicitly state users can still create their first workspace during initial signup. This is a real functional regression that would break the documented behavior for new users trying to sign up when workspace creation is restricted to admins.",2026-02-01T22:14:51.006028+00:00
democratizedspace/dspace,3294,Add canonical routes catalog and prefer canonical-route-index in docs RAG,https://github.com/democratizedspace/dspace/pull/3294,"<details>
<summary>Additional Comments (1)</summary>

**`scripts/build-docs-rag-index.mjs`**
The route paths (like `/contentbackup`, `/quests`) are wrapped in backticks in the ROUTES.md markdown table and are being stripped by this regex, making the canonical route table useless for RAG queries.

Checked the generated chunk with `anchor: ""canonical-route-index""` and confirmed it contains `| Home | | Homepage. | Main nav |` instead of `| Home | / | Homepage. | Main nav |` - the `/` route is missing.

This defeats the PR's goal of preventing /chat hallucinations about routes since the actual route paths aren't in the indexed content.
</details>",https://github.com/democratizedspace/dspace/pull/3294#issuecomment-3832310515,,2026-02-01T23:49:45+00:00,logic,high,"Comment 1 identifies a critical flaw where the regex strips backtick-wrapped route paths from the markdown table during indexing, making the entire PR's goal ineffective. The reviewer verified this by checking the actual generated chunk content, proving routes like `/` and `/contentbackup` are missing from the indexed data. This completely defeats the feature's purpose of providing canonical route information to prevent hallucinations.",2026-02-02T00:52:34.870299+00:00
traceroot-ai/traceroot,402,[AgentOps][Refactor] Migrate from SqlAlchmey to Prisma for Postgres DB [1/n],https://github.com/traceroot-ai/traceroot/pull/402,"Schema changed table names from `membership_invitations` to `invites`, `organization_memberships` to `workspace_members`, and `organizations` to `workspaces`, but no Prisma migration was included. This will break on existing databases.

You need to either:
1. Create a Prisma migration to rename the tables: `npx prisma migrate dev --name rename_org_to_workspace`
2. Manually create SQL ALTER TABLE RENAME statements in a migration file

Without migrations, `prisma migrate deploy` will fail in production.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/prisma/schema.prisma
Line: 28:41

Comment:
Schema changed table names from `membership_invitations` to `invites`, `organization_memberships` to `workspace_members`, and `organizations` to `workspaces`, but no Prisma migration was included. This will break on existing databases.

You need to either:
1. Create a Prisma migration to rename the tables: `npx prisma migrate dev --name rename_org_to_workspace`
2. Manually create SQL ALTER TABLE RENAME statements in a migration file

Without migrations, `prisma migrate deploy` will fail in production.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/402#discussion_r2752147969,,2026-02-02T00:04:16+00:00,data_integrity,high,"This catch identifies a critical migration issue - table names were changed in the Prisma schema without including database migrations. This WILL cause production deployment failures when `prisma migrate deploy` is run against existing databases, potentially causing data loss or service outages. The issue is concrete, verifiable from the schema changes, and requires action before deployment.",2026-02-02T00:52:34.870359+00:00
alpaca-network/railway-grafana-stack,94,Fix-Entry-Point-Grafana,https://github.com/Alpaca-Network/railway-grafana-stack/pull/94,"Prometheus still references the deleted alertmanager service via `depends_on`. This will cause Prometheus to fail to start.

```suggestion
    depends_on:
      mimir:
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: docker-compose.yml
Line: 85:86

Comment:
Prometheus still references the deleted alertmanager service via `depends_on`. This will cause Prometheus to fail to start.

```suggestion
    depends_on:
      mimir:
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/railway-grafana-stack/pull/94#discussion_r2752302436,,2026-02-02T02:26:55+00:00,runtime,high,"Comment 0 correctly identifies that the diff shows the alertmanager service being deleted, but the depends_on reference to alertmanager was only partially removed (the diff shows '-      alertmanager:' and '-        condition: service_started' being removed). However, looking at the diff more carefully, those lines ARE being removed (shown with minus signs). Wait - re-reading the diff, the alertmanager dependency IS being removed. Let me check Comment 0's claim again... The diff shows the depends_on block going from having both alertmanager and mimir to just having mimir. So Comment 0 appears to be a false positive - the dependency WAS removed correctly.",2026-02-02T03:18:06.097144+00:00
stack-auth/stack-auth,1145,Queries view,https://github.com/stack-auth/stack-auth/pull/1145,"`onRetry` is being invoked immediately with `onRetry()` instead of passing the function reference. This causes the async operation to execute during render.

```suggestion
        onClick={() => runAsynchronouslyWithAlert(onRetry)}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/dashboard/src/app/(main)/(protected)/projects/[projectId]/analytics/queries/page-client.tsx
Line: 292:292

Comment:
`onRetry` is being invoked immediately with `onRetry()` instead of passing the function reference. This causes the async operation to execute during render.

```suggestion
        onClick={() => runAsynchronouslyWithAlert(onRetry)}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/stack-auth/stack-auth/pull/1145#discussion_r2752407305,,2026-02-02T03:41:01+00:00,runtime,high,"This is a genuine bug where `runAsynchronouslyWithAlert(onRetry())` immediately invokes `onRetry()` during render instead of passing it as a callback. This would cause the async operation to fire every time the component renders, which is incorrect behavior that would affect users in normal usage. The fix to use `() => runAsynchronouslyWithAlert(onRetry)` is correct.",2026-02-02T04:53:10.040960+00:00
twentyhq/twenty,17615,Add Claude Code GitHub Workflow,https://github.com/twentyhq/twenty/pull/17615,"Workflow can be triggered by any user who can comment, including external contributors. While the PR description mentions ""Only users with write access can trigger the workflow,"" the conditional checks don't enforce this. Consider adding an authorization check:

```suggestion
    if: |
      github.event.comment.author_association == 'OWNER' ||
      github.event.comment.author_association == 'MEMBER' ||
      github.event.comment.author_association == 'COLLABORATOR' &&
      ((github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' && (contains(github.event.issue.body, '@claude') || contains(github.event.issue.title, '@claude'))))
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/claude.yml
Line: 15:19

Comment:
Workflow can be triggered by any user who can comment, including external contributors. While the PR description mentions ""Only users with write access can trigger the workflow,"" the conditional checks don't enforce this. Consider adding an authorization check:

```suggestion
    if: |
      github.event.comment.author_association == 'OWNER' ||
      github.event.comment.author_association == 'MEMBER' ||
      github.event.comment.author_association == 'COLLABORATOR' &&
      ((github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' && (contains(github.event.issue.body, '@claude') || contains(github.event.issue.title, '@claude'))))
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17615#discussion_r2753128267,,2026-02-02T08:21:17+00:00,security,high,"This is a legitimate security concern - the workflow can be triggered by any external contributor who can comment on issues/PRs, potentially allowing them to run Claude Code actions on the repository. The PR description claims 'Only users with write access can trigger the workflow' but the code doesn't enforce this restriction. This could allow unauthorized users to consume API credits or potentially trigger unintended actions.",2026-02-02T08:29:10.754097+00:00
runanywhereai/runanywhere-sdks,323,"ci(swift): Swift CI/CD only – auto-tag, build-release, release, Packa…",https://github.com/RunanywhereAI/runanywhere-sdks/pull/323,"[P1] Broad trigger: tags on every push to `main`

This workflow triggers on any push to `main` (including merges, reverts, and automated commits from other workflows). If Phase 2 commits to `main`, that commit will also trigger Phase 1 and create another tag unless you add a guard (e.g., skip when actor is `github-actions[bot]` or when commit message matches the checksum update). Otherwise you can end up in a tag churn/feedback loop.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/swift-auto-tag.yml
Line: 15:21

Comment:
[P1] Broad trigger: tags on every push to `main`

This workflow triggers on any push to `main` (including merges, reverts, and automated commits from other workflows). If Phase 2 commits to `main`, that commit will also trigger Phase 1 and create another tag unless you add a guard (e.g., skip when actor is `github-actions[bot]` or when commit message matches the checksum update). Otherwise you can end up in a tag churn/feedback loop.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/RunanywhereAI/runanywhere-sdks/pull/323#discussion_r2753353558,,2026-02-02T09:21:06+00:00,logic,high,"This catch identifies a critical feedback loop bug: Phase 2 commits to main, which triggers Phase 1 to create another tag, which triggers Phase 2 again, causing infinite tag churn. This is a real CI/CD bug that would cause production issues in the release pipeline. The analysis is correct and actionable.",2026-02-02T10:33:32.391973+00:00
nvidia/dali,6185,"Update DALI_DEPS: libsound, openssl",https://github.com/NVIDIA/DALI/pull/6185,"The commit hash has been replaced with ""FIXME"" placeholder. This will break the Docker build process.

The `DALI_DEPS_VERSION` file must contain a valid Git commit SHA from the DALI_deps repository. The Dockerfile at `docker/Dockerfile.deps:38` uses this value to `git checkout ${DALI_DEPS_VERSION_SHA}`, which will fail with ""FIXME"".

According to the PR description, this should propagate updates from https://github.com/NVIDIA/DALI_deps/pull/151. You need to replace ""FIXME"" with the actual commit hash from that merged PR.

```suggestion
<actual-commit-hash-from-DALI_deps-PR-151>
```",https://github.com/NVIDIA/DALI/pull/6185#discussion_r2753765641,,2026-02-02T10:59:23+00:00,runtime,high,"This catch identifies that a critical version file was replaced with a 'FIXME' placeholder, which will cause the Docker build to fail at runtime when attempting to checkout a non-existent git reference. This is a clear build-breaking issue that would affect anyone trying to build the project, and the analysis correctly identifies the exact failure point in the Dockerfile.",2026-02-02T12:28:03.705517+00:00
gptme/gptme,1218,fix: use VITE_GPTME_FLEET_BASE_URL as env var for base urls,https://github.com/gptme/gptme/pull/1218,"`VITE_GPTME_FLEET_BASE_URL` is being used for both `FLEET_OPERATOR_URL` (line 5) and `baseUrl` (line 105). These serve different purposes:
- `FLEET_OPERATOR_URL` should point to the fleet operator service (`https://fleet.gptme.ai`) for auth code exchange
- `baseUrl` should point to the specific gptme API instance URL

Using the same env var for both will cause the fleet operator exchange URL to incorrectly point to the instance URL, breaking auth code exchange. The previous code used `VITE_API_URL` for `baseUrl` which was separate from `VITE_FLEET_OPERATOR_URL`.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: webui/src/utils/connectionConfig.ts
Line: 102:106

Comment:
`VITE_GPTME_FLEET_BASE_URL` is being used for both `FLEET_OPERATOR_URL` (line 5) and `baseUrl` (line 105). These serve different purposes:
- `FLEET_OPERATOR_URL` should point to the fleet operator service (`https://fleet.gptme.ai`) for auth code exchange
- `baseUrl` should point to the specific gptme API instance URL

Using the same env var for both will cause the fleet operator exchange URL to incorrectly point to the instance URL, breaking auth code exchange. The previous code used `VITE_API_URL` for `baseUrl` which was separate from `VITE_FLEET_OPERATOR_URL`.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1218#discussion_r2753685529,,2026-02-02T10:38:47+00:00,logic,high,Greptile correctly identified that using the same environment variable (VITE_GPTME_FLEET_BASE_URL) for two different purposes - fleet operator URL for auth exchange AND the API base URL - will break authentication. The fleet operator URL needs to point to the auth service while baseUrl should point to the API instance. This conflation would cause auth code exchange to fail in production when users configure their own instance URL.,2026-02-02T12:28:03.705605+00:00
getarcaneapp/arcane,1635,feat(autoupdate/prune): implement container exclusion and prune notif…,https://github.com/getarcaneapp/arcane/pull/1635,"`result.SpaceReclaimed` already contains the container space from line 68. Adding it again to `ContainerSpaceReclaimed` creates double counting since `SpaceReclaimed` is set in `pruneContainers` at line 284.

```suggestion
		} else {
			result.ContainerSpaceReclaimed = result.SpaceReclaimed
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/internal/services/system_service.go
Line: 71:71

Comment:
`result.SpaceReclaimed` already contains the container space from line 68. Adding it again to `ContainerSpaceReclaimed` creates double counting since `SpaceReclaimed` is set in `pruneContainers` at line 284.

```suggestion
		} else {
			result.ContainerSpaceReclaimed = result.SpaceReclaimed
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/getarcaneapp/arcane/pull/1635#discussion_r2753783923,fixed in commit 2cd8b76e,2026-02-02T11:04:01+00:00,data_integrity,medium,"Greptile correctly identified a double-counting bug where space reclaimed values would be incorrectly reported. The developer confirmed and fixed this issue. While not critical, this is a real data integrity bug that would cause incorrect metrics to be displayed to users.",2026-02-02T12:28:03.705681+00:00
posthog/posthog,46531,WIP: Turn off posthog analytics,https://github.com/PostHog/posthog/pull/46531,"The entire exception logging functionality has been removed. This function now does nothing (`pass`), which means exceptions won't be logged to `structlog` anymore. The commented code shows that even when `api_key` was not set, exceptions were still logged via `logger.exception(error)`.

This will make debugging production issues much harder. Consider keeping at least the basic logging:

```suggestion
def capture_exception(error=None, additional_properties=None):
    import structlog
    logger = structlog.get_logger(__name__)
    logger.exception(error)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/exceptions_capture.py
Line: 16:42

Comment:
The entire exception logging functionality has been removed. This function now does nothing (`pass`), which means exceptions won't be logged to `structlog` anymore. The commented code shows that even when `api_key` was not set, exceptions were still logged via `logger.exception(error)`.

This will make debugging production issues much harder. Consider keeping at least the basic logging:

```suggestion
def capture_exception(error=None, additional_properties=None):
    import structlog
    logger = structlog.get_logger(__name__)
    logger.exception(error)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46531#discussion_r2754129728,,2026-02-02T12:36:25+00:00,logic,high,"Greptile correctly identified that the `pass` statement at the beginning of `capture_exception()` causes the function to return immediately, meaning no exceptions will be logged anymore - not even to structlog. This is a real bug that will affect production debugging. The PR title says 'WIP' but this catch prevents silent exception swallowing that could make it to production.",2026-02-02T14:29:34.861319+00:00
openreplay/openreplay,4283,refactor(DB): changed DB scripts,https://github.com/openreplay/openreplay/pull/4283,"Missing `CREATE MATERIALIZED VIEW` after this DROP statement. The v1.24.0 schema includes `users_all_properties_extractor_mv` with columns `display_name`, `description`, `status`, and `_edited_by_user`. The rollback should recreate this view:

```suggestion
DROP TABLE IF EXISTS product_analytics.users_all_properties_extractor_mv;
CREATE MATERIALIZED VIEW IF NOT EXISTS product_analytics.users_all_properties_extractor_mv
    TO product_analytics.all_properties AS
SELECT project_id,
       'users'                                 AS source,
       property_name,
       FALSE                                   AS is_event_property,
       auto_captured_property                  AS auto_captured,
       or_property_display_name(property_name) AS display_name,
       ''                                      AS description,
       or_property_visibility(property_name)   AS status,
       0                                       AS data_count,
       0                                       AS query_count,
       _timestamp                              AS created_at,
       FALSE                                   AS _edited_by_user
FROM product_analytics.user_properties;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/scripts/schema/db/rollback_dbs/clickhouse/1.25.0/1.25.0.sql
Line: 41:41

Comment:
Missing `CREATE MATERIALIZED VIEW` after this DROP statement. The v1.24.0 schema includes `users_all_properties_extractor_mv` with columns `display_name`, `description`, `status`, and `_edited_by_user`. The rollback should recreate this view:

```suggestion
DROP TABLE IF EXISTS product_analytics.users_all_properties_extractor_mv;
CREATE MATERIALIZED VIEW IF NOT EXISTS product_analytics.users_all_properties_extractor_mv
    TO product_analytics.all_properties AS
SELECT project_id,
       'users'                                 AS source,
       property_name,
       FALSE                                   AS is_event_property,
       auto_captured_property                  AS auto_captured,
       or_property_display_name(property_name) AS display_name,
       ''                                      AS description,
       or_property_visibility(property_name)   AS status,
       0                                       AS data_count,
       0                                       AS query_count,
       _timestamp                              AS created_at,
       FALSE                                   AS _edited_by_user
FROM product_analytics.user_properties;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4283#discussion_r2754406936,,2026-02-02T13:44:19+00:00,data_integrity,high,"This is a legitimate catch of an incomplete database rollback script. The script drops `users_all_properties_extractor_mv` but fails to recreate it, which would leave the database in an inconsistent state during rollback operations. This would cause the materialized view to be missing after a rollback, breaking functionality that depends on it. Comment 0 is more detailed than Comment 1 which is a duplicate.",2026-02-02T14:29:34.861425+00:00
posthog/posthog,46552,fix(billing): Implement a proxy for usage signals to avoid CH timeouts,https://github.com/PostHog/posthog/pull/46552,"`heatmaps` missing from `usage_to_product` mapping

The `usage_to_product` dict only maps 3 products but `PRODUCT_EVENT_MAP` defines 4 products including `heatmaps`. Orgs with heatmap usage won't be detected as having active products.

Either add heatmaps to the mapping, or document why it's intentionally excluded.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/billing/dags/productled_outbound_targets.py
Line: 244:249

Comment:
`heatmaps` missing from `usage_to_product` mapping

The `usage_to_product` dict only maps 3 products but `PRODUCT_EVENT_MAP` defines 4 products including `heatmaps`. Orgs with heatmap usage won't be detected as having active products.

Either add heatmaps to the mapping, or document why it's intentionally excluded.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46552#discussion_r2754935933,Resolved with: [`ce4a5de` (this PR)](https://github.com/PostHog/posthog/pull/46552/commits/ce4a5deef4bd9d27454ae77b73c88c5df2f4fa6b),2026-02-02T15:31:53+00:00,data_integrity,medium,"Greptile correctly identified that the `usage_to_product` mapping was missing 'heatmaps' while the original `PRODUCT_EVENT_MAP` included 4 products. This would cause organizations using heatmaps to not be detected as having active products, affecting billing/usage tracking. The developer confirmed and fixed the issue, validating it was a real bug.",2026-02-02T16:22:28.170611+00:00
posthog/posthog,46549,feat(logs): allow filtering on resource_fingerprint,https://github.com/PostHog/posthog/pull/46549,"<details>
<summary>Additional Comments (1)</summary>

**`products/logs/backend/logs_query_runner.py`**
the `resourceFingerprint` filter is not being applied in the `where()` method. the parameter is accepted in the schema and API but never checked here. add a filter like:

```suggestion
    def where(self) -> ast.Expr:
        exprs: list[ast.Expr] = []

        if self.query.serviceNames:
            exprs.append(
                parse_expr(
                    ""service_name IN {serviceNames}"",
                    placeholders={
                        ""serviceNames"": ast.Tuple(exprs=[ast.Constant(value=str(sn)) for sn in self.query.serviceNames])
                    },
                )
            )

        if self.query.resourceFingerprint:
            exprs.append(
                parse_expr(
                    ""resource_fingerprint = {resourceFingerprint}"",
                    placeholders={
                        ""resourceFingerprint"": ast.Constant(value=self.query.resourceFingerprint)
                    },
                )
            )

        if self.query.filterGroup:
            exprs.append(self.resource_filter(existing_filters=exprs))

            if self.attribute_filters:
                exprs.append(property_to_expr(self.attribute_filters, team=self.team))

            if self.log_filters:
                exprs.append(property_to_expr(self.log_filters, team=self.team))

        exprs.append(ast.Placeholder(expr=ast.Field(chain=[""filters""])))

        if self.query.severityLevels:
            exprs.append(
                parse_expr(
                    ""severity_text IN {severityLevels}"",
                    placeholders={
                        ""severityLevels"": ast.Tuple(
                            exprs=[ast.Constant(value=str(sl)) for sl in self.query.severityLevels]
                        )
                    },
                )
            )

        if self.query.liveLogsCheckpoint:
            exprs.append(
                parse_expr(
                    ""observed_timestamp >= {liveLogsCheckpoint}"",
                    placeholders={""liveLogsCheckpoint"": ast.Constant(value=self.query.liveLogsCheckpoint)},
                )
            )

        if self.query.after:
            try:
                cursor = json.loads(base64.b64decode(self.query.after).decode(""utf-8""))
                cursor_ts = dt.datetime.fromisoformat(cursor[""timestamp""])
                cursor_uuid = cursor[""uuid""]
            except (KeyError, ValueError, json.JSONDecodeError) as e:
                raise ValueError(f""Invalid cursor format: {e}"")
            # For ASC (earliest first): get rows where (timestamp, uuid) > cursor
            # For DESC (latest first, default): get rows where (timestamp, uuid) < cursor
            op = "">"" if self.query.orderBy == ""earliest"" else ""<""
            ts_op = "">="" if self.query.orderBy == ""earliest"" else ""<=""
            # The logs table is sorted by (team_id, time_bucket, ..., timestamp) where
            # time_bucket = toStartOfDay(timestamp). ClickHouse only prunes efficiently when
            # the WHERE clause matches the sorting key. A tuple comparison like
            # (timestamp, uuid) < (x, y) won't trigger pruning.
            # We add explicit scalar bounds on both time_bucket and timestamp to ensure
            # ClickHouse can use the primary index and skip irrelevant parts.
            exprs.append(
                parse_expr(
                    f""time_bucket {ts_op} toStartOfDay({{cursor_ts}})"",
                    placeholders={""cursor_ts"": ast.Constant(value=cursor_ts)},
                )
            )
            exprs.append(
                parse_expr(
                    f""timestamp {ts_op} {{cursor_ts}}"",
                    placeholders={""cursor_ts"": ast.Constant(value=cursor_ts)},
                )
            )
            # Tuple comparison handles the exact cursor position (same timestamp, different uuid)
            exprs.append(
                parse_expr(
                    f""(timestamp, uuid) {op} ({{cursor_ts}}, {{cursor_uuid}})"",
                    placeholders={
                        ""cursor_ts"": ast.Constant(value=cursor_ts),
                        ""cursor_uuid"": ast.Constant(value=cursor_uuid),
                    },
                )
            )

        return ast.And(exprs=exprs)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/logs/backend/logs_query_runner.py
Line: 242:326

Comment:
the `resourceFingerprint` filter is not being applied in the `where()` method. the parameter is accepted in the schema and API but never checked here. add a filter like:

```suggestion
    def where(self) -> ast.Expr:
        exprs: list[ast.Expr] = []

        if self.query.serviceNames:
            exprs.append(
                parse_expr(
                    ""service_name IN {serviceNames}"",
                    placeholders={
                        ""serviceNames"": ast.Tuple(exprs=[ast.Constant(value=str(sn)) for sn in self.query.serviceNames])
                    },
                )
            )

        if self.query.resourceFingerprint:
            exprs.append(
                parse_expr(
                    ""resource_fingerprint = {resourceFingerprint}"",
                    placeholders={
                        ""resourceFingerprint"": ast.Constant(value=self.query.resourceFingerprint)
                    },
                )
            )

        if self.query.filterGroup:
            exprs.append(self.resource_filter(existing_filters=exprs))

            if self.attribute_filters:
                exprs.append(property_to_expr(self.attribute_filters, team=self.team))

            if self.log_filters:
                exprs.append(property_to_expr(self.log_filters, team=self.team))

        exprs.append(ast.Placeholder(expr=ast.Field(chain=[""filters""])))

        if self.query.severityLevels:
            exprs.append(
                parse_expr(
                    ""severity_text IN {severityLevels}"",
                    placeholders={
                        ""severityLevels"": ast.Tuple(
                            exprs=[ast.Constant(value=str(sl)) for sl in self.query.severityLevels]
                        )
                    },
                )
            )

        if self.query.liveLogsCheckpoint:
            exprs.append(
                parse_expr(
                    ""observed_timestamp >= {liveLogsCheckpoint}"",
                    placeholders={""liveLogsCheckpoint"": ast.Constant(value=self.query.liveLogsCheckpoint)},
                )
            )

        if self.query.after:
            try:
                cursor = json.loads(base64.b64decode(self.query.after).decode(""utf-8""))
                cursor_ts = dt.datetime.fromisoformat(cursor[""timestamp""])
                cursor_uuid = cursor[""uuid""]
            except (KeyError, ValueError, json.JSONDecodeError) as e:
                raise ValueError(f""Invalid cursor format: {e}"")
            # For ASC (earliest first): get rows where (timestamp, uuid) > cursor
            # For DESC (latest first, default): get rows where (timestamp, uuid) < cursor
            op = "">"" if self.query.orderBy == ""earliest"" else ""<""
            ts_op = "">="" if self.query.orderBy == ""earliest"" else ""<=""
            # The logs table is sorted by (team_id, time_bucket, ..., timestamp) where
            # time_bucket = toStartOfDay(timestamp). ClickHouse only prunes efficiently when
            # the WHERE clause matches the sorting key. A tuple comparison like
            # (timestamp, uuid) < (x, y) won't trigger pruning.
            # We add explicit scalar bounds on both time_bucket and timestamp to ensure
            # ClickHouse can use the primary index and skip irrelevant parts.
            exprs.append(
                parse_expr(
                    f""time_bucket {ts_op} toStartOfDay({{cursor_ts}})"",
                    placeholders={""cursor_ts"": ast.Constant(value=cursor_ts)},
                )
            )
            exprs.append(
                parse_expr(
                    f""timestamp {ts_op} {{cursor_ts}}"",
                    placeholders={""cursor_ts"": ast.Constant(value=cursor_ts)},
                )
            )
            # Tuple comparison handles the exact cursor position (same timestamp, different uuid)
            exprs.append(
                parse_expr(
                    f""(timestamp, uuid) {op} ({{cursor_ts}}, {{cursor_uuid}})"",
                    placeholders={
                        ""cursor_ts"": ast.Constant(value=cursor_ts),
                        ""cursor_uuid"": ast.Constant(value=cursor_uuid),
                    },
                )
            )

        return ast.And(exprs=exprs)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/PostHog/posthog/pull/46549#issuecomment-3835611906,,2026-02-02T14:54:06+00:00,logic,high,"Greptile correctly identified that the `resourceFingerprint` parameter is accepted in the schema/API but never actually applied in the `where()` method, meaning the feature is completely broken - users would expect filtering by resource fingerprint to work, but queries would return unfiltered results. Comment 0 is a duplicate identifying the same issue from the test perspective. This is a clear functional bug that would affect users in normal usage.",2026-02-02T16:22:28.170686+00:00
posthog/posthog,46546,[WIP] feat(dashboards): use ai to explain changes,https://github.com/PostHog/posthog/pull/46546,"missing validation allows users to access other users' cached data by providing arbitrary cache keys

The endpoint accepts any cache key from the request body without verifying it belongs to the current user and dashboard. This allows unauthorized access to cached analysis data from other users' sessions.

Validate that the provided cache key matches the format created in the `retrieve` method for this specific user and dashboard before using it.

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/dashboards/dashboard.py
Line: 885:896

Comment:
missing validation allows users to access other users' cached data by providing arbitrary cache keys

The endpoint accepts any cache key from the request body without verifying it belongs to the current user and dashboard. This allows unauthorized access to cached analysis data from other users' sessions.

Validate that the provided cache key matches the format created in the `retrieve` method for this specific user and dashboard before using it.

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46546#discussion_r2754684248,,2026-02-02T14:41:47+00:00,security,critical,Comment 0 identifies a security vulnerability where the endpoint accepts arbitrary cache keys from request body without validating ownership. This allows users to access other users' cached AI analysis data by guessing/providing cache keys. This is a real data exposure issue that could leak sensitive dashboard analysis information between users.,2026-02-02T16:22:28.170906+00:00
nvidia/nvflare,4089,[2.7] amplify tutorial updates,https://github.com/NVIDIA/NVFlare/pull/4089,"String interpolation uses `$task` instead of f-string or `.format()` syntax. In Python, this will print the literal text ""Combing $task CSV data"" instead of the actual task name.

```suggestion
    print(f""Combing {task} CSV data"")
```",https://github.com/NVIDIA/NVFlare/pull/4089#discussion_r2755039646,fixed,2026-02-02T15:54:06+00:00,runtime,medium,"Greptile correctly identified a Python string interpolation bug where `$task` was used instead of f-string syntax `{task}`. This would cause the code to print the literal text '$task' instead of the actual task name, breaking the expected functionality. The developer confirmed and fixed the issue.",2026-02-02T16:22:28.170954+00:00
nvidia/nvflare,4088,Update amplify tutorial notebook,https://github.com/NVIDIA/NVFlare/pull/4088,"String interpolation syntax incorrect - `$task` won't work in Python

```suggestion
    print(f""Combing {task} CSV data"")
```",https://github.com/NVIDIA/NVFlare/pull/4088#discussion_r2754997517,fixed,2026-02-02T15:45:42+00:00,runtime,low,"Greptile correctly identified that `$task` is shell/Perl interpolation syntax, not valid Python f-string syntax. The code would print the literal string '$task' instead of the variable value. Developer confirmed and fixed. However, this is in a tutorial notebook and is a fairly obvious syntax error that would be caught quickly during testing.",2026-02-02T16:22:28.170976+00:00
posthog/posthog,46571,feat(mcp): add queryable memory system to MCP server,https://github.com/PostHog/posthog/pull/46571,"The HogQL query in the `/query/` endpoint doesn't filter by team_id in the `document_embeddings` table. This means users could potentially query memories from other teams if they can guess document IDs.

Add team filtering in the WHERE clause:

```suggestion
        query = f""""""
            SELECT
                document_id,
                content,
                metadata,
                cosineDistance(embedding, embedText({{query_text}}, {{model_name}})) as distance
            FROM (
                SELECT
                    document_id,
                    argMax(content, inserted_at) as content,
                    argMax(metadata, inserted_at) as metadata,
                    argMax(embedding, inserted_at) as embedding
                FROM document_embeddings
                WHERE model_name = {{model_name}}
                  AND product = 'posthog-ai'
                  AND document_type = 'memory'
                  AND team_id = {{team_id}}
                GROUP BY document_id, model_name, product, document_type, rendering
            )
            WHERE ({{skip_user_filter}} OR JSONExtractString(metadata, 'user_id') = {{user_id}})
              AND NOT JSONExtractBool(metadata, 'deleted')
              AND ({metadata_filter_sql})
            ORDER BY distance ASC
            LIMIT {{limit}}
        """"""
```

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/posthog_ai/backend/api.py
Line: 101:124

Comment:
The HogQL query in the `/query/` endpoint doesn't filter by team_id in the `document_embeddings` table. This means users could potentially query memories from other teams if they can guess document IDs.

Add team filtering in the WHERE clause:

```suggestion
        query = f""""""
            SELECT
                document_id,
                content,
                metadata,
                cosineDistance(embedding, embedText({{query_text}}, {{model_name}})) as distance
            FROM (
                SELECT
                    document_id,
                    argMax(content, inserted_at) as content,
                    argMax(metadata, inserted_at) as metadata,
                    argMax(embedding, inserted_at) as embedding
                FROM document_embeddings
                WHERE model_name = {{model_name}}
                  AND product = 'posthog-ai'
                  AND document_type = 'memory'
                  AND team_id = {{team_id}}
                GROUP BY document_id, model_name, product, document_type, rendering
            )
            WHERE ({{skip_user_filter}} OR JSONExtractString(metadata, 'user_id') = {{user_id}})
              AND NOT JSONExtractBool(metadata, 'deleted')
              AND ({metadata_filter_sql})
            ORDER BY distance ASC
            LIMIT {{limit}}
        """"""
```

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46571#discussion_r2755592219,,2026-02-02T17:59:48+00:00,security,critical,"This is a legitimate security vulnerability - the HogQL query searches the document_embeddings table without filtering by team_id, which means users could potentially access memories from other teams. Comment 0 and 1 are related (0 identifies the missing WHERE clause, 1 adds the placeholder), but Comment 0 is the primary catch that identifies the root security issue. This is a classic multi-tenant data isolation bug that could expose sensitive data across organizational boundaries.",2026-02-02T18:27:23.493934+00:00
twentyhq/twenty,17637,Files field - add files field display and input + filtering,https://github.com/twentyhq/twenty/pull/17637,"[P0] This file references `FieldFileValue` but does not import or define it, so it won’t type-check/compile.

```suggestion
import { type FilesFilter, type FieldFileValue } from '@/types';
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-shared/src/utils/filter/utils/isMatchingFilesFilter.ts
Line: 1:3

Comment:
[P0] This file references `FieldFileValue` but does not import or define it, so it won’t type-check/compile.

```suggestion
import { type FilesFilter, type FieldFileValue } from '@/types';
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17637#discussion_r2755634397,,2026-02-02T18:11:43+00:00,type_error,high,"Comment 7 identifies a clear compilation error where `FieldFileValue` type is used but never imported or defined in the file. This would cause TypeScript compilation to fail, preventing the code from building. This is a straightforward, verifiable bug that would break the build.",2026-02-02T18:27:23.494034+00:00
twentyhq/twenty,17646,Migrate workflow serverless to logic,https://github.com/twentyhq/twenty/pull/17646,"Check that files are successfully uploaded before cleanup - if `applicationUniversalIdentifier` is undefined, files are migrated to temp but never uploaded, then deleted at line 228. This could result in data loss.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/database/commands/upgrade-version-command/1-17/1-17-migrate-workflow-code-steps.command.ts
Line: 219:226

Comment:
Check that files are successfully uploaded before cleanup - if `applicationUniversalIdentifier` is undefined, files are migrated to temp but never uploaded, then deleted at line 228. This could result in data loss.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17646#discussion_r2755394467,,2026-02-02T17:11:17+00:00,data_integrity,high,This catch identifies a potential data loss scenario in a migration command where files could be deleted from temporary storage after migration but before successful upload if the applicationUniversalIdentifier is undefined. Migration commands that can cause data loss are serious issues since they affect production data and are typically run once.,2026-02-02T18:27:23.494067+00:00
nvidia/nvflare,4087,[2.7] Fix xgboost recipes,https://github.com/NVIDIA/NVFlare/pull/4087,"<details>
<summary>Additional Comments (1)</summary>

**`nvflare/app_opt/xgboost/recipes/vertical.py`**
missing `data_loader` parameter in `__init__` signature, but line 202 references `data_loader` variable causing `NameError`

```suggestion
        data_loader: Optional[""XGBDataLoader""] = None,
```
</details>",https://github.com/NVIDIA/NVFlare/pull/4087#issuecomment-3836812838,,2026-02-02T18:03:29+00:00,runtime,high,"This identifies a clear NameError bug where `data_loader` is referenced on line 202 but never defined as a parameter in the `__init__` signature. This would cause a runtime crash when the code path using `data_loader` is executed, affecting users in normal usage of the vertical XGBoost recipe.",2026-02-02T18:27:23.494095+00:00
nvidia/nvflare,4090,[2.7] Fix incorrect test_filter.py and resources.json,https://github.com/NVIDIA/NVFlare/pull/4090,"Missing parentheses in `super()` call causes runtime error

```suggestion
        super().__init__()
```",https://github.com/NVIDIA/NVFlare/pull/4090#discussion_r2755431542,,2026-02-02T17:18:55+00:00,runtime,high,"This is a genuine bug that will cause a runtime error. Using `super.__init__()` instead of `super().__init__()` will fail when the class is instantiated because `super` without parentheses returns the super type object, not a bound super object. This will affect any normal usage of the TestFilter class.",2026-02-02T18:27:23.494113+00:00
ever-co/ever-teams,4245,Refactor: Separate useTeamTasks into more atomic Hooks,https://github.com/ever-co/ever-teams/pull/4245,"Removed critical data synchronization effect that syncs React Query data with Jotai state. The original code had:

```typescript
useConditionalUpdateEffect(
  () => {
    if (teamTasksQuery.data?.items) {
      deepCheckAndUpdateTasks(teamTasksQuery.data.items, true);
    }
  },
  [teamTasksQuery.data?.items],
  Boolean(tasks?.length)
);
```

This effect ensures that when React Query fetches new data, it's synchronized with the Jotai store. Without it, components relying on the Jotai state won't see updates from the server.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/web/core/hooks/organizations/teams/use-team-tasks.ts
Line: 454:454

Comment:
Removed critical data synchronization effect that syncs React Query data with Jotai state. The original code had:

```typescript
useConditionalUpdateEffect(
  () => {
    if (teamTasksQuery.data?.items) {
      deepCheckAndUpdateTasks(teamTasksQuery.data.items, true);
    }
  },
  [teamTasksQuery.data?.items],
  Boolean(tasks?.length)
);
```

This effect ensures that when React Query fetches new data, it's synchronized with the Jotai store. Without it, components relying on the Jotai state won't see updates from the server.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-teams/pull/4245#discussion_r2755604568,,2026-02-02T18:03:22+00:00,data_integrity,high,"This catch identifies a critical data synchronization issue where the refactoring removed the effect that syncs React Query data with Jotai state. Without this synchronization, components relying on Jotai state won't see server updates, causing stale data to be displayed to users in normal usage. This is a real functional bug that would affect all users.",2026-02-02T18:27:23.494164+00:00
constellation-labs/tessellation,1410,fix: fixing wrong incremental mpt update and optimizing batch insertion,https://github.com/Constellation-Labs/tessellation/pull/1410,"Grouping by first nibble doesn't provide the optimization claimed in the comment. The sequential `foldLeftM` on line 66-72 still processes entries one at a time, and grouping them by first nibble only helps if multiple entries have the same prefix. However, this grouping adds overhead for the common case where entries have different prefixes. Consider whether this optimization is measurably beneficial for your workload.

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: modules/shared/src/main/scala/io/constellationnetwork/schema/mpt/IncrementalTrieOps.scala
Line: 53:63

Comment:
Grouping by first nibble doesn't provide the optimization claimed in the comment. The sequential `foldLeftM` on line 66-72 still processes entries one at a time, and grouping them by first nibble only helps if multiple entries have the same prefix. However, this grouping adds overhead for the common case where entries have different prefixes. Consider whether this optimization is measurably beneficial for your workload.

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Constellation-Labs/tessellation/pull/1410#discussion_r2755338377,"The comment is right, this is a hallucination because the keys all share a common `0` starting char
```

  /** Insert multiple entries sequentially.
    *
    * Note: Parallelization by prefix grouping is not effective here because GlobalStateKey serialization
    * (see GlobalStateKey.toHex) produces keys where the first nibble is always 0 (namespace type bytes
    * 0x00, 0x01, 0x02 all start with '0'). This means all entries would fall into the same group,
    * making prefix-based batching a no-op with added overhead.
    */
  def insertMultiple[F[_]: Async: Hasher](
    root: MerklePatriciaNode,
    entries: List[(Hex, Hash)]
  ): F[MerklePatriciaNode] =
    entries.foldLeftM(root) { case (node, (key, hash)) => insert(node, key, hash) }
```
---
I've marked wrongly as resolved, I've rolled back this",2026-02-02T16:57:10+00:00,performance,medium,"Greptile correctly identified that the optimization of grouping by first nibble would be ineffective. The developer confirmed this was a 'hallucination' because all keys share a common '0' starting character due to GlobalStateKey serialization, making the prefix-based batching a no-op with added overhead. The code was rolled back based on this catch.",2026-02-02T18:27:23.494202+00:00
alpaca-network/gatewayz-backend,1024,Autofix Backend / #1018: Migrate to model_name and centralized credits,https://github.com/Alpaca-Network/gatewayz-backend/pull/1024,"<details>
<summary>Additional Comments (2)</summary>

**`src/db/chat_completion_requests.py`**
[P1] Still references removed pricing_* columns in model search results

`search_models_with_chat_summary` now notes `models.model_id` was removed, but it still reads `model.get('pricing_prompt')` / `model.get('pricing_completion')`. If those columns were dropped along with the migration to `model_pricing`, this will always return `None` (or raise if select no longer includes them), and any callers expecting pricing will regress.

This should source pricing from the `model_pricing` relationship (similar to `failover_db.py`) or remove these fields from the response if they're no longer part of the schema.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/db/chat_completion_requests.py
Line: 964:974

Comment:
[P1] Still references removed pricing_* columns in model search results

`search_models_with_chat_summary` now notes `models.model_id` was removed, but it still reads `model.get('pricing_prompt')` / `model.get('pricing_completion')`. If those columns were dropped along with the migration to `model_pricing`, this will always return `None` (or raise if select no longer includes them), and any callers expecting pricing will regress.

This should source pricing from the `model_pricing` relationship (similar to `failover_db.py`) or remove these fields from the response if they're no longer part of the schema.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`src/routes/chat.py`**
[P2] Tracing cost uses sync `calculate_cost` while billing uses async pricing

In the main request path, the trace cost is computed via `calculate_cost(...)` (sync) while actual billing now goes through `handle_credits_and_usage` → `calculate_cost_async`. If live pricing or DB pricing differs from cache/default, traces can diverge from billed cost, which makes audit/debug harder.

Consider computing trace cost via the same pricing path used for billing, or explicitly marking trace cost as an estimate.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/routes/chat.py
Line: 2639:2651

Comment:
[P2] Tracing cost uses sync `calculate_cost` while billing uses async pricing

In the main request path, the trace cost is computed via `calculate_cost(...)` (sync) while actual billing now goes through `handle_credits_and_usage` → `calculate_cost_async`. If live pricing or DB pricing differs from cache/default, traces can diverge from billed cost, which makes audit/debug harder.

Consider computing trace cost via the same pricing path used for billing, or explicitly marking trace cost as an estimate.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/Alpaca-Network/gatewayz-backend/pull/1024#issuecomment-3836582344,,2026-02-02T17:17:08+00:00,data_integrity,high,"The comment identifies that `search_models_with_chat_summary` still references `pricing_prompt` and `pricing_completion` columns that were removed in the migration to `model_pricing`. This will cause pricing data to always return `None`, breaking any callers expecting pricing information. This is a concrete regression from the migration that will affect users in normal usage.",2026-02-02T18:27:23.494255+00:00
posthog/posthog-dotnet,150,"feat: SDK compliance - retry logic, gzip compression, and UUID generation",https://github.com/PostHog/posthog-dotnet/pull/150,"The `finally` block disposes `response` (line 108) even when returning from the success path, potentially closing the stream from `ReadAsStreamAsync` before deserialization completes in `DeserializeFromCamelCaseJsonAsync`.

Move response disposal to after deserialization by wrapping the success path with `using (response)`, then skip disposal in the finally block for this path. This ensures the stream remains open during deserialization.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/PostHog/Library/HttpClientExtensions.cs
Line: 81:87

Comment:
The `finally` block disposes `response` (line 108) even when returning from the success path, potentially closing the stream from `ReadAsStreamAsync` before deserialization completes in `DeserializeFromCamelCaseJsonAsync`.

Move response disposal to after deserialization by wrapping the success path with `using (response)`, then skip disposal in the finally block for this path. This ensures the stream remains open during deserialization.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog-dotnet/pull/150#discussion_r2755460252,,2026-02-02T17:25:42+00:00,resource_leak,high,This is a legitimate concurrency/resource issue where the finally block disposes the HttpResponseMessage before the stream from ReadAsStreamAsync can be fully consumed for deserialization. This could cause runtime errors or data corruption in normal usage when processing successful API responses. The analysis correctly identifies the timing issue between stream access and disposal.,2026-02-02T18:27:23.494330+00:00
rapidsai/cugraph-gnn,395,restore conda-python-tests on CUDA 13,https://github.com/rapidsai/cugraph-gnn/pull/395,"Version constraint mismatch: `dependencies.yaml:417` specifies `pytorch_geometric>=2.5,<2.8` but this file uses `<2.7`

```suggestion
    - pytorch_geometric >=2.5,<2.8
```",https://github.com/rapidsai/cugraph-gnn/pull/395#discussion_r2755253467,"You're right, looks like we missed that in #360

Updated in https://github.com/rapidsai/cugraph-gnn/pull/395/commits/e67bef10fa047e18ff2f02a894b15a5812299e95",2026-02-02T16:38:54+00:00,data_integrity,medium,"Greptile correctly identified a version constraint mismatch between dependencies.yaml and recipe.yaml that would cause inconsistent behavior across build configurations. The developer confirmed this was indeed missed in a previous PR and fixed it. While not a critical bug, catching dependency version inconsistencies prevents potential compatibility issues in production environments.",2026-02-02T18:27:23.494364+00:00
huskit/website-agent,30,Feat/intergrate langfuse,https://github.com/HuskIT/website-agent/pull/30,"Breaking change: response structure changed from returning `data: BusinessData` to markdown-only format. `CreateProjectDialog.tsx:214` still expects `result.data` and will break.

```suggestion
    // Return response with both markdown AND business data for backward compatibility
    return json(
      {
        success: true,
        session_id: sessionId,
        data: result.data, // Keep for CreateProjectDialog.tsx
        google_maps_markdown: googleMapsMarkdown,
        website_markdown: websiteMarkdown,
        has_website: !!crawledWebsiteUrl,
      },
      { status: 200 },
    );
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: app/routes/api.crawler.extract.ts
Line: 323:333

Comment:
Breaking change: response structure changed from returning `data: BusinessData` to markdown-only format. `CreateProjectDialog.tsx:214` still expects `result.data` and will break.

```suggestion
    // Return response with both markdown AND business data for backward compatibility
    return json(
      {
        success: true,
        session_id: sessionId,
        data: result.data, // Keep for CreateProjectDialog.tsx
        google_maps_markdown: googleMapsMarkdown,
        website_markdown: websiteMarkdown,
        has_website: !!crawledWebsiteUrl,
      },
      { status: 200 },
    );
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/HuskIT/website-agent/pull/30#discussion_r2755186253,,2026-02-02T16:24:46+00:00,logic,high,Comment 0 identifies a breaking API change where the response structure was modified to remove `result.data` that `CreateProjectDialog.tsx:214` depends on. This will cause the frontend to break in normal usage when trying to access the now-missing property. The reviewer correctly identified the backward compatibility issue and provided a concrete fix.,2026-02-02T18:27:23.494385+00:00
posthog/posthog,46598,fix(flags): Validate flag matching operators on create,https://github.com/PostHog/posthog/pull/46598,"`SUPPORTED_OPERATORS` is missing `PropertyOperator.SEMVER_CARET` and `PropertyOperator.SEMVER_WILDCARD` which are present in the Rust `OperatorType` enum (property_models.rs:24-25). This will cause flags using these operators to be rejected even though they're supported by the evaluation service.

```suggestion
    # Validate all operators are supported by remote feature flag evaluation
    # Keep in sync with `OperatorType` enum in rust/feature-flags/src/properties/property_models.rs
    SUPPORTED_OPERATORS = {
        PropertyOperator.EXACT,
        PropertyOperator.IS_NOT,
        PropertyOperator.ICONTAINS,
        PropertyOperator.NOT_ICONTAINS,
        PropertyOperator.REGEX,
        PropertyOperator.NOT_REGEX,
        PropertyOperator.GT,
        PropertyOperator.GTE,
        PropertyOperator.LT,
        PropertyOperator.LTE,
        PropertyOperator.IS_SET,
        PropertyOperator.IS_NOT_SET,
        PropertyOperator.IS_DATE_EXACT,
        PropertyOperator.IS_DATE_BEFORE,
        PropertyOperator.IS_DATE_AFTER,
        PropertyOperator.IN_,
        PropertyOperator.NOT_IN,
        PropertyOperator.FLAG_EVALUATES_TO,
        PropertyOperator.SEMVER_EQ,
        PropertyOperator.SEMVER_NEQ,
        PropertyOperator.SEMVER_GT,
        PropertyOperator.SEMVER_GTE,
        PropertyOperator.SEMVER_LT,
        PropertyOperator.SEMVER_LTE,
        PropertyOperator.SEMVER_TILDE,
        PropertyOperator.SEMVER_CARET,
        PropertyOperator.SEMVER_WILDCARD,
        None,  # operator can be omitted (defaults to ""exact"")
    }


<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/feature_flag.py
Line: 388:415

Comment:
`SUPPORTED_OPERATORS` is missing `PropertyOperator.SEMVER_CARET` and `PropertyOperator.SEMVER_WILDCARD` which are present in the Rust `OperatorType` enum (property_models.rs:24-25). This will cause flags using these operators to be rejected even though they're supported by the evaluation service.

```suggestion
    # Validate all operators are supported by remote feature flag evaluation
    # Keep in sync with `OperatorType` enum in rust/feature-flags/src/properties/property_models.rs
    SUPPORTED_OPERATORS = {
        PropertyOperator.EXACT,
        PropertyOperator.IS_NOT,
        PropertyOperator.ICONTAINS,
        PropertyOperator.NOT_ICONTAINS,
        PropertyOperator.REGEX,
        PropertyOperator.NOT_REGEX,
        PropertyOperator.GT,
        PropertyOperator.GTE,
        PropertyOperator.LT,
        PropertyOperator.LTE,
        PropertyOperator.IS_SET,
        PropertyOperator.IS_NOT_SET,
        PropertyOperator.IS_DATE_EXACT,
        PropertyOperator.IS_DATE_BEFORE,
        PropertyOperator.IS_DATE_AFTER,
        PropertyOperator.IN_,
        PropertyOperator.NOT_IN,
        PropertyOperator.FLAG_EVALUATES_TO,
        PropertyOperator.SEMVER_EQ,
        PropertyOperator.SEMVER_NEQ,
        PropertyOperator.SEMVER_GT,
        PropertyOperator.SEMVER_GTE,
        PropertyOperator.SEMVER_LT,
        PropertyOperator.SEMVER_LTE,
        PropertyOperator.SEMVER_TILDE,
        PropertyOperator.SEMVER_CARET,
        PropertyOperator.SEMVER_WILDCARD,
        None,  # operator can be omitted (defaults to ""exact"")
    }


How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46598#discussion_r2756264013,argh I thought I'd fixed this!,2026-02-02T22:07:22+00:00,logic,medium,"Greptile correctly identified that the SUPPORTED_OPERATORS set was missing SEMVER_CARET and SEMVER_WILDCARD operators that are supported in the Rust evaluation service. This would cause valid feature flags using these operators to be incorrectly rejected during validation. The developer confirmed this was a real bug with 'argh I thought I'd fixed this!', validating the catch.",2026-02-03T01:00:18.068635+00:00
greptileai/react-mirror,452,I added a test that asserts malformed profiling operations do not throw and that errors/warnings are logged,https://github.com/greptileai/react-mirror/pull/452,"using `break` here without advancing `i` past all ADD operation parameters will cause subsequent parameters to be misinterpreted as operation codes. For non-root nodes, there are still 5+ parameters to skip (parentID, ownerID, displayNameStringID, keyStringID, name prop). This will corrupt the operation stream parsing.

need to advance `i` to skip remaining parameters before breaking, similar to how the normal ADD case increments `i` throughout lines 234-248

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react-devtools-shared/src/devtools/views/Profiler/CommitTreeBuilder.js
Line: 202:207

Comment:
using `break` here without advancing `i` past all ADD operation parameters will cause subsequent parameters to be misinterpreted as operation codes. For non-root nodes, there are still 5+ parameters to skip (parentID, ownerID, displayNameStringID, keyStringID, name prop). This will corrupt the operation stream parsing.

need to advance `i` to skip remaining parameters before breaking, similar to how the normal ADD case increments `i` throughout lines 234-248

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/452#discussion_r2756449165,,2026-02-02T23:24:13+00:00,logic,high,"Greptile correctly identifies that using `break` without advancing the index `i` past the remaining ADD operation parameters will cause stream corruption. The operation stream uses variable-length parameter sequences, and early breaking without consuming all parameters will cause subsequent bytes to be misinterpreted as operation codes, leading to unpredictable behavior and likely crashes. This is a real bug that would affect users during profiling with malformed operations.",2026-02-03T01:00:18.068782+00:00
infisical/infisical,5346,feat(pki): add slack webhook to alerts,https://github.com/Infisical/infisical/pull/5346,"Missing SSRF (Server-Side Request Forgery) protection. The Slack webhook URL is not validated against private/internal IP addresses before making the HTTP request.

**Security Risk:** An attacker could configure a Slack webhook URL pointing to internal services (e.g., internal metadata endpoints, database servers, or other private infrastructure). This could expose sensitive internal infrastructure.

**How to exploit:** Create a PKI alert with a malicious Slack webhook URL that resolves to an internal IP address, bypassing the `hooks.slack.com` domain check through DNS rebinding or URL manipulation.

**Fix:** Use the existing `blockLocalAndPrivateIpAddresses` function before making webhook requests. This function is already used in other parts of the codebase (e.g., `backend/src/services/app-connection/github/github-connection-fns.ts`).

```typescript
import { blockLocalAndPrivateIpAddresses } from ""@app/lib/validator/validate-url"";

// Before triggerSlackWebhook
await blockLocalAndPrivateIpAddresses(config.webhookUrl);
```",https://github.com/Infisical/infisical/pull/5346#discussion_r2756158957,Fixed - added `blockLocalAndPrivateIpAddresses()` check before triggering webhook.,2026-02-02T21:21:26+00:00,security,critical,"This is a legitimate SSRF vulnerability catch. The code was accepting user-provided Slack webhook URLs and making HTTP requests to them without validating they don't point to internal/private IP addresses. An attacker could exploit this to access internal infrastructure, metadata endpoints, or other private services. The developer confirmed and fixed the issue by adding the `blockLocalAndPrivateIpAddresses()` check.",2026-02-03T01:00:18.068855+00:00
ethyca/fides,7292,ENG-2338 - Configure when Banners resurface,https://github.com/ethyca/fides/pull/7292,"enum values use uppercase ('REJECT', 'DISMISS') but Python model uses lowercase ('reject', 'dismiss'). This mismatch will cause runtime errors.

```suggestion
    op.execute(
        ""CREATE TYPE resurfacebehavior AS ENUM ('reject', 'dismiss')""
    )
```",https://github.com/ethyca/fides/pull/7292#discussion_r2755891925,,2026-02-02T19:33:20+00:00,runtime,high,"Comments 0-3 all identify the same critical issue: the database migration creates an enum with uppercase values ('REJECT', 'DISMISS') but the Python model uses lowercase values ('reject', 'dismiss'). This mismatch WILL cause runtime errors when the application tries to store/retrieve these enum values in production. Comment 0 is the best-written as it explains the root cause and provides the fix at the enum creation point.",2026-02-03T01:00:18.068916+00:00
nvidia/nvflare,4096,[2.7] Job CLI Tutorial Fixes,https://github.com/NVIDIA/NVFlare/pull/4096,"Loss averaging logic appears incorrect. `running_loss` accumulates losses but is divided by 3000 regardless of actual iterations. At iteration 0, this divides a very small accumulated loss by 3000. Should track actual number of iterations since last print or reset running_loss after each print.",https://github.com/NVIDIA/NVFlare/pull/4096#discussion_r2756019680,fixed by only computing loss at end of epoch,2026-02-02T20:23:47+00:00,logic,medium,"Comment 0 correctly identifies a loss averaging bug where running_loss is divided by 3000 regardless of actual iterations, producing incorrect loss values especially at iteration 0. The developer confirmed the issue with 'fixed by only computing loss at end of epoch'. While this is tutorial code, it's a real logic bug that would cause misleading training metrics.",2026-02-03T01:00:18.069001+00:00
ever-co/ever-gauzy,9406,[Feat] Implement load more functionality,https://github.com/ever-co/ever-gauzy/pull/9406,"Initial load uses `skip: 1` which skips the first item

Standard pagination starts at `skip: 0` to get items 0-9. Using `skip: 1` will skip the first plugin:

```suggestion
					this.loadPendingPlugins(plugins, { skip: 0, take: 10 }).pipe(
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/desktop-ui-lib/src/lib/settings/plugins/component/+state/pending-installation.effect.ts
Line: 140:140

Comment:
Initial load uses `skip: 1` which skips the first item

Standard pagination starts at `skip: 0` to get items 0-9. Using `skip: 1` will skip the first plugin:

```suggestion
					this.loadPendingPlugins(plugins, { skip: 0, take: 10 }).pipe(
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-gauzy/pull/9406#discussion_r2755740981,,2026-02-02T18:41:06+00:00,logic,high,"Comments 2, 3, and 0 all relate to the same pagination bug where skip starts at 1 instead of 0. Comment 2 best identifies the user-facing impact: using skip: 1 on initial load will skip the first plugin entirely, meaning users will never see the first item in the list. This is a real bug that WILL affect all users in normal usage.",2026-02-03T01:00:18.069038+00:00
openreplay/openreplay,4287,fix(chalice): fixed get event's properties,https://github.com/openreplay/openreplay/pull/4287,"INNER JOIN only on `property_name` could match properties from different projects. Consider adding `project_id` to the join condition:

```suggestion
                        INNER JOIN product_analytics.all_properties USING (project_id, property_name)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: api/chalicelib/core/product_analytics/properties.py
Line: 325:325

Comment:
INNER JOIN only on `property_name` could match properties from different projects. Consider adding `project_id` to the join condition:

```suggestion
                        INNER JOIN product_analytics.all_properties USING (project_id, property_name)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4287#discussion_r2756042249,,2026-02-02T20:32:56+00:00,data_integrity,high,"The INNER JOIN on only `property_name` without `project_id` could cause cross-tenant data leakage in a multi-tenant system, returning properties from other projects. This is a real bug that would affect users in production - properties with the same name across different projects would incorrectly be joined together, leading to wrong data being returned.",2026-02-03T01:00:18.069087+00:00
openreplay/openreplay,4286,fix(chalice): fixed get event's properties,https://github.com/openreplay/openreplay/pull/4286,"`all_properties_customized` doesn't have a `value_type` column. Should be `event_properties.value_type`

```suggestion
                      array_agg(DISTINCT event_properties.value_type) AS possible_types
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: api/chalicelib/core/product_analytics/properties.py
Line: 323:323

Comment:
`all_properties_customized` doesn't have a `value_type` column. Should be `event_properties.value_type`

```suggestion
                      array_agg(DISTINCT event_properties.value_type) AS possible_types
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4286#discussion_r2756012891,,2026-02-02T20:21:04+00:00,runtime,high,"Greptile correctly identified that the PR introduced a bug by referencing `all_properties_customized.value_type` when this column doesn't exist on that table - it should be `event_properties.value_type`. This would cause a SQL query error at runtime when users try to get event properties, breaking core functionality.",2026-02-03T01:00:18.069135+00:00
alpaca-network/gatewayz-backend,1027,Audit API credit deductions and provider-based image pricing,https://github.com/Alpaca-Network/gatewayz-backend/pull/1027,"<details>
<summary>Additional Comments (3)</summary>

**`src/routes/images.py`**
[P0] Tracing cost fields are now inconsistent with actual image pricing

`trace_ctx.set_cost(cost)` still uses the old `tokens_charged = 100 * req.n` math (`images.py:270-272`), but credits are deducted using `total_cost` from `get_image_cost()` (`images.py:292-305`). This will make tracing/observability report a different cost than billing/usage, which will break cost dashboards and audits for images.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/routes/images.py
Line: 266:273

Comment:
[P0] Tracing cost fields are now inconsistent with actual image pricing

`trace_ctx.set_cost(cost)` still uses the old `tokens_charged = 100 * req.n` math (`images.py:270-272`), but credits are deducted using `total_cost` from `get_image_cost()` (`images.py:292-305`). This will make tracing/observability report a different cost than billing/usage, which will break cost dashboards and audits for images.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`src/routes/images.py`**
[P2] ThreadPoolExecutor is created per-request and never used as a context manager

`executor = ThreadPoolExecutor()` is created for every request (`images.py:148`) and only shut down with `wait=False` in `finally`. This can cause unnecessary thread churn under load; consider using a shared executor or `with ThreadPoolExecutor() as executor:` so threads are deterministically joined.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/routes/images.py
Line: 147:149

Comment:
[P2] ThreadPoolExecutor is created per-request and never used as a context manager

`executor = ThreadPoolExecutor()` is created for every request (`images.py:148`) and only shut down with `wait=False` in `finally`. This can cause unnecessary thread churn under load; consider using a shared executor or `with ThreadPoolExecutor() as executor:` so threads are deterministically joined.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>

---

**`tests/routes/test_images.py`**
[P0] Tests still assert old token-based billing fields and will fail after this PR

The route now returns `gateway_usage.cost_usd`/`cost_per_image` and deducts `total_cost`, but success-path tests still assert `gateway_usage['tokens_charged'] == 100` and expect `deduct_credits(..., 100)` (`test_images.py:158-166`, also `:224-229`, `:292-300`, `:590-601`). These assertions no longer match the updated API contract and deduction behavior introduced in this PR.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: tests/routes/test_images.py
Line: 158:166

Comment:
[P0] Tests still assert old token-based billing fields and will fail after this PR

The route now returns `gateway_usage.cost_usd`/`cost_per_image` and deducts `total_cost`, but success-path tests still assert `gateway_usage['tokens_charged'] == 100` and expect `deduct_credits(..., 100)` (`test_images.py:158-166`, also `:224-229`, `:292-300`, `:590-601`). These assertions no longer match the updated API contract and deduction behavior introduced in this PR.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/Alpaca-Network/gatewayz-backend/pull/1027#issuecomment-3837541902,,2026-02-02T21:47:52+00:00,data_integrity,high,"Greptile identified a real inconsistency where tracing/observability uses the old hardcoded cost calculation (100 * req.n) while billing now uses the new dynamic pricing from get_image_cost(). This will cause cost dashboards and audits to show incorrect data that doesn't match actual billing, which is a significant data integrity issue affecting production observability.",2026-02-03T01:00:18.069180+00:00
beanie-brick-band/leopard,100,feat(vscode-extension): move convex url from env to vscode settings,https://github.com/Beanie-Brick-Band/leopard/pull/100,"<details>
<summary>Additional Comments (1)</summary>

**`apps/vscode-extension/src/env.ts`**
Deleting this file breaks `extension.ts` which still imports and uses it on lines 9 and 18. Need to update `extension.ts` to use the new VS Code configuration:

```typescript
const config = vscode.workspace.getConfiguration('leopard');
const convexUrl = config.get<string>('convexUrl');
const client = new ConvexHttpClient(convexUrl);
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/vscode-extension/src/env.ts
Line: 1:7

Comment:
Deleting this file breaks `extension.ts` which still imports and uses it on lines 9 and 18. Need to update `extension.ts` to use the new VS Code configuration:

```typescript
const config = vscode.workspace.getConfiguration('leopard');
const convexUrl = config.get<string>('convexUrl');
const client = new ConvexHttpClient(convexUrl);
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/Beanie-Brick-Band/leopard/pull/100#issuecomment-3837831363,,2026-02-02T23:36:50+00:00,runtime,high,"Greptile correctly identified that deleting env.ts breaks extension.ts which still imports from it on lines 9 and 18. This would cause a guaranteed runtime error when the extension loads, completely breaking the VS Code extension functionality. This is a straightforward but important catch that prevents a broken build/runtime.",2026-02-03T01:00:18.069264+00:00
nvidia/dgx-cloud-examples,40,fix: OIDC token expiration for long-running deployments,https://github.com/NVIDIA/dgx-cloud-examples/pull/40,"The API call here doesn't use the `_with_retry()` wrapper, so if the reloaded config still has an expired token, this will throw a 401 and mark auth as expired. However, the whole point of the periodic refresh is to proactively prevent expiration. Consider whether this should attempt retry to align with the reactive 401 handling elsewhere

```suggestion
                _analyzer.pvc_service.k8s._with_retry(lambda: _analyzer.pvc_service.k8s.core_v1.list_namespace(limit=1))
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>",https://github.com/NVIDIA/dgx-cloud-examples/pull/40#discussion_r2755757679,"Good point — addressed in commit `cfa9f48`.

The periodic refresh verification now uses `K8sClient._with_retry(lambda: core_v1.list_namespace(limit=1))` after `_reload_config()`. This aligns the proactive refresh verification with the same retry semantics used by normal request paths.",2026-02-02T18:46:27+00:00,logic,medium,"Greptile correctly identified an inconsistency in the token refresh implementation - the periodic refresh verification doesn't use the retry wrapper, meaning it could fail even after config reload if the token is still expired. The developer confirmed this was a valid catch and fixed it in a follow-up commit, showing it was a real issue that would have affected the OIDC token refresh functionality in edge cases.",2026-02-03T01:00:18.069310+00:00
traceroot-ai/traceroot,407,[AgentOps] Support membership invite [1/n],https://github.com/traceroot-ai/traceroot/pull/407,"XSS vulnerability - user-provided data (`inviterName`, `inviterEmail`, `workspaceName`) is directly interpolated into HTML without escaping. If an attacker sets their name to `<script>alert('xss')</script>`, it will execute in the recipient's email client.

```suggestion
function buildHtmlEmail(params: EmailContentParams): string {
  const { inviterName, inviterEmail, workspaceName, acceptLink, roleName } = params;
  
  // Escape HTML to prevent XSS
  const escapeHtml = (text: string) => text
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/""/g, '&quot;')
    .replace(/'/g, '&#x27;');

  return `
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""utf-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
  </head>
  <body style=""font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.6; color: #333; margin: 0; padding: 20px; background-color: #fafafa;"">
    <table cellpadding=""0"" cellspacing=""0"" border=""0"" width=""100%"" style=""max-width: 480px; margin: 0 auto; background: #fff; border: 1px solid #e5e5e5;"">
      <!-- Logo section -->
      <tr>
        <td style=""padding: 40px 40px 32px 40px; text-align: center;"">
          <img src=""https://raw.githubusercontent.com/traceroot-ai/traceroot/pivot/agentops/ui/public/images/icon.png"" alt=""TraceRoot"" width=""72"" height=""72"" style=""display: block; margin: 0 auto; border-radius: 14px;"" />
        </td>
      </tr>

      <!-- Title -->
      <tr>
        <td style=""padding: 0 40px 24px 40px; text-align: center;"">
          <h1 style=""font-size: 24px; font-weight: 600; margin: 0; color: #000; letter-spacing: -0.5px;"">
            Join ${escapeHtml(workspaceName)} on TraceRoot
          </h1>
        </td>
      </tr>

      <!-- Body -->
      <tr>
        <td style=""padding: 0 40px 32px 40px;"">
          <p style=""margin: 0; color: #333; font-size: 15px; line-height: 1.6; text-align: center;"">
            <strong>${escapeHtml(inviterName)}</strong> (${escapeHtml(inviterEmail)}) has invited you to join the <strong>${escapeHtml(workspaceName)}</strong> workspace as a <strong>${escapeHtml(roleName)}</strong>.
          </p>
        </td>
      </tr>

      <!-- Button -->
      <tr>
        <td style=""padding: 0 40px 40px 40px; text-align: center;"">
          <table cellpadding=""0"" cellspacing=""0"" border=""0"" style=""margin: 0 auto;"">
            <tr>
              <td style=""background-color: #000;"">
                <a href=""${escapeHtml(acceptLink)}"" style=""display: inline-block; padding: 10px 20px; color: #ffffff; text-decoration: none; font-weight: 500; font-size: 14px;"">
                  Accept Invitation
                </a>
              </td>
            </tr>
          </table>
        </td>
      </tr>

      <!-- Divider -->
      <tr>
        <td style=""border-top: 1px solid #e5e5e5;""></td>
      </tr>

      <!-- Footer -->
      <tr>
        <td style=""padding: 24px 40px; background-color: #fafafa;"">
          <p style=""color: #999; font-size: 12px; margin: 0; text-align: center;"">
            If you were not expecting this invitation, you can ignore this email.
          </p>
        </td>
      </tr>
    </table>
  </body>
</html>
  `.trim();
}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/lib/email/send-invite-email.ts
Line: 68:136

Comment:
XSS vulnerability - user-provided data (`inviterName`, `inviterEmail`, `workspaceName`) is directly interpolated into HTML without escaping. If an attacker sets their name to `<script>alert('xss')</script>`, it will execute in the recipient's email client.

```suggestion
function buildHtmlEmail(params: EmailContentParams): string {
  const { inviterName, inviterEmail, workspaceName, acceptLink, roleName } = params;
  
  // Escape HTML to prevent XSS
  const escapeHtml = (text: string) => text
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/""/g, '&quot;')
    .replace(/'/g, '&#x27;');

  return `
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""utf-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
  </head>
  <body style=""font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.6; color: #333; margin: 0; padding: 20px; background-color: #fafafa;"">
    <table cellpadding=""0"" cellspacing=""0"" border=""0"" width=""100%"" style=""max-width: 480px; margin: 0 auto; background: #fff; border: 1px solid #e5e5e5;"">
      <!-- Logo section -->
      <tr>
        <td style=""padding: 40px 40px 32px 40px; text-align: center;"">
          <img src=""https://raw.githubusercontent.com/traceroot-ai/traceroot/pivot/agentops/ui/public/images/icon.png"" alt=""TraceRoot"" width=""72"" height=""72"" style=""display: block; margin: 0 auto; border-radius: 14px;"" />
        </td>
      </tr>

      <!-- Title -->
      <tr>
        <td style=""padding: 0 40px 24px 40px; text-align: center;"">
          <h1 style=""font-size: 24px; font-weight: 600; margin: 0; color: #000; letter-spacing: -0.5px;"">
            Join ${escapeHtml(workspaceName)} on TraceRoot
          </h1>
        </td>
      </tr>

      <!-- Body -->
      <tr>
        <td style=""padding: 0 40px 32px 40px;"">
          <p style=""margin: 0; color: #333; font-size: 15px; line-height: 1.6; text-align: center;"">
            <strong>${escapeHtml(inviterName)}</strong> (${escapeHtml(inviterEmail)}) has invited you to join the <strong>${escapeHtml(workspaceName)}</strong> workspace as a <strong>${escapeHtml(roleName)}</strong>.
          </p>
        </td>
      </tr>

      <!-- Button -->
      <tr>
        <td style=""padding: 0 40px 40px 40px; text-align: center;"">
          <table cellpadding=""0"" cellspacing=""0"" border=""0"" style=""margin: 0 auto;"">
            <tr>
              <td style=""background-color: #000;"">
                <a href=""${escapeHtml(acceptLink)}"" style=""display: inline-block; padding: 10px 20px; color: #ffffff; text-decoration: none; font-weight: 500; font-size: 14px;"">
                  Accept Invitation
                </a>
              </td>
            </tr>
          </table>
        </td>
      </tr>

      <!-- Divider -->
      <tr>
        <td style=""border-top: 1px solid #e5e5e5;""></td>
      </tr>

      <!-- Footer -->
      <tr>
        <td style=""padding: 24px 40px; background-color: #fafafa;"">
          <p style=""color: #999; font-size: 12px; margin: 0; text-align: center;"">
            If you were not expecting this invitation, you can ignore this email.
          </p>
        </td>
      </tr>
    </table>
  </body>
</html>
  `.trim();
}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/407#discussion_r2756486103,fixed.,2026-02-02T23:43:54+00:00,security,high,"This is a legitimate XSS vulnerability where user-provided data (inviterName, inviterEmail, workspaceName) is directly interpolated into HTML email content without escaping. While email client XSS is generally more limited than browser XSS, it's still a real security concern. The developer confirmed and fixed the issue, validating the catch.",2026-02-03T01:00:18.069345+00:00
browserbase/stagehand,1650,feat: prefer CSS selectors over XPath in resolveSelector,https://github.com/browserbase/stagehand/pull/1650,"CSS selectors in `cssMap` are per-frame only and cannot traverse iframes like XPath. When an element is inside an iframe, the CSS selector will fail when called from the root page context, because CSS selectors aren't prefixed with iframe paths during snapshot capture (see `capture.ts:446`). 

The `deepLocator` only treats selectors starting with `/` or `xpath=` as XPath that can cross iframe boundaries. A plain CSS selector like `#button-id` will only search in the current frame.

Consider checking if the CSS selector contains iframe navigation (e.g., `>>` notation) or if the ref is known to be in an iframe before preferring CSS.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/index.ts
Line: 749:752

Comment:
CSS selectors in `cssMap` are per-frame only and cannot traverse iframes like XPath. When an element is inside an iframe, the CSS selector will fail when called from the root page context, because CSS selectors aren't prefixed with iframe paths during snapshot capture (see `capture.ts:446`). 

The `deepLocator` only treats selectors starting with `/` or `xpath=` as XPath that can cross iframe boundaries. A plain CSS selector like `#button-id` will only search in the current frame.

Consider checking if the CSS selector contains iframe navigation (e.g., `>>` notation) or if the ref is known to be in an iframe before preferring CSS.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/browserbase/stagehand/pull/1650#discussion_r2756657312,,2026-02-03T00:58:55+00:00,logic,high,"The comment identifies a real functional bug where CSS selectors cannot traverse iframes like XPath can. When elements are inside iframes, preferring CSS selectors will cause the selector to fail since CSS only searches in the current frame context. This would break functionality for any page using iframes, which is common in web applications.",2026-02-03T03:15:16.253241+00:00
ethyca/fides,7284,Add RBAC management UI with alpha feature flag,https://github.com/ethyca/fides/pull/7284,"[P1] `selectThisUsersScopes` treats an empty RBAC permission set as “not loaded” and silently falls back to legacy permissions, which can grant UI access when RBAC is enabled but the user legitimately has 0 permissions (or the RBAC endpoint returns `[]`). This is most likely for brand-new users/roles or when the RBAC service is up but returns no scopes.

Concretely, the gate `if (rbacPermissions && rbacPermissions.length > 0)` means `[]` will fall back to `user/{id}/permission`, potentially rendering screens based on legacy scopes even though RBAC is enabled.

Suggestion: distinguish “query not loaded / errored” from “loaded empty”, and if RBAC is enabled + query succeeded, return the RBAC permissions even if empty.

Also appears in: `clients/admin-ui/src/features/common/CommonSubscriptions.tsx` (RBAC prefetch assumes selector uses cache).",https://github.com/ethyca/fides/pull/7284#discussion_r2757017517,,2026-02-03T03:32:15+00:00,logic,high,This catch identifies a critical authorization logic flaw where an empty RBAC permissions array (legitimate case for users with no permissions) is incorrectly treated as 'not loaded' and falls back to legacy permissions. This could grant unauthorized UI access to users who should have zero permissions under the new RBAC system - a real security-adjacent bug that would affect users in normal production usage when RBAC is enabled.,2026-02-03T04:45:37.634952+00:00
posthog/posthog,46633,feat: add Enhance SDK,https://github.com/PostHog/posthog/pull/46633,"Loading remote JavaScript via `importScripts()` from an external CDN without Subresource Integrity (SRI) checks creates a critical security vulnerability. If the CDN is compromised or the response is intercepted, arbitrary malicious code could be executed with service worker privileges, potentially compromising the entire application and user data.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/public/enhance-sw.js
Line: 3:3

Comment:
Loading remote JavaScript via `importScripts()` from an external CDN without Subresource Integrity (SRI) checks creates a critical security vulnerability. If the CDN is compromised or the response is intercepted, arbitrary malicious code could be executed with service worker privileges, potentially compromising the entire application and user data.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46633#discussion_r2757290409,,2026-02-03T05:39:05+00:00,security,critical,"Loading remote JavaScript via importScripts() in a service worker without SRI checks is a legitimate critical security vulnerability. Service workers have elevated privileges and can intercept all network requests. If the CDN is compromised, attackers could execute arbitrary code with full access to intercept and modify all application traffic.",2026-02-03T06:32:37.323347+00:00
posthog/posthog,46640,ci(storybook): add diagnostic logging for webkit timeout issues,https://github.com/PostHog/posthog/pull/46640,"timing measurement won't capture `page.goto` timeout

the load event listener is registered before `page.goto` is called, but if `page.goto` times out (the issue being diagnosed), the load event never fires, so the listener never executes. the measurement will be incomplete for the exact failure case you're trying to diagnose.

to measure `page.goto` time, move this timing code to wrap around the actual `page.goto` call, or add timeout handling:

```suggestion
        // Log timing for webkit tests to diagnose timeout issues
        const browserContext = page.context()
        const browserName = browserContext.browser()?.browserType().name()
        if (browserName === 'webkit' && process.env.CI) {
            const pageLoadStart = Date.now()
            // Listen for load event to measure actual load time
            page.once('load', () => {
                const loadTime = Date.now() - pageLoadStart
                if (loadTime > 15000) {
                    // eslint-disable-next-line no-console
                    console.warn(
                        `[webkit-diagnostics] SLOW page load: ${loadTime}ms for ${context.id} (threshold: 15000ms)`
                    )
                } else if (loadTime > 10000) {
                    // eslint-disable-next-line no-console
                    console.log(`[webkit-diagnostics] Page load: ${loadTime}ms for ${context.id}`)
                }
            })
            // Add timeout warning if load event doesn't fire within 25s
            setTimeout(() => {
                // eslint-disable-next-line no-console
                console.error(`[webkit-diagnostics] page.goto timeout - load event not fired after 25s for ${context.id}`)
            }, 25000)
        }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: common/storybook/.storybook/test-runner.ts
Line: 105:119

Comment:
timing measurement won't capture `page.goto` timeout

the load event listener is registered before `page.goto` is called, but if `page.goto` times out (the issue being diagnosed), the load event never fires, so the listener never executes. the measurement will be incomplete for the exact failure case you're trying to diagnose.

to measure `page.goto` time, move this timing code to wrap around the actual `page.goto` call, or add timeout handling:

```suggestion
        // Log timing for webkit tests to diagnose timeout issues
        const browserContext = page.context()
        const browserName = browserContext.browser()?.browserType().name()
        if (browserName === 'webkit' && process.env.CI) {
            const pageLoadStart = Date.now()
            // Listen for load event to measure actual load time
            page.once('load', () => {
                const loadTime = Date.now() - pageLoadStart
                if (loadTime > 15000) {
                    // eslint-disable-next-line no-console
                    console.warn(
                        `[webkit-diagnostics] SLOW page load: ${loadTime}ms for ${context.id} (threshold: 15000ms)`
                    )
                } else if (loadTime > 10000) {
                    // eslint-disable-next-line no-console
                    console.log(`[webkit-diagnostics] Page load: ${loadTime}ms for ${context.id}`)
                }
            })
            // Add timeout warning if load event doesn't fire within 25s
            setTimeout(() => {
                // eslint-disable-next-line no-console
                console.error(`[webkit-diagnostics] page.goto timeout - load event not fired after 25s for ${context.id}`)
            }, 25000)
        }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46640#discussion_r2757503346,Good catch! Added a 25s timeout warning that logs an error if the load event hasn't fired yet. This will capture the exact failure case where `page.goto` is about to timeout (30s). The warning fires 5s before the timeout to give us diagnostic output before the test fails.,2026-02-03T06:54:11+00:00,logic,medium,"Greptile correctly identified that the diagnostic logging code won't capture the exact failure case it's meant to diagnose - if page.goto times out, the load event never fires, so the listener never executes. The developer confirmed this with 'Good catch!' and implemented the suggested fix. This is a meaningful logic error in diagnostic code that would have made the debugging effort ineffective.",2026-02-03T08:24:47.142399+00:00
democratizedspace/dspace,3354,Fix ci-image workflow heredoc for SHA check,https://github.com/democratizedspace/dspace/pull/3354,"heredoc terminator must be at column 1 without leading spaces. With `<<'NODE'`, bash requires the terminating `NODE` marker to have no leading whitespace. The current indentation will cause bash to not recognize this as the terminator, resulting in ""here-document delimited by end-of-file"" error.

```suggestion
NODE
```",https://github.com/democratizedspace/dspace/pull/3354#discussion_r2757698082,,2026-02-03T07:53:16+00:00,runtime,high,"This is a correct and important catch - bash heredoc terminators must be at column 1 with no leading whitespace, or bash will fail to recognize the end of the heredoc, causing a 'here-document delimited by end-of-file' error. This would cause the CI workflow to fail 100% of the time when this job runs, breaking the build pipeline.",2026-02-03T08:24:47.142489+00:00
mrmps/smry,59,feat: adding ai chatbot feature replacing the our exisiting summary section,https://github.com/mrmps/SMRY/pull/59,"[P0] Rate limiting key can be `undefined` for anonymous requests

`rateLimitKey` is set as `userId || clientIp` (server/routes/chat.ts:85), but `extractClientIp(request)` can be falsy depending on deployment/proxy headers. In that case `dailyRateLimit.limit(rateLimitKey)` will be called with `undefined` (server/routes/chat.ts:91), which can cause Upstash Ratelimit to throw or effectively bucket all such requests together, breaking throttling for anonymous users.

Consider ensuring you always have a non-empty string key (e.g., fallback to something derived from request headers) before calling `.limit()`.
",https://github.com/mrmps/SMRY/pull/59#discussion_r2757710484,,2026-02-03T07:56:17+00:00,security,high,"This catch identifies that the rate limiting key can be undefined when both userId and clientIp are falsy, causing all anonymous requests without identifiable IPs to share the same rate limit bucket. This effectively breaks rate limiting for anonymous users and could allow abuse. This is a real security issue that would affect production users.",2026-02-03T08:24:47.142510+00:00
raycast/extensions,25101,File Content Search: add extension,https://github.com/raycast/extensions/pull/25101,"[P0] Shell command injection via unescaped `path` / exclude values

`buildGrepCommand` returns a single shell string that is executed via `spawn(""sh"", [""-c"", command])` (see `useGrep.ts`). While the query is single-quoted, `path` is only wrapped in double quotes and `excludedDirectories`/`excludedExtensions` are interpolated unquoted. A preference value like `excludedDirectories: ""foo --exclude-dir=$(rm -rf ~)""` or a search path containing `""` / `$()` can break quoting and inject arbitrary shell commands. This matters whenever a user sets these preferences (or picks a directory with special characters).

Consider building the grep invocation with `spawn` args (no shell) or robustly escaping all interpolated values (path + exclude args), not just the query.",https://github.com/raycast/extensions/pull/25101#discussion_r2758321896,,2026-02-03T10:18:40+00:00,security,critical,"This is a genuine shell command injection vulnerability. The code builds a shell command string that's executed via `spawn('sh', ['-c', command])`, and while the query is single-quoted, the `path` parameter and exclude values from user preferences are not properly escaped. A malicious path or preference value containing shell metacharacters like `$()` or backticks could execute arbitrary commands. This is a serious security issue that could affect any user of this extension.",2026-02-03T10:27:09.955750+00:00
