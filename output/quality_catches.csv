repo,pr_number,pr_title,pr_url,comment_body,comment_url,reply_body,created_at,title,bug_category,severity,quality_score,llm_reasoning,evaluated_at
posthog/posthog,46208,fix(sig): re-use ongoing video export workflow,https://github.com/PostHog/posthog/pull/46208,"<details>
<summary>Additional Comments (1)</summary>

**`posthog/temporal/ai/session_summary/activities/a1_export_session_video.py`**
when catching `WorkflowAlreadyStartedError`, the code waits for the existing workflow but then returns the NEW `exported_asset.id` that was never processed. The existing workflow is processing a DIFFERENT asset ID (from the first request). This means the returned asset will never have its `content_location` or `content` populated.

need to either:
1. query for the existing asset (checking lines 46-54) BEFORE creating a new one when a workflow is already running, OR
2. look up and return the existing asset's ID that the already-running workflow is processing

the workflow_id is based on `team_id` and `session_id`, but each request creates its own unique `exported_asset`. when the second request catches the exception, it should return the FIRST request's asset ID, not its own.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/ai/session_summary/activities/a1_export_session_video.py
Line: 130:148

Comment:
when catching `WorkflowAlreadyStartedError`, the code waits for the existing workflow but then returns the NEW `exported_asset.id` that was never processed. The existing workflow is processing a DIFFERENT asset ID (from the first request). This means the returned asset will never have its `content_location` or `content` populated.

need to either:
1. query for the existing asset (checking lines 46-54) BEFORE creating a new one when a workflow is already running, OR
2. look up and return the existing asset's ID that the already-running workflow is processing

the workflow_id is based on `team_id` and `session_id`, but each request creates its own unique `exported_asset`. when the second request catches the exception, it should return the FIRST request's asset ID, not its own.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/PostHog/posthog/pull/46208#issuecomment-3814257126,,2026-01-28T22:28:21+00:00,Wrong Asset ID Returned After WorkflowAlreadyStartedError,logic,high,8,"This is a meaningful logic bug in concurrent workflow handling - when catching WorkflowAlreadyStartedError, the code returns a newly created asset ID that will never be populated with data, while the actual export is happening for a different asset. This would cause silent data integrity issues where users receive empty/unpopulated assets, a non-obvious bug requiring understanding of the temporal workflow semantics.",2026-02-05T22:30:01.283882+00:00
posthog/posthog,46262,feat(logs): json parse log lines in log-ingestion based on team settings,https://github.com/PostHog/posthog/pull/46262,"Double stringification - `value` is already a string from `flattenJson` (line 117-145 converts everything to strings), so `value.toString()` returns the same string, then `JSON.stringify()` adds extra quotes around it. This means a value like `""info""` becomes `""\""info\""""`.

```suggestion
        newAttributes[key] = value
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nodejs/src/logs-ingestion/log-record-avro.ts
Line: 176:176

Comment:
Double stringification - `value` is already a string from `flattenJson` (line 117-145 converts everything to strings), so `value.toString()` returns the same string, then `JSON.stringify()` adds extra quotes around it. This means a value like `""info""` becomes `""\""info\""""`.

```suggestion
        newAttributes[key] = value
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46262#discussion_r2742153502,,2026-01-29T15:14:36+00:00,Double stringification causing extra quotes in attributes,data_integrity,high,8,"This catches a real data corruption bug where double stringification would corrupt log attribute values (turning 'info' into '""info""'). This would affect all logs processed through this ingestion pipeline, causing data quality issues that would be confusing to debug and would corrupt production data.",2026-02-05T22:30:01.285066+00:00
onyx-dot-app/onyx,7977,fix: input masking,https://github.com/onyx-dot-app/onyx/pull/7977,"<details>
<summary>Additional Comments (1)</summary>

**`web/src/app/admin/embeddings/modals/ChangeCredentialsModal.tsx`**
Missing `data-ph-no-capture` attribute. Since PostHog's `maskAllInputs` is now set to `false` (in `providers.tsx:21`), this API key input needs the explicit `data-ph-no-capture` attribute to prevent session recordings from capturing sensitive credentials.

```suggestion
                    <input
                      type=""password""
                      data-ph-no-capture
                      className=""border border-border rounded w-full py-2 px-3 bg-background-emphasis""
                      value={apiKey}
                      onChange={(e: any) => setApiKey(e.target.value)}
                      placeholder=""Paste your API key here""
                    />
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/admin/embeddings/modals/ChangeCredentialsModal.tsx
Line: 206:212

Comment:
Missing `data-ph-no-capture` attribute. Since PostHog's `maskAllInputs` is now set to `false` (in `providers.tsx:21`), this API key input needs the explicit `data-ph-no-capture` attribute to prevent session recordings from capturing sensitive credentials.

```suggestion
                    <input
                      type=""password""
                      data-ph-no-capture
                      className=""border border-border rounded w-full py-2 px-3 bg-background-emphasis""
                      value={apiKey}
                      onChange={(e: any) => setApiKey(e.target.value)}
                      placeholder=""Paste your API key here""
                    />
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/onyx-dot-app/onyx/pull/7977#issuecomment-3819293947,,2026-01-29T17:57:22+00:00,API Key Input Missing PostHog Privacy Attribute,security,critical,9,This is a significant security catch - disabling PostHog's maskAllInputs globally would expose API keys in session recordings without the explicit data-ph-no-capture attribute. The reviewer correctly identified that changing the masking setting creates a credential exposure risk that requires explicit protection on sensitive inputs.,2026-02-05T22:30:01.285094+00:00
openreplay/openreplay,4265,refactor(chalice): changed displayName&description handling refactor(DB): changed displayName&description handling,https://github.com/openreplay/openreplay/pull/4265,"Missing data migration - existing `display_name`, `description`, and `status` values should be migrated to the new `_customized` tables before dropping columns

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/scripts/schema/db/init_dbs/clickhouse/1.25.0/1.25.0.sql
Line: 148:158

Comment:
Missing data migration - existing `display_name`, `description`, and `status` values should be migrated to the new `_customized` tables before dropping columns

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4265#discussion_r2742737361,,2026-01-29T17:25:46+00:00,Missing data migration to _customized tables,data_integrity,high,8,This catches a significant data integrity issue - dropping database columns without first migrating existing data to new tables would result in permanent data loss in production. This is a meaningful catch that prevents real user data from being lost during the schema migration.,2026-02-05T22:30:01.285124+00:00
gptme/gptme-contrib,230,feat(plugins): add gptme-retrieval plugin for automatic context retrieval,https://github.com/gptme/gptme-contrib/pull/230,"Shell injection vulnerability - user-controlled `backend` value passed directly to shell. If backend comes from untrusted config, this allows arbitrary command execution.

```suggestion
    cmd = [backend, query, ""--mode"", mode, ""-n"", str(max_results)]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: plugins/gptme-retrieval/src/gptme_retrieval/__init__.py
Line: 201:201

Comment:
Shell injection vulnerability - user-controlled `backend` value passed directly to shell. If backend comes from untrusted config, this allows arbitrary command execution.

```suggestion
    cmd = [backend, query, ""--mode"", mode, ""-n"", str(max_results)]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/230#discussion_r2744032789,,2026-01-30T00:02:55+00:00,Shell injection via unsanitized backend parameter,security,high,8,"Shell injection vulnerability is a genuine security issue - if a user-controlled backend value is passed to shell execution, it could allow arbitrary command execution. The fix correctly switches from shell string to list-based command execution which prevents injection. This is a meaningful security catch, though the exploitability depends on how 'backend' is actually sourced (from config which may or may not be user-controlled).",2026-02-05T22:30:01.285144+00:00
gptme/gptme-contrib,229,fix(gptodo): ensure tmux sessions inherit environment variables,https://github.com/gptme/gptme-contrib/pull/229,"`workspace` is not shell-escaped, creating command injection vulnerability if workspace path contains special characters

```suggestion
        full_cmd = f""bash -l -c {shlex.quote(f'cd {shlex.quote(str(workspace))} && {env_setup}{shell_cmd}')}""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/subagent.py
Line: 173:173

Comment:
`workspace` is not shell-escaped, creating command injection vulnerability if workspace path contains special characters

```suggestion
        full_cmd = f""bash -l -c {shlex.quote(f'cd {shlex.quote(str(workspace))} && {env_setup}{shell_cmd}')}""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/229#discussion_r2743888735,✅ Fixed in commit ee90d31 - added `safe_workspace = shlex.quote(str(workspace))` before using it in the command.,2026-01-29T23:01:24+00:00,Workspace path not shell-escaped enables command injection,security,high,8,"This is a legitimate command injection vulnerability where unescaped workspace paths containing special characters (like semicolons or backticks) could allow arbitrary command execution. The fix correctly applies shell escaping using shlex.quote(). While command injection is a serious security issue, the attack vector requires a malicious workspace path which is a somewhat constrained scenario.",2026-02-05T22:30:01.285784+00:00
posthog/posthog,46349,chore(ducklake): simplify schema and table naming,https://github.com/PostHog/posthog/pull/46349,"removing `_{schema.id.hex[:8]}` from table names will cause naming conflicts when a team has multiple `ExternalDataSchema` objects with the same `source_type` and `normalized_name`. The codebase explicitly acknowledges this possibility (see `external_data_schema.py:368`), and the schema's unique ID is used in `folder_path()` to avoid conflicts. Without the schema ID suffix, `CREATE OR REPLACE TABLE` will overwrite data from different schemas.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/ducklake/ducklake_copy_data_imports_workflow.py
Line: 197:199

Comment:
removing `_{schema.id.hex[:8]}` from table names will cause naming conflicts when a team has multiple `ExternalDataSchema` objects with the same `source_type` and `normalized_name`. The codebase explicitly acknowledges this possibility (see `external_data_schema.py:368`), and the schema's unique ID is used in `folder_path()` to avoid conflicts. Without the schema ID suffix, `CREATE OR REPLACE TABLE` will overwrite data from different schemas.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46349#discussion_r2744668903,,2026-01-30T05:28:21+00:00,Table naming conflicts from removed schema ID suffix,data_integrity,high,8,"This is a meaningful catch of a data integrity bug - removing the schema ID suffix from table names would cause data overwrites when multiple schemas share the same source_type and normalized_name. The reviewer demonstrates deep understanding of the codebase by referencing the specific location where this edge case is acknowledged, showing this isn't an obvious issue but requires understanding the domain logic.",2026-02-05T22:30:01.285804+00:00
green-coding-solutions/green-metrics-tool,1529,(fix): Hardware Info Root Sudoers entry was possible to hijack,https://github.com/green-coding-solutions/green-metrics-tool/pull/1529,"**CRITICAL: Sudoers entry doesn't match actual code invocations**

The sudoers entries now require direct path format:
- `/usr/bin/python3 /path/to/lib/hardware_info_root.py`

But the actual code still uses module format:
- `lib/scenario_runner.py:797`: `sudo /usr/bin/python3 -m lib.hardware_info_root`
- `lib/utils.py:216`: `sudo python3 -m lib.hardware_info_root --read-rapl-energy-filtering`

These won't match the sudoers entry and sudo will prompt for password or fail.

**Fix:** Either update `scenario_runner.py` and `utils.py` to use direct path invocation, OR keep the sudoers entry using `-m lib.hardware_info_root` format (which was the original vulnerable format that this PR aims to fix).

The secure solution is to update the code invocations to match the new sudoers format.",https://github.com/green-coding-solutions/green-metrics-tool/pull/1529#discussion_r2745104559,,2026-01-30T08:04:48+00:00,Sudoers Path Mismatch with Module Invocation Format,security,high,8,"This catches a critical mismatch between sudoers entries and actual code invocations that would break the security fix entirely. The PR is meant to fix a sudoers hijacking vulnerability, but the code still uses the old vulnerable module invocation format (-m lib.hardware_info_root) while sudoers was updated to require direct path format, meaning either the fix won't work or sudo will fail.",2026-02-05T22:30:01.285832+00:00
openreplay/openreplay,4268,fix(chalice): fixed idp_group_prefixes default value,https://github.com/openreplay/openreplay/pull/4268,"Migration drops columns with existing data but doesn't migrate that data to the new `all_events_customized` table. Any existing `display_name`, `description`, or `status` values will be permanently lost.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/scripts/schema/db/init_dbs/clickhouse/1.25.0/1.25.0.sql
Line: 147:156

Comment:
Migration drops columns with existing data but doesn't migrate that data to the new `all_events_customized` table. Any existing `display_name`, `description`, or `status` values will be permanently lost.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4268#discussion_r2746479945,,2026-01-30T14:05:07+00:00,Migration drops columns without preserving existing data,data_integrity,high,8,This catches a real data loss issue in a database migration - dropping columns without migrating existing data to the new table would permanently destroy user data. This is a meaningful production bug that could cause significant problems and wouldn't be caught by automated tools.,2026-02-05T22:30:01.285848+00:00
helixdb/helix-db,845,impr (hql+core): nested ids and math ops,https://github.com/HelixDB/helix-db/pull/845,"Division by zero not handled - `wrapping_div` panics on zero divisor

```suggestion
            // Same-type signed integer divisions
            (Value::I8(a), Value::I8(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I8(a.wrapping_div(b))
            }
            (Value::I16(a), Value::I16(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I16(a.wrapping_div(b))
            }
            (Value::I32(a), Value::I32(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I32(a.wrapping_div(b))
            }
            (Value::I64(a), Value::I64(b)) => {
                if b == 0 { return Value::Empty; }
                Value::I64(a.wrapping_div(b))
            }
```",https://github.com/HelixDB/helix-db/pull/845#discussion_r2746255502,,2026-01-30T13:22:47+00:00,Missing zero divisor checks in wrapping_div operations,runtime,high,8,"Division by zero causing a panic is a genuine runtime bug that would crash the program in production. The `wrapping_div` function in Rust panics when the divisor is zero, so this catch prevents a production crash. This is a meaningful bug that isn't immediately obvious since the wrapping_* functions handle overflow but not zero division.",2026-02-05T22:30:01.285862+00:00
infisical/infisical,5325,feat(ui): add app connection id permission condition to secret syncs,https://github.com/Infisical/infisical/pull/5325,"<details>
<summary>Additional Comments (1)</summary>

**`backend/src/services/secret-sync/secret-sync-service.ts`**
**Critical Security Issue: Permission bypass when updating `connectionId`**

The permission checks use the OLD `connectionId` from the existing sync, but when `params.connectionId` is provided (line 502-511), the update allows changing to a NEW connection without verifying permission against that new `connectionId`.

**Attack scenario:**
1. User has Read/Edit on syncs with `connectionId=conn-1`  
2. User does NOT have access to `connectionId=conn-2`
3. User calls update with `params.connectionId=conn-2`
4. Permission check at line 436-443 passes (checks OLD `conn-1`)
5. Line 502-511 only validates the connection exists
6. User successfully updates sync to use `conn-2`, bypassing permission restrictions

**Fix required:**
When `params.connectionId` is provided, add a permission check against the NEW connection:

```typescript
if (params.connectionId && params.connectionId !== connectionId) {
  // Check permission against NEW connectionId
  ForbiddenError.from(permission).throwUnlessCan(
    ProjectPermissionSecretSyncActions.Edit,
    secretSync.environment?.slug && secretSync.folder?.path
      ? subject(ProjectPermissionSub.SecretSyncs, {
          environment: secretSync.environment.slug,
          secretPath: secretSync.folder.path,
          connectionId: params.connectionId
        })
      : ProjectPermissionSub.SecretSyncs
  );
}
```

Note: Secret rotation correctly prevents this by excluding `connectionId` from `TUpdateSecretRotationV2DTO`.
</details>",https://github.com/Infisical/infisical/pull/5325#issuecomment-3824534353,,2026-01-30T16:16:14+00:00,Permission bypass when updating connectionId in secret sync,security,critical,9,"This is a critical security vulnerability - a permission bypass that allows users to update secret syncs to use connections they don't have access to. The comment provides a clear attack scenario demonstrating how the old connectionId is checked but the new one isn't, enabling unauthorized access to connections. This is exactly the type of authorization bypass that could lead to data exposure in production.",2026-02-05T22:30:01.285896+00:00
openucx/ucc,1260,TL/NCCL: add user buffer registration via memmap,https://github.com/openucx/ucc/pull/1260,"buffer size calculation is incorrect for variable-size collectives (ALLTOALLV, ALLGATHERV, GATHERV, SCATTERV). For these collectives, `coll_args->args.src` uses the `info_v` union member with `counts*` array, not `info.count` scalar. Accessing `.info.count` reads the `counts` pointer as an integer, resulting in incorrect buffer size. Need to check `coll_args->args.coll_type` and use appropriate buffer size calculation for each collective type.

```suggestion
            // TODO: Need to calculate correct buffer size based on coll_type
            // For variable-size collectives (ALLTOALLV, ALLGATHERV, etc), 
            // must sum counts array instead of using single count value
            status = ucc_tl_nccl_lazy_register_memh(
                coll_args->args.src.info.buffer,
                coll_args->args.src.info.count *
                    ucc_dt_size(coll_args->args.src.info.datatype),
```",https://github.com/openucx/ucc/pull/1260#discussion_r2747554746,,2026-01-30T18:55:12+00:00,Wrong union member used for variable-size collective buffer calculation,logic,high,8,"This is a meaningful logic bug where accessing `.info.count` on variable-size collectives (ALLTOALLV, etc.) would incorrectly interpret a pointer as an integer value, leading to completely wrong buffer size calculations. This could cause memory corruption, incorrect data transfers, or crashes in production MPI/collective operations. The catch requires understanding of the union structure and collective operation semantics.",2026-02-05T22:30:01.285923+00:00
huskit/website-agent,28,Feat/improve template flow,https://github.com/HuskIT/website-agent/pull/28,"path traversal check uses simple string matching which doesn't catch all encoding variants

The current `isPathSafe` implementation checks for `..` using `includes('...')` which misses encoded variants like:
- URL encoding: `%2e%2e/` or `..%2f`
- Unicode normalization issues
- Mixed separators: `..\` on Windows

Consider using `path.normalize()` and checking if the resolved path stays within the expected directory:

```suggestion
function isPathSafe(filePath: string): boolean {
  // Reject absolute paths
  if (filePath.startsWith('/') || filePath.startsWith('\\')) {
    return false;
  }

  // Reject Windows absolute paths
  if (/^[a-zA-Z]:/.test(filePath)) {
    return false;
  }

  // Normalize and check for path traversal
  const normalized = path.normalize(filePath);
  if (normalized.startsWith('..') || normalized.includes(`${path.sep}..`)) {
    return false;
  }

  return true;
}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: app/lib/.server/templates/zip-template-fetcher.ts
Line: 166:183

Comment:
path traversal check uses simple string matching which doesn't catch all encoding variants

The current `isPathSafe` implementation checks for `..` using `includes('...')` which misses encoded variants like:
- URL encoding: `%2e%2e/` or `..%2f`
- Unicode normalization issues
- Mixed separators: `..\` on Windows

Consider using `path.normalize()` and checking if the resolved path stays within the expected directory:

```suggestion
function isPathSafe(filePath: string): boolean {
  // Reject absolute paths
  if (filePath.startsWith('/') || filePath.startsWith('\\')) {
    return false;
  }

  // Reject Windows absolute paths
  if (/^[a-zA-Z]:/.test(filePath)) {
    return false;
  }

  // Normalize and check for path traversal
  const normalized = path.normalize(filePath);
  if (normalized.startsWith('..') || normalized.includes(`${path.sep}..`)) {
    return false;
  }

  return true;
}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/HuskIT/website-agent/pull/28#discussion_r2749161211,,2026-01-31T07:32:56+00:00,Path traversal bypass via encoded dot sequences,security,critical,9,"This is a legitimate path traversal vulnerability catch in a zip file extraction utility. The comment correctly identifies that simple string matching for '..' is insufficient and can be bypassed with URL encoding (%2e%2e), Unicode normalization, or mixed separators. Path traversal in file extraction is a well-known security vulnerability (Zip Slip) that could allow attackers to write files outside the intended directory.",2026-02-05T22:30:01.285942+00:00
mrmps/smry,57,"Add Gravity micro/inline ads, AI copy menu, and buy-click device metadata",https://github.com/mrmps/SMRY/pull/57,"`setState` called during render (lines 73-74) will cause infinite re-renders. Use `useEffect` instead.

```suggestion
  const [prevSourcesLen, setPrevSourcesLen] = useState(sources.length);
  
  useEffect(() => {
    if (sources.length !== prevSourcesLen) {
      setPrevSourcesLen(sources.length);
      if (deselectedSources.size > 0) setDeselectedSources(new Set());
    }
  }, [sources.length, prevSourcesLen, deselectedSources.size]);
```",https://github.com/mrmps/SMRY/pull/57#discussion_r2749168539,,2026-01-31T07:46:34+00:00,setState called during render causes infinite loop,runtime,high,8,"Calling setState during render causes infinite re-renders, which is a genuine React anti-pattern that would cause the component to crash or freeze in production. This is a meaningful runtime bug, though experienced React developers would likely catch it quickly during development.",2026-02-05T22:30:01.285970+00:00
twentyhq/twenty,17602,feat(imap): Enterprise-Grade IDLE + Chunked Sync,https://github.com/twentyhq/twenty/pull/17602,"[P0] `startIdle` awaits `client.idle()`, which typically only resolves when IDLE ends. This means `startIdle` likely never returns while the connection is healthy, and `activeClients.set(accountId, client)` / the ""IDLE started"" log may never execute. In practice this can block whatever code path calls `startIdle` and prevent tracking the active client.

If you want IDLE to run in the background, it usually needs to be started without awaiting completion (or managed in a long-lived task), and `activeClients` should be set before entering the long wait.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/modules/messaging/message-import-manager/drivers/imap/services/imap-idle.service.ts
Line: 15:18

Comment:
[P0] `startIdle` awaits `client.idle()`, which typically only resolves when IDLE ends. This means `startIdle` likely never returns while the connection is healthy, and `activeClients.set(accountId, client)` / the ""IDLE started"" log may never execute. In practice this can block whatever code path calls `startIdle` and prevent tracking the active client.

If you want IDLE to run in the background, it usually needs to be started without awaiting completion (or managed in a long-lived task), and `activeClients` should be set before entering the long wait.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17602#discussion_r2749733668,,2026-01-31T16:48:54+00:00,Blocking await on IMAP IDLE prevents client tracking,concurrency,high,8,"This is a meaningful concurrency/async bug where awaiting a long-running IDLE operation would block the function indefinitely, preventing the client from being tracked in activeClients and blocking the calling code path. This is a real architectural issue that would cause problems in production with IMAP idle connections.",2026-02-05T22:30:01.285989+00:00
sqlfluff/sqlfluff,7414,Support globs in COMMA_SEPARATED_PATH_KEYS,https://github.com/sqlfluff/sqlfluff/pull/7414,"<details>
<summary>Additional Comments (1)</summary>

**`src/sqlfluff/core/config/file.py`**
Breaking change: `_resolve_path()` now returns a list, but this code expects a string for single path keys (like `library_path`). This will cause type errors when code tries to use these paths with string operations (e.g., `os.path.join(library_path, ""__init__.py"")` in jinja.py:266).

The fix requires handling the list return value. For single path keys, take the first match or original value if no matches:

```suggestion
        # It it's a single path key, resolve it.
        elif key.lower().endswith(RESOLVE_PATH_SUFFIXES):
            assert isinstance(
                val, str
            ), f""Value for {key} in {log_filename} must be a string not {type(val)}.""
            matches = _resolve_path(filepath, val)
            config[key] = matches[0] if matches else val
```
</details>",https://github.com/sqlfluff/sqlfluff/pull/7414#issuecomment-3829312171,,2026-01-31T20:51:10+00:00,_resolve_path() returns list but string expected,type_error,high,8,"This catches a real breaking change where a function's return type changed from string to list, which would cause runtime errors in production code that expects string operations like os.path.join(). The comment identifies the specific downstream code location (jinja.py:266) that would break and provides a concrete fix.",2026-02-05T22:30:01.286013+00:00
raycast/extensions,25035,Update moneybird extension: paginating contacts/lists (#24390) and repeat time entry (#23438),https://github.com/raycast/extensions/pull/25035,"infinite loop if `links.next` is not present - `nextUrl` is never set to null

```suggestion
  while (nextUrl) {
    const response = await fetchWithAuth(nextUrl.toString());
    const data = await response.json();
    if (Array.isArray(data)) results.push(...(data as T[]));

    const links = parseLinkHeader(response.headers.get(""Link""));
    if (links.next) {
      nextUrl = new URL(links.next, apiBaseUrl);
    } else {
      nextUrl = null as unknown as URL;
      break;
    }
  }
```",https://github.com/raycast/extensions/pull/25035#discussion_r2750134117,,2026-01-31T22:41:40+00:00,Missing nextUrl null assignment causes infinite loop,logic,high,8,"This catches a genuine infinite loop bug - if the API response doesn't include a 'next' link in the headers, `nextUrl` would never be set to null, causing the loop to run forever. This would freeze the extension and potentially cause resource exhaustion. A meaningful logic bug that would cause real problems in production.",2026-02-05T22:30:01.286029+00:00
onyx-dot-app/onyx,8056,refactor(craft): make usage limit overrides feature flags instead of env vars,https://github.com/onyx-dot-app/onyx/pull/8056,"`_should_skip_rate_limiting` unconditionally uses `get_default_feature_flag_provider().feature_enabled(...)`, but unlike `is_onyx_craft_enabled` it doesn’t fall back when PostHog isn’t configured (NoOp provider). If NoOp returns False by default, this will make `not has_rate_limit` = True and effectively grant unlimited usage to *all* cloud users whenever PostHog is unavailable/misconfigured.

Consider mirroring the existing pattern:
- if provider is `NoOpFeatureFlagProvider`, default to applying limits (return False), or gate this behind an env/config that’s explicitly opt-in.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/server/features/build/api/rate_limit.py
Line: 33:36

Comment:
`_should_skip_rate_limiting` unconditionally uses `get_default_feature_flag_provider().feature_enabled(...)`, but unlike `is_onyx_craft_enabled` it doesn’t fall back when PostHog isn’t configured (NoOp provider). If NoOp returns False by default, this will make `not has_rate_limit` = True and effectively grant unlimited usage to *all* cloud users whenever PostHog is unavailable/misconfigured.

Consider mirroring the existing pattern:
- if provider is `NoOpFeatureFlagProvider`, default to applying limits (return False), or gate this behind an env/config that’s explicitly opt-in.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/8056#discussion_r2750167381,,2026-01-31T23:19:36+00:00,Rate limiting bypassed when PostHog provider unavailable,security,high,8,"This is a meaningful security/logic bug where a NoOp feature flag provider returning False would cause `not has_rate_limit` to be True, effectively bypassing rate limits for all cloud users when PostHog is unavailable. This could lead to resource abuse and is a real production concern that requires understanding the feature flag fallback pattern.",2026-02-05T22:30:01.286298+00:00
traceroot-ai/traceroot,397,Support org and project management #373,https://github.com/traceroot-ai/traceroot/pull/397,"[P0] API key endpoints no longer enforce any project existence or user authorization.

In this PR, `ProjectAccess` and the explicit `get_active_project_by_id` check were removed from `create_api_key_endpoint`, `list_api_keys_endpoint`, and `delete_api_key_endpoint`, meaning anyone who can reach the API can create/list/delete keys for any `project_id` (including IDs they don’t own), and `create` will also succeed for non-existent projects unless DB constraints reject it. If these routes are intended to be protected (even in MVP), they need to reintroduce an auth/access dependency or at least verify the project exists + caller has access.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: rest/routers/api_keys.py
Line: 24:29

Comment:
[P0] API key endpoints no longer enforce any project existence or user authorization.

In this PR, `ProjectAccess` and the explicit `get_active_project_by_id` check were removed from `create_api_key_endpoint`, `list_api_keys_endpoint`, and `delete_api_key_endpoint`, meaning anyone who can reach the API can create/list/delete keys for any `project_id` (including IDs they don’t own), and `create` will also succeed for non-existent projects unless DB constraints reject it. If these routes are intended to be protected (even in MVP), they need to reintroduce an auth/access dependency or at least verify the project exists + caller has access.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/397#discussion_r2750210955,,2026-01-31T23:51:03+00:00,API Key Endpoints Missing Authorization Controls,security,critical,9,"This is a critical security vulnerability - the removal of authorization checks means any authenticated user (or potentially unauthenticated user) can create, list, and delete API keys for any project, including ones they don't own. This is a classic authorization bypass (IDOR) that could lead to unauthorized access and data exposure. The catch requires understanding the security implications of removing access control dependencies.",2026-02-05T22:30:01.286334+00:00
getarcaneapp/arcane,1614,fix: container exec sessions not being cleaned up after websockets close,https://github.com/getarcaneapp/arcane/pull/1614,"using cancelled context for timeout won't work

When this cleanup is called from `watchExecContext` (line 489), the `ctx` will already be cancelled. `context.WithTimeout(ctx, 5*time.Second)` inherits the cancellation from the parent, so `cleanupCtx` will be immediately cancelled.

```suggestion
		cleanupCtx, cleanupCancel := context.WithTimeout(context.Background(), 5*time.Second)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/internal/api/ws_handler.go
Line: 478:479

Comment:
using cancelled context for timeout won't work

When this cleanup is called from `watchExecContext` (line 489), the `ctx` will already be cancelled. `context.WithTimeout(ctx, 5*time.Second)` inherits the cancellation from the parent, so `cleanupCtx` will be immediately cancelled.

```suggestion
		cleanupCtx, cleanupCancel := context.WithTimeout(context.Background(), 5*time.Second)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/getarcaneapp/arcane/pull/1614#discussion_r2750334846,,2026-02-01T01:14:07+00:00,Context timeout using already cancelled parent context,logic,high,8,"This is a meaningful logic bug where cleanup operations would silently fail due to using an already-cancelled context. The cleanup of exec sessions would never actually execute, leading to resource leaks in production. This requires understanding Go's context propagation and the specific call flow, making it a non-trivial catch.",2026-02-05T22:30:01.286604+00:00
nvidia/warp,1206,Fix:Automatic workaround for CUDA compiler crash with local matrices …,https://github.com/NVIDIA/warp/pull/1206,"Pattern check happens before `kernel.adj.build(self)` on line 2099, but the detection flags are only populated during the build process, so this check will always evaluate to `False`

Move this check to after line 2099:
```python
kernel.adj.build(self)

# Check for known compiler bugs after building (Issue #1200)
if warp.config.auto_detect_cuda_compiler_bugs:
    if kernel.adj.detect_issue_1200_pattern() and self.options.get(""optimization_level"", 3) == 3:
        # Warning and fix logic here
        ...
```

However, this creates a new problem: by the time we detect the pattern, the kernel has already been built with -O3. The architecture needs redesign to either:
1. Build the kernel, check the pattern, then rebuild if needed (inefficient)
2. Do a pre-scan pass before building to detect patterns (complex)
3. Check the pattern after build and mark kernel for rebuild on next use (better)",https://github.com/NVIDIA/warp/pull/1206#discussion_r2750574738,,2026-02-01T05:37:25+00:00,Pattern check executes before required build process,logic,high,8,"This is a meaningful logic bug catch - the pattern detection check is placed before the build process that populates the detection flags, meaning the check would always evaluate to False and the automatic workaround feature would never trigger. The reviewer not only identified the timing issue but also thoughtfully analyzed the architectural implications of fixing it, providing multiple solution approaches.",2026-02-05T22:30:01.286625+00:00
traceroot-ai/traceroot,401,[AgentOps] Backend support for org and project management,https://github.com/traceroot-ai/traceroot/pull/401,"[P1] Org-scoping bug in delete: the handler authorizes against `orgId`, but deletes by `where: { id: invitationId }` without verifying the invitation belongs to that org. An OWNER/ADMIN in Org A could delete an invitation in Org B if they know/guess its ID.

Consider deleting with an org-scoped predicate (e.g., `id` + `org_id`) or fetching and validating `invitation.org_id === orgId` first.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/app/api/organizations/[orgId]/invitations/[invitationId]/route.ts
Line: 28:30

Comment:
[P1] Org-scoping bug in delete: the handler authorizes against `orgId`, but deletes by `where: { id: invitationId }` without verifying the invitation belongs to that org. An OWNER/ADMIN in Org A could delete an invitation in Org B if they know/guess its ID.

Consider deleting with an org-scoped predicate (e.g., `id` + `org_id`) or fetching and validating `invitation.org_id === orgId` first.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/401#discussion_r2750674454,,2026-02-01T07:35:05+00:00,Missing org-scoped authorization in invitation deletion,security,critical,9,This is a serious authorization bypass vulnerability (IDOR - Insecure Direct Object Reference). An admin in one organization could delete invitations belonging to a completely different organization by guessing invitation IDs. This is a significant security bug that could lead to unauthorized data manipulation across tenant boundaries.,2026-02-05T22:30:01.286926+00:00
ever-co/ever-gauzy,9401,Apps,https://github.com/ever-co/ever-gauzy/pull/9401,"[P0] TypeORM ManyToOne decorator can receive the *options object* as the inverse-side argument.

In `MultiORMManyToOne`, when `inverseSideOrOptions` is an object it is treated as `options` and `inverseSideProperty` remains unset, but the TypeORM branch still calls `TypeOrmManyToOne(..., inverseSideOrOptions as TypeORMInverseSide<T>, ...)`. Under TypeORM, calls like `MultiORMManyToOne(User, { nullable: true })` will pass that options object as the inverse side and likely break relation metadata registration.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/core/src/lib/core/decorators/entity/relations/many-to-one.decorator.ts
Line: 73:79

Comment:
[P0] TypeORM ManyToOne decorator can receive the *options object* as the inverse-side argument.

In `MultiORMManyToOne`, when `inverseSideOrOptions` is an object it is treated as `options` and `inverseSideProperty` remains unset, but the TypeORM branch still calls `TypeOrmManyToOne(..., inverseSideOrOptions as TypeORMInverseSide<T>, ...)`. Under TypeORM, calls like `MultiORMManyToOne(User, { nullable: true })` will pass that options object as the inverse side and likely break relation metadata registration.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-gauzy/pull/9401#discussion_r2751743497,,2026-02-01T18:08:56+00:00,TypeORM options object incorrectly passed as inverse side,logic,high,8,"This is a meaningful logic bug in a decorator that handles argument overloading. When options are passed as the second argument, the code incorrectly forwards them to TypeORM as if they were the inverse side function, which would break relation metadata registration. This is a subtle issue in argument parsing logic that would cause real problems in production when developers use the common pattern of passing only options without an inverse side.",2026-02-05T22:30:01.287182+00:00
stack-auth/stack-auth,1145,Queries view,https://github.com/stack-auth/stack-auth/pull/1145,"`onRetry` is being invoked immediately with `onRetry()` instead of passing the function reference. This causes the async operation to execute during render.

```suggestion
        onClick={() => runAsynchronouslyWithAlert(onRetry)}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/dashboard/src/app/(main)/(protected)/projects/[projectId]/analytics/queries/page-client.tsx
Line: 292:292

Comment:
`onRetry` is being invoked immediately with `onRetry()` instead of passing the function reference. This causes the async operation to execute during render.

```suggestion
        onClick={() => runAsynchronouslyWithAlert(onRetry)}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/stack-auth/stack-auth/pull/1145#discussion_r2752407305,,2026-02-02T03:41:01+00:00,Function invoked during render instead of passed as reference,runtime,high,8,"This is a meaningful React bug where calling `onRetry()` instead of passing the function reference causes the async operation to execute during render, which violates React's render purity and would cause unintended side effects and potential infinite re-render loops. This is a real bug that would cause problems in production.",2026-02-05T22:30:01.287216+00:00
twentyhq/twenty,17615,Add Claude Code GitHub Workflow,https://github.com/twentyhq/twenty/pull/17615,"Workflow can be triggered by any user who can comment, including external contributors. While the PR description mentions ""Only users with write access can trigger the workflow,"" the conditional checks don't enforce this. Consider adding an authorization check:

```suggestion
    if: |
      github.event.comment.author_association == 'OWNER' ||
      github.event.comment.author_association == 'MEMBER' ||
      github.event.comment.author_association == 'COLLABORATOR' &&
      ((github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' && (contains(github.event.issue.body, '@claude') || contains(github.event.issue.title, '@claude'))))
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/claude.yml
Line: 15:19

Comment:
Workflow can be triggered by any user who can comment, including external contributors. While the PR description mentions ""Only users with write access can trigger the workflow,"" the conditional checks don't enforce this. Consider adding an authorization check:

```suggestion
    if: |
      github.event.comment.author_association == 'OWNER' ||
      github.event.comment.author_association == 'MEMBER' ||
      github.event.comment.author_association == 'COLLABORATOR' &&
      ((github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' && (contains(github.event.issue.body, '@claude') || contains(github.event.issue.title, '@claude'))))
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17615#discussion_r2753128267,,2026-02-02T08:21:17+00:00,Missing GitHub Workflow Authorization Check for External Contributors,security,high,8,"This is a meaningful security catch - the workflow could be triggered by any external contributor who can comment, potentially allowing unauthorized users to run Claude Code with repository access. The AI correctly identified that the PR description claimed write-access restriction but the code didn't enforce it, which could lead to resource abuse or unintended code execution.",2026-02-05T22:30:01.287231+00:00
openreplay/openreplay,4283,refactor(DB): changed DB scripts,https://github.com/openreplay/openreplay/pull/4283,"Missing `CREATE MATERIALIZED VIEW` after this DROP statement. The v1.24.0 schema includes `users_all_properties_extractor_mv` with columns `display_name`, `description`, `status`, and `_edited_by_user`. The rollback should recreate this view:

```suggestion
DROP TABLE IF EXISTS product_analytics.users_all_properties_extractor_mv;
CREATE MATERIALIZED VIEW IF NOT EXISTS product_analytics.users_all_properties_extractor_mv
    TO product_analytics.all_properties AS
SELECT project_id,
       'users'                                 AS source,
       property_name,
       FALSE                                   AS is_event_property,
       auto_captured_property                  AS auto_captured,
       or_property_display_name(property_name) AS display_name,
       ''                                      AS description,
       or_property_visibility(property_name)   AS status,
       0                                       AS data_count,
       0                                       AS query_count,
       _timestamp                              AS created_at,
       FALSE                                   AS _edited_by_user
FROM product_analytics.user_properties;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/scripts/schema/db/rollback_dbs/clickhouse/1.25.0/1.25.0.sql
Line: 41:41

Comment:
Missing `CREATE MATERIALIZED VIEW` after this DROP statement. The v1.24.0 schema includes `users_all_properties_extractor_mv` with columns `display_name`, `description`, `status`, and `_edited_by_user`. The rollback should recreate this view:

```suggestion
DROP TABLE IF EXISTS product_analytics.users_all_properties_extractor_mv;
CREATE MATERIALIZED VIEW IF NOT EXISTS product_analytics.users_all_properties_extractor_mv
    TO product_analytics.all_properties AS
SELECT project_id,
       'users'                                 AS source,
       property_name,
       FALSE                                   AS is_event_property,
       auto_captured_property                  AS auto_captured,
       or_property_display_name(property_name) AS display_name,
       ''                                      AS description,
       or_property_visibility(property_name)   AS status,
       0                                       AS data_count,
       0                                       AS query_count,
       _timestamp                              AS created_at,
       FALSE                                   AS _edited_by_user
FROM product_analytics.user_properties;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4283#discussion_r2754406936,,2026-02-02T13:44:19+00:00,Missing CREATE statement after DROP materialized view,data_integrity,high,8,"This catches a real database rollback bug where a materialized view is dropped but never recreated, which would cause data loss and break functionality when rolling back from v1.25.0 to v1.24.0. The rollback script would leave the database in an inconsistent state, potentially causing production issues during version downgrades.",2026-02-05T22:30:01.287258+00:00
posthog/posthog,46549,feat(logs): allow filtering on resource_fingerprint,https://github.com/PostHog/posthog/pull/46549,"<details>
<summary>Additional Comments (1)</summary>

**`products/logs/backend/logs_query_runner.py`**
the `resourceFingerprint` filter is not being applied in the `where()` method. the parameter is accepted in the schema and API but never checked here. add a filter like:

```suggestion
    def where(self) -> ast.Expr:
        exprs: list[ast.Expr] = []

        if self.query.serviceNames:
            exprs.append(
                parse_expr(
                    ""service_name IN {serviceNames}"",
                    placeholders={
                        ""serviceNames"": ast.Tuple(exprs=[ast.Constant(value=str(sn)) for sn in self.query.serviceNames])
                    },
                )
            )

        if self.query.resourceFingerprint:
            exprs.append(
                parse_expr(
                    ""resource_fingerprint = {resourceFingerprint}"",
                    placeholders={
                        ""resourceFingerprint"": ast.Constant(value=self.query.resourceFingerprint)
                    },
                )
            )

        if self.query.filterGroup:
            exprs.append(self.resource_filter(existing_filters=exprs))

            if self.attribute_filters:
                exprs.append(property_to_expr(self.attribute_filters, team=self.team))

            if self.log_filters:
                exprs.append(property_to_expr(self.log_filters, team=self.team))

        exprs.append(ast.Placeholder(expr=ast.Field(chain=[""filters""])))

        if self.query.severityLevels:
            exprs.append(
                parse_expr(
                    ""severity_text IN {severityLevels}"",
                    placeholders={
                        ""severityLevels"": ast.Tuple(
                            exprs=[ast.Constant(value=str(sl)) for sl in self.query.severityLevels]
                        )
                    },
                )
            )

        if self.query.liveLogsCheckpoint:
            exprs.append(
                parse_expr(
                    ""observed_timestamp >= {liveLogsCheckpoint}"",
                    placeholders={""liveLogsCheckpoint"": ast.Constant(value=self.query.liveLogsCheckpoint)},
                )
            )

        if self.query.after:
            try:
                cursor = json.loads(base64.b64decode(self.query.after).decode(""utf-8""))
                cursor_ts = dt.datetime.fromisoformat(cursor[""timestamp""])
                cursor_uuid = cursor[""uuid""]
            except (KeyError, ValueError, json.JSONDecodeError) as e:
                raise ValueError(f""Invalid cursor format: {e}"")
            # For ASC (earliest first): get rows where (timestamp, uuid) > cursor
            # For DESC (latest first, default): get rows where (timestamp, uuid) < cursor
            op = "">"" if self.query.orderBy == ""earliest"" else ""<""
            ts_op = "">="" if self.query.orderBy == ""earliest"" else ""<=""
            # The logs table is sorted by (team_id, time_bucket, ..., timestamp) where
            # time_bucket = toStartOfDay(timestamp). ClickHouse only prunes efficiently when
            # the WHERE clause matches the sorting key. A tuple comparison like
            # (timestamp, uuid) < (x, y) won't trigger pruning.
            # We add explicit scalar bounds on both time_bucket and timestamp to ensure
            # ClickHouse can use the primary index and skip irrelevant parts.
            exprs.append(
                parse_expr(
                    f""time_bucket {ts_op} toStartOfDay({{cursor_ts}})"",
                    placeholders={""cursor_ts"": ast.Constant(value=cursor_ts)},
                )
            )
            exprs.append(
                parse_expr(
                    f""timestamp {ts_op} {{cursor_ts}}"",
                    placeholders={""cursor_ts"": ast.Constant(value=cursor_ts)},
                )
            )
            # Tuple comparison handles the exact cursor position (same timestamp, different uuid)
            exprs.append(
                parse_expr(
                    f""(timestamp, uuid) {op} ({{cursor_ts}}, {{cursor_uuid}})"",
                    placeholders={
                        ""cursor_ts"": ast.Constant(value=cursor_ts),
                        ""cursor_uuid"": ast.Constant(value=cursor_uuid),
                    },
                )
            )

        return ast.And(exprs=exprs)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/logs/backend/logs_query_runner.py
Line: 242:326

Comment:
the `resourceFingerprint` filter is not being applied in the `where()` method. the parameter is accepted in the schema and API but never checked here. add a filter like:

```suggestion
    def where(self) -> ast.Expr:
        exprs: list[ast.Expr] = []

        if self.query.serviceNames:
            exprs.append(
                parse_expr(
                    ""service_name IN {serviceNames}"",
                    placeholders={
                        ""serviceNames"": ast.Tuple(exprs=[ast.Constant(value=str(sn)) for sn in self.query.serviceNames])
                    },
                )
            )

        if self.query.resourceFingerprint:
            exprs.append(
                parse_expr(
                    ""resource_fingerprint = {resourceFingerprint}"",
                    placeholders={
                        ""resourceFingerprint"": ast.Constant(value=self.query.resourceFingerprint)
                    },
                )
            )

        if self.query.filterGroup:
            exprs.append(self.resource_filter(existing_filters=exprs))

            if self.attribute_filters:
                exprs.append(property_to_expr(self.attribute_filters, team=self.team))

            if self.log_filters:
                exprs.append(property_to_expr(self.log_filters, team=self.team))

        exprs.append(ast.Placeholder(expr=ast.Field(chain=[""filters""])))

        if self.query.severityLevels:
            exprs.append(
                parse_expr(
                    ""severity_text IN {severityLevels}"",
                    placeholders={
                        ""severityLevels"": ast.Tuple(
                            exprs=[ast.Constant(value=str(sl)) for sl in self.query.severityLevels]
                        )
                    },
                )
            )

        if self.query.liveLogsCheckpoint:
            exprs.append(
                parse_expr(
                    ""observed_timestamp >= {liveLogsCheckpoint}"",
                    placeholders={""liveLogsCheckpoint"": ast.Constant(value=self.query.liveLogsCheckpoint)},
                )
            )

        if self.query.after:
            try:
                cursor = json.loads(base64.b64decode(self.query.after).decode(""utf-8""))
                cursor_ts = dt.datetime.fromisoformat(cursor[""timestamp""])
                cursor_uuid = cursor[""uuid""]
            except (KeyError, ValueError, json.JSONDecodeError) as e:
                raise ValueError(f""Invalid cursor format: {e}"")
            # For ASC (earliest first): get rows where (timestamp, uuid) > cursor
            # For DESC (latest first, default): get rows where (timestamp, uuid) < cursor
            op = "">"" if self.query.orderBy == ""earliest"" else ""<""
            ts_op = "">="" if self.query.orderBy == ""earliest"" else ""<=""
            # The logs table is sorted by (team_id, time_bucket, ..., timestamp) where
            # time_bucket = toStartOfDay(timestamp). ClickHouse only prunes efficiently when
            # the WHERE clause matches the sorting key. A tuple comparison like
            # (timestamp, uuid) < (x, y) won't trigger pruning.
            # We add explicit scalar bounds on both time_bucket and timestamp to ensure
            # ClickHouse can use the primary index and skip irrelevant parts.
            exprs.append(
                parse_expr(
                    f""time_bucket {ts_op} toStartOfDay({{cursor_ts}})"",
                    placeholders={""cursor_ts"": ast.Constant(value=cursor_ts)},
                )
            )
            exprs.append(
                parse_expr(
                    f""timestamp {ts_op} {{cursor_ts}}"",
                    placeholders={""cursor_ts"": ast.Constant(value=cursor_ts)},
                )
            )
            # Tuple comparison handles the exact cursor position (same timestamp, different uuid)
            exprs.append(
                parse_expr(
                    f""(timestamp, uuid) {op} ({{cursor_ts}}, {{cursor_uuid}})"",
                    placeholders={
                        ""cursor_ts"": ast.Constant(value=cursor_ts),
                        ""cursor_uuid"": ast.Constant(value=cursor_uuid),
                    },
                )
            )

        return ast.And(exprs=exprs)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/PostHog/posthog/pull/46549#issuecomment-3835611906,,2026-02-02T14:54:06+00:00,ResourceFingerprint filter parameter not applied in query execution,logic,high,8,"This is a meaningful logic bug - the PR title explicitly says 'allow filtering on resource_fingerprint' but the filter was never actually applied in the query builder. Users would be able to specify the filter parameter but it would silently have no effect, causing incorrect/unfiltered results. This is a real functional bug that would affect users in production.",2026-02-05T22:30:01.287298+00:00
posthog/posthog,46546,[WIP] feat(dashboards): use ai to explain changes,https://github.com/PostHog/posthog/pull/46546,"missing validation allows users to access other users' cached data by providing arbitrary cache keys

The endpoint accepts any cache key from the request body without verifying it belongs to the current user and dashboard. This allows unauthorized access to cached analysis data from other users' sessions.

Validate that the provided cache key matches the format created in the `retrieve` method for this specific user and dashboard before using it.

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/dashboards/dashboard.py
Line: 885:896

Comment:
missing validation allows users to access other users' cached data by providing arbitrary cache keys

The endpoint accepts any cache key from the request body without verifying it belongs to the current user and dashboard. This allows unauthorized access to cached analysis data from other users' sessions.

Validate that the provided cache key matches the format created in the `retrieve` method for this specific user and dashboard before using it.

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46546#discussion_r2754684248,,2026-02-02T14:41:47+00:00,Cache Key Authorization Bypass Vulnerability,security,critical,9,"This is a significant security vulnerability - an authorization bypass that allows users to access other users' cached analysis data by providing arbitrary cache keys. This is an IDOR (Insecure Direct Object Reference) vulnerability that could expose sensitive dashboard data across user boundaries, making it a high-impact security issue that warrants a score of 9.",2026-02-05T22:30:01.287876+00:00
posthog/posthog,46571,feat(mcp): add queryable memory system to MCP server,https://github.com/PostHog/posthog/pull/46571,"The HogQL query in the `/query/` endpoint doesn't filter by team_id in the `document_embeddings` table. This means users could potentially query memories from other teams if they can guess document IDs.

Add team filtering in the WHERE clause:

```suggestion
        query = f""""""
            SELECT
                document_id,
                content,
                metadata,
                cosineDistance(embedding, embedText({{query_text}}, {{model_name}})) as distance
            FROM (
                SELECT
                    document_id,
                    argMax(content, inserted_at) as content,
                    argMax(metadata, inserted_at) as metadata,
                    argMax(embedding, inserted_at) as embedding
                FROM document_embeddings
                WHERE model_name = {{model_name}}
                  AND product = 'posthog-ai'
                  AND document_type = 'memory'
                  AND team_id = {{team_id}}
                GROUP BY document_id, model_name, product, document_type, rendering
            )
            WHERE ({{skip_user_filter}} OR JSONExtractString(metadata, 'user_id') = {{user_id}})
              AND NOT JSONExtractBool(metadata, 'deleted')
              AND ({metadata_filter_sql})
            ORDER BY distance ASC
            LIMIT {{limit}}
        """"""
```

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/posthog_ai/backend/api.py
Line: 101:124

Comment:
The HogQL query in the `/query/` endpoint doesn't filter by team_id in the `document_embeddings` table. This means users could potentially query memories from other teams if they can guess document IDs.

Add team filtering in the WHERE clause:

```suggestion
        query = f""""""
            SELECT
                document_id,
                content,
                metadata,
                cosineDistance(embedding, embedText({{query_text}}, {{model_name}})) as distance
            FROM (
                SELECT
                    document_id,
                    argMax(content, inserted_at) as content,
                    argMax(metadata, inserted_at) as metadata,
                    argMax(embedding, inserted_at) as embedding
                FROM document_embeddings
                WHERE model_name = {{model_name}}
                  AND product = 'posthog-ai'
                  AND document_type = 'memory'
                  AND team_id = {{team_id}}
                GROUP BY document_id, model_name, product, document_type, rendering
            )
            WHERE ({{skip_user_filter}} OR JSONExtractString(metadata, 'user_id') = {{user_id}})
              AND NOT JSONExtractBool(metadata, 'deleted')
              AND ({metadata_filter_sql})
            ORDER BY distance ASC
            LIMIT {{limit}}
        """"""
```

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46571#discussion_r2755592219,,2026-02-02T17:59:48+00:00,Missing team_id filter enables cross-team data access,security,critical,9,"This is a critical security vulnerability - missing team_id filtering in a database query allows cross-tenant data access. Users could query and retrieve memories belonging to other teams, which is a significant data exposure issue in a multi-tenant SaaS application.",2026-02-05T22:30:01.287904+00:00
twentyhq/twenty,17646,Migrate workflow serverless to logic,https://github.com/twentyhq/twenty/pull/17646,"Check that files are successfully uploaded before cleanup - if `applicationUniversalIdentifier` is undefined, files are migrated to temp but never uploaded, then deleted at line 228. This could result in data loss.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/database/commands/upgrade-version-command/1-17/1-17-migrate-workflow-code-steps.command.ts
Line: 219:226

Comment:
Check that files are successfully uploaded before cleanup - if `applicationUniversalIdentifier` is undefined, files are migrated to temp but never uploaded, then deleted at line 228. This could result in data loss.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17646#discussion_r2755394467,,2026-02-02T17:11:17+00:00,Missing Upload Validation Causes File Deletion Data Loss,data_integrity,high,8,"This catches a genuine data loss scenario where files could be migrated to temp storage, fail to upload due to undefined applicationUniversalIdentifier, and then be deleted during cleanup. This is a meaningful data integrity bug that would cause real problems in production during the migration process.",2026-02-05T22:30:01.288113+00:00
ever-co/ever-teams,4245,Refactor: Separate useTeamTasks into more atomic Hooks,https://github.com/ever-co/ever-teams/pull/4245,"Removed critical data synchronization effect that syncs React Query data with Jotai state. The original code had:

```typescript
useConditionalUpdateEffect(
  () => {
    if (teamTasksQuery.data?.items) {
      deepCheckAndUpdateTasks(teamTasksQuery.data.items, true);
    }
  },
  [teamTasksQuery.data?.items],
  Boolean(tasks?.length)
);
```

This effect ensures that when React Query fetches new data, it's synchronized with the Jotai store. Without it, components relying on the Jotai state won't see updates from the server.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/web/core/hooks/organizations/teams/use-team-tasks.ts
Line: 454:454

Comment:
Removed critical data synchronization effect that syncs React Query data with Jotai state. The original code had:

```typescript
useConditionalUpdateEffect(
  () => {
    if (teamTasksQuery.data?.items) {
      deepCheckAndUpdateTasks(teamTasksQuery.data.items, true);
    }
  },
  [teamTasksQuery.data?.items],
  Boolean(tasks?.length)
);
```

This effect ensures that when React Query fetches new data, it's synchronized with the Jotai store. Without it, components relying on the Jotai state won't see updates from the server.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-teams/pull/4245#discussion_r2755604568,,2026-02-02T18:03:22+00:00,Removed React Query to Jotai synchronization effect,data_integrity,high,8,"This catches a critical data synchronization bug where removing the effect would cause React Query data to never sync with Jotai state, meaning components using Jotai would show stale/outdated data and miss server updates. This is a meaningful architectural bug that would cause real user-facing problems in production.",2026-02-05T22:30:01.288133+00:00
posthog/posthog-dotnet,150,"feat: SDK compliance - retry logic, gzip compression, and UUID generation",https://github.com/PostHog/posthog-dotnet/pull/150,"The `finally` block disposes `response` (line 108) even when returning from the success path, potentially closing the stream from `ReadAsStreamAsync` before deserialization completes in `DeserializeFromCamelCaseJsonAsync`.

Move response disposal to after deserialization by wrapping the success path with `using (response)`, then skip disposal in the finally block for this path. This ensures the stream remains open during deserialization.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/PostHog/Library/HttpClientExtensions.cs
Line: 81:87

Comment:
The `finally` block disposes `response` (line 108) even when returning from the success path, potentially closing the stream from `ReadAsStreamAsync` before deserialization completes in `DeserializeFromCamelCaseJsonAsync`.

Move response disposal to after deserialization by wrapping the success path with `using (response)`, then skip disposal in the finally block for this path. This ensures the stream remains open during deserialization.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog-dotnet/pull/150#discussion_r2755460252,,2026-02-02T17:25:42+00:00,HttpResponse Disposed Before Stream Deserialization Completes,resource_leak,high,8,"This catches a real runtime bug where the response stream is disposed before deserialization completes, which would cause intermittent failures or data corruption when reading HTTP responses. This is a subtle resource management issue that could cause production problems and wouldn't be caught by static analysis tools.",2026-02-05T22:30:01.288158+00:00
huskit/website-agent,30,Feat/intergrate langfuse,https://github.com/HuskIT/website-agent/pull/30,"Breaking change: response structure changed from returning `data: BusinessData` to markdown-only format. `CreateProjectDialog.tsx:214` still expects `result.data` and will break.

```suggestion
    // Return response with both markdown AND business data for backward compatibility
    return json(
      {
        success: true,
        session_id: sessionId,
        data: result.data, // Keep for CreateProjectDialog.tsx
        google_maps_markdown: googleMapsMarkdown,
        website_markdown: websiteMarkdown,
        has_website: !!crawledWebsiteUrl,
      },
      { status: 200 },
    );
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: app/routes/api.crawler.extract.ts
Line: 323:333

Comment:
Breaking change: response structure changed from returning `data: BusinessData` to markdown-only format. `CreateProjectDialog.tsx:214` still expects `result.data` and will break.

```suggestion
    // Return response with both markdown AND business data for backward compatibility
    return json(
      {
        success: true,
        session_id: sessionId,
        data: result.data, // Keep for CreateProjectDialog.tsx
        google_maps_markdown: googleMapsMarkdown,
        website_markdown: websiteMarkdown,
        has_website: !!crawledWebsiteUrl,
      },
      { status: 200 },
    );
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/HuskIT/website-agent/pull/30#discussion_r2755186253,,2026-02-02T16:24:46+00:00,API Response Structure Breaking Change Missing Data Field,runtime,high,8,"This catches a breaking API change where the response structure was modified to remove `data: BusinessData`, but a consumer component (`CreateProjectDialog.tsx:214`) still expects `result.data`. This would cause a runtime error or undefined behavior in production when the dialog tries to access the missing property. The reviewer demonstrated cross-file awareness to identify this compatibility issue.",2026-02-05T22:30:01.288176+00:00
greptileai/react-mirror,452,I added a test that asserts malformed profiling operations do not throw and that errors/warnings are logged,https://github.com/greptileai/react-mirror/pull/452,"using `break` here without advancing `i` past all ADD operation parameters will cause subsequent parameters to be misinterpreted as operation codes. For non-root nodes, there are still 5+ parameters to skip (parentID, ownerID, displayNameStringID, keyStringID, name prop). This will corrupt the operation stream parsing.

need to advance `i` to skip remaining parameters before breaking, similar to how the normal ADD case increments `i` throughout lines 234-248

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react-devtools-shared/src/devtools/views/Profiler/CommitTreeBuilder.js
Line: 202:207

Comment:
using `break` here without advancing `i` past all ADD operation parameters will cause subsequent parameters to be misinterpreted as operation codes. For non-root nodes, there are still 5+ parameters to skip (parentID, ownerID, displayNameStringID, keyStringID, name prop). This will corrupt the operation stream parsing.

need to advance `i` to skip remaining parameters before breaking, similar to how the normal ADD case increments `i` throughout lines 234-248

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/452#discussion_r2756449165,,2026-02-02T23:24:13+00:00,Break statement skips ADD operation parameters corrupting stream parsing,logic,high,8,"This is a meaningful logic bug where breaking from a loop without advancing the index past remaining parameters would corrupt the operation stream parsing, causing subsequent data to be misinterpreted as operation codes. This would cause incorrect behavior in the React DevTools profiler, affecting debugging functionality. The catch demonstrates good understanding of the parsing logic and stream processing.",2026-02-05T22:30:01.288420+00:00
infisical/infisical,5346,feat(pki): add slack webhook to alerts,https://github.com/Infisical/infisical/pull/5346,"Missing SSRF (Server-Side Request Forgery) protection. The Slack webhook URL is not validated against private/internal IP addresses before making the HTTP request.

**Security Risk:** An attacker could configure a Slack webhook URL pointing to internal services (e.g., internal metadata endpoints, database servers, or other private infrastructure). This could expose sensitive internal infrastructure.

**How to exploit:** Create a PKI alert with a malicious Slack webhook URL that resolves to an internal IP address, bypassing the `hooks.slack.com` domain check through DNS rebinding or URL manipulation.

**Fix:** Use the existing `blockLocalAndPrivateIpAddresses` function before making webhook requests. This function is already used in other parts of the codebase (e.g., `backend/src/services/app-connection/github/github-connection-fns.ts`).

```typescript
import { blockLocalAndPrivateIpAddresses } from ""@app/lib/validator/validate-url"";

// Before triggerSlackWebhook
await blockLocalAndPrivateIpAddresses(config.webhookUrl);
```",https://github.com/Infisical/infisical/pull/5346#discussion_r2756158957,Fixed - added `blockLocalAndPrivateIpAddresses()` check before triggering webhook.,2026-02-02T21:21:26+00:00,Missing SSRF validation on Slack webhook URLs,security,critical,9,"This is a genuine SSRF vulnerability catch in production code. The comment correctly identifies that user-controlled webhook URLs could be abused to probe internal infrastructure, provides a concrete exploitation path (DNS rebinding), and recommends using an existing project pattern for the fix. SSRF is a well-known critical security issue that could expose internal services and metadata endpoints.",2026-02-05T22:30:01.288441+00:00
openreplay/openreplay,4287,fix(chalice): fixed get event's properties,https://github.com/openreplay/openreplay/pull/4287,"INNER JOIN only on `property_name` could match properties from different projects. Consider adding `project_id` to the join condition:

```suggestion
                        INNER JOIN product_analytics.all_properties USING (project_id, property_name)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: api/chalicelib/core/product_analytics/properties.py
Line: 325:325

Comment:
INNER JOIN only on `property_name` could match properties from different projects. Consider adding `project_id` to the join condition:

```suggestion
                        INNER JOIN product_analytics.all_properties USING (project_id, property_name)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4287#discussion_r2756042249,,2026-02-02T20:32:56+00:00,INNER JOIN missing project_id causes cross-project property matching,data_integrity,high,8,"This is a meaningful data integrity bug where a SQL JOIN missing the project_id condition could cause cross-project data leakage, returning properties from unrelated projects. This is a real bug that would affect production correctness and could have security/privacy implications in a multi-tenant system.",2026-02-05T22:30:01.288468+00:00
traceroot-ai/traceroot,407,[AgentOps] Support membership invite [1/n],https://github.com/traceroot-ai/traceroot/pull/407,"XSS vulnerability - user-provided data (`inviterName`, `inviterEmail`, `workspaceName`) is directly interpolated into HTML without escaping. If an attacker sets their name to `<script>alert('xss')</script>`, it will execute in the recipient's email client.

```suggestion
function buildHtmlEmail(params: EmailContentParams): string {
  const { inviterName, inviterEmail, workspaceName, acceptLink, roleName } = params;
  
  // Escape HTML to prevent XSS
  const escapeHtml = (text: string) => text
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/""/g, '&quot;')
    .replace(/'/g, '&#x27;');

  return `
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""utf-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
  </head>
  <body style=""font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.6; color: #333; margin: 0; padding: 20px; background-color: #fafafa;"">
    <table cellpadding=""0"" cellspacing=""0"" border=""0"" width=""100%"" style=""max-width: 480px; margin: 0 auto; background: #fff; border: 1px solid #e5e5e5;"">
      <!-- Logo section -->
      <tr>
        <td style=""padding: 40px 40px 32px 40px; text-align: center;"">
          <img src=""https://raw.githubusercontent.com/traceroot-ai/traceroot/pivot/agentops/ui/public/images/icon.png"" alt=""TraceRoot"" width=""72"" height=""72"" style=""display: block; margin: 0 auto; border-radius: 14px;"" />
        </td>
      </tr>

      <!-- Title -->
      <tr>
        <td style=""padding: 0 40px 24px 40px; text-align: center;"">
          <h1 style=""font-size: 24px; font-weight: 600; margin: 0; color: #000; letter-spacing: -0.5px;"">
            Join ${escapeHtml(workspaceName)} on TraceRoot
          </h1>
        </td>
      </tr>

      <!-- Body -->
      <tr>
        <td style=""padding: 0 40px 32px 40px;"">
          <p style=""margin: 0; color: #333; font-size: 15px; line-height: 1.6; text-align: center;"">
            <strong>${escapeHtml(inviterName)}</strong> (${escapeHtml(inviterEmail)}) has invited you to join the <strong>${escapeHtml(workspaceName)}</strong> workspace as a <strong>${escapeHtml(roleName)}</strong>.
          </p>
        </td>
      </tr>

      <!-- Button -->
      <tr>
        <td style=""padding: 0 40px 40px 40px; text-align: center;"">
          <table cellpadding=""0"" cellspacing=""0"" border=""0"" style=""margin: 0 auto;"">
            <tr>
              <td style=""background-color: #000;"">
                <a href=""${escapeHtml(acceptLink)}"" style=""display: inline-block; padding: 10px 20px; color: #ffffff; text-decoration: none; font-weight: 500; font-size: 14px;"">
                  Accept Invitation
                </a>
              </td>
            </tr>
          </table>
        </td>
      </tr>

      <!-- Divider -->
      <tr>
        <td style=""border-top: 1px solid #e5e5e5;""></td>
      </tr>

      <!-- Footer -->
      <tr>
        <td style=""padding: 24px 40px; background-color: #fafafa;"">
          <p style=""color: #999; font-size: 12px; margin: 0; text-align: center;"">
            If you were not expecting this invitation, you can ignore this email.
          </p>
        </td>
      </tr>
    </table>
  </body>
</html>
  `.trim();
}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ui/src/lib/email/send-invite-email.ts
Line: 68:136

Comment:
XSS vulnerability - user-provided data (`inviterName`, `inviterEmail`, `workspaceName`) is directly interpolated into HTML without escaping. If an attacker sets their name to `<script>alert('xss')</script>`, it will execute in the recipient's email client.

```suggestion
function buildHtmlEmail(params: EmailContentParams): string {
  const { inviterName, inviterEmail, workspaceName, acceptLink, roleName } = params;
  
  // Escape HTML to prevent XSS
  const escapeHtml = (text: string) => text
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/""/g, '&quot;')
    .replace(/'/g, '&#x27;');

  return `
<!DOCTYPE html>
<html>
  <head>
    <meta charset=""utf-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
  </head>
  <body style=""font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.6; color: #333; margin: 0; padding: 20px; background-color: #fafafa;"">
    <table cellpadding=""0"" cellspacing=""0"" border=""0"" width=""100%"" style=""max-width: 480px; margin: 0 auto; background: #fff; border: 1px solid #e5e5e5;"">
      <!-- Logo section -->
      <tr>
        <td style=""padding: 40px 40px 32px 40px; text-align: center;"">
          <img src=""https://raw.githubusercontent.com/traceroot-ai/traceroot/pivot/agentops/ui/public/images/icon.png"" alt=""TraceRoot"" width=""72"" height=""72"" style=""display: block; margin: 0 auto; border-radius: 14px;"" />
        </td>
      </tr>

      <!-- Title -->
      <tr>
        <td style=""padding: 0 40px 24px 40px; text-align: center;"">
          <h1 style=""font-size: 24px; font-weight: 600; margin: 0; color: #000; letter-spacing: -0.5px;"">
            Join ${escapeHtml(workspaceName)} on TraceRoot
          </h1>
        </td>
      </tr>

      <!-- Body -->
      <tr>
        <td style=""padding: 0 40px 32px 40px;"">
          <p style=""margin: 0; color: #333; font-size: 15px; line-height: 1.6; text-align: center;"">
            <strong>${escapeHtml(inviterName)}</strong> (${escapeHtml(inviterEmail)}) has invited you to join the <strong>${escapeHtml(workspaceName)}</strong> workspace as a <strong>${escapeHtml(roleName)}</strong>.
          </p>
        </td>
      </tr>

      <!-- Button -->
      <tr>
        <td style=""padding: 0 40px 40px 40px; text-align: center;"">
          <table cellpadding=""0"" cellspacing=""0"" border=""0"" style=""margin: 0 auto;"">
            <tr>
              <td style=""background-color: #000;"">
                <a href=""${escapeHtml(acceptLink)}"" style=""display: inline-block; padding: 10px 20px; color: #ffffff; text-decoration: none; font-weight: 500; font-size: 14px;"">
                  Accept Invitation
                </a>
              </td>
            </tr>
          </table>
        </td>
      </tr>

      <!-- Divider -->
      <tr>
        <td style=""border-top: 1px solid #e5e5e5;""></td>
      </tr>

      <!-- Footer -->
      <tr>
        <td style=""padding: 24px 40px; background-color: #fafafa;"">
          <p style=""color: #999; font-size: 12px; margin: 0; text-align: center;"">
            If you were not expecting this invitation, you can ignore this email.
          </p>
        </td>
      </tr>
    </table>
  </body>
</html>
  `.trim();
}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/traceroot-ai/traceroot/pull/407#discussion_r2756486103,fixed.,2026-02-02T23:43:54+00:00,XSS vulnerability in email HTML interpolation,security,high,8,"This is a legitimate XSS vulnerability where user-controlled data (inviterName, inviterEmail, workspaceName) is directly interpolated into HTML email content without escaping. While email client XSS is more limited than browser XSS, it's still a real security issue that could execute malicious scripts in some email clients. The fix with proper HTML escaping is correct and necessary.",2026-02-05T22:30:01.288486+00:00
browserbase/stagehand,1650,feat: prefer CSS selectors over XPath in resolveSelector,https://github.com/browserbase/stagehand/pull/1650,"CSS selectors in `cssMap` are per-frame only and cannot traverse iframes like XPath. When an element is inside an iframe, the CSS selector will fail when called from the root page context, because CSS selectors aren't prefixed with iframe paths during snapshot capture (see `capture.ts:446`). 

The `deepLocator` only treats selectors starting with `/` or `xpath=` as XPath that can cross iframe boundaries. A plain CSS selector like `#button-id` will only search in the current frame.

Consider checking if the CSS selector contains iframe navigation (e.g., `>>` notation) or if the ref is known to be in an iframe before preferring CSS.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/index.ts
Line: 749:752

Comment:
CSS selectors in `cssMap` are per-frame only and cannot traverse iframes like XPath. When an element is inside an iframe, the CSS selector will fail when called from the root page context, because CSS selectors aren't prefixed with iframe paths during snapshot capture (see `capture.ts:446`). 

The `deepLocator` only treats selectors starting with `/` or `xpath=` as XPath that can cross iframe boundaries. A plain CSS selector like `#button-id` will only search in the current frame.

Consider checking if the CSS selector contains iframe navigation (e.g., `>>` notation) or if the ref is known to be in an iframe before preferring CSS.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/browserbase/stagehand/pull/1650#discussion_r2756657312,,2026-02-03T00:58:55+00:00,CSS selectors fail for iframe elements,logic,high,8,"This catch identifies a real functional bug where CSS selectors will silently fail for elements inside iframes, while XPath would work correctly. This is a non-obvious cross-frame navigation issue that would cause incorrect behavior in production for any page with iframes, and requires domain knowledge of how selectors and frames interact.",2026-02-05T22:30:01.288553+00:00
ethyca/fides,7284,Add RBAC management UI with alpha feature flag,https://github.com/ethyca/fides/pull/7284,"[P1] `selectThisUsersScopes` treats an empty RBAC permission set as “not loaded” and silently falls back to legacy permissions, which can grant UI access when RBAC is enabled but the user legitimately has 0 permissions (or the RBAC endpoint returns `[]`). This is most likely for brand-new users/roles or when the RBAC service is up but returns no scopes.

Concretely, the gate `if (rbacPermissions && rbacPermissions.length > 0)` means `[]` will fall back to `user/{id}/permission`, potentially rendering screens based on legacy scopes even though RBAC is enabled.

Suggestion: distinguish “query not loaded / errored” from “loaded empty”, and if RBAC is enabled + query succeeded, return the RBAC permissions even if empty.

Also appears in: `clients/admin-ui/src/features/common/CommonSubscriptions.tsx` (RBAC prefetch assumes selector uses cache).",https://github.com/ethyca/fides/pull/7284#discussion_r2757017517,,2026-02-03T03:32:15+00:00,RBAC empty permissions fallback to legacy grants unauthorized access,security,critical,9,"This is a significant security vulnerability where the RBAC permission check incorrectly treats an empty permission set as 'not loaded', causing fallback to legacy permissions. This could grant unauthorized UI access to users who legitimately should have zero permissions under RBAC, representing a privilege escalation issue. The analysis is specific, identifies the exact problematic condition, and explains the real-world impact.",2026-02-05T22:30:01.288583+00:00
posthog/posthog,46633,feat: add Enhance SDK,https://github.com/PostHog/posthog/pull/46633,"Loading remote JavaScript via `importScripts()` from an external CDN without Subresource Integrity (SRI) checks creates a critical security vulnerability. If the CDN is compromised or the response is intercepted, arbitrary malicious code could be executed with service worker privileges, potentially compromising the entire application and user data.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/public/enhance-sw.js
Line: 3:3

Comment:
Loading remote JavaScript via `importScripts()` from an external CDN without Subresource Integrity (SRI) checks creates a critical security vulnerability. If the CDN is compromised or the response is intercepted, arbitrary malicious code could be executed with service worker privileges, potentially compromising the entire application and user data.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46633#discussion_r2757290409,,2026-02-03T05:39:05+00:00,Missing SRI checks for importScripts CDN resources,security,critical,9,"This is a critical security vulnerability - loading external JavaScript in a service worker without SRI checks could allow arbitrary code execution if the CDN is compromised, with elevated service worker privileges that could compromise the entire application. Service workers have significant power over network requests and caching, making this a high-impact security issue.",2026-02-05T22:30:01.288602+00:00
raycast/extensions,25101,File Content Search: add extension,https://github.com/raycast/extensions/pull/25101,"[P0] Shell command injection via unescaped `path` / exclude values

`buildGrepCommand` returns a single shell string that is executed via `spawn(""sh"", [""-c"", command])` (see `useGrep.ts`). While the query is single-quoted, `path` is only wrapped in double quotes and `excludedDirectories`/`excludedExtensions` are interpolated unquoted. A preference value like `excludedDirectories: ""foo --exclude-dir=$(rm -rf ~)""` or a search path containing `""` / `$()` can break quoting and inject arbitrary shell commands. This matters whenever a user sets these preferences (or picks a directory with special characters).

Consider building the grep invocation with `spawn` args (no shell) or robustly escaping all interpolated values (path + exclude args), not just the query.",https://github.com/raycast/extensions/pull/25101#discussion_r2758321896,,2026-02-03T10:18:40+00:00,Shell command injection via unescaped path parameters,security,critical,9,"This is a legitimate shell command injection vulnerability where user-controlled values (path, excludedDirectories, excludedExtensions) are interpolated into a shell command without proper escaping. The comment provides a concrete exploit example showing how arbitrary commands could be executed. This is a serious security issue that wouldn't be caught by automated tools.",2026-02-05T22:30:01.288619+00:00
gptme/gptme,1224,Fix/server fd leak minimal,https://github.com/gptme/gptme/pull/1224,"[P1] `ShellSession.close()` now closes stdout/stderr before terminating the process group, which can raise and skip termination.

Closing `self.process.stdout/stderr` can throw (e.g., already closed, or I/O errors). Since it happens before the `try:` that sends SIGTERM/SIGKILL, an exception here would prevent the process group from being terminated, leaving a running bash + children (worse than an fd leak). Wrapping the pipe closes in the existing `try:` (or guarding with `contextlib.suppress(Exception)`) would ensure termination still happens even if a pipe close fails.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/tools/shell.py
Line: 730:737

Comment:
[P1] `ShellSession.close()` now closes stdout/stderr before terminating the process group, which can raise and skip termination.

Closing `self.process.stdout/stderr` can throw (e.g., already closed, or I/O errors). Since it happens before the `try:` that sends SIGTERM/SIGKILL, an exception here would prevent the process group from being terminated, leaving a running bash + children (worse than an fd leak). Wrapping the pipe closes in the existing `try:` (or guarding with `contextlib.suppress(Exception)`) would ensure termination still happens even if a pipe close fails.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1224#discussion_r2758802930,,2026-02-03T12:21:30+00:00,Exception in pipe close prevents process termination,resource_leak,high,8,"This is a meaningful catch identifying that an exception during stdout/stderr closing could prevent process termination, leaving orphan processes running - which is actually worse than the original fd leak the PR was trying to fix. The comment identifies a real logic bug in exception handling order that could cause resource leaks in production.",2026-02-05T22:30:01.288639+00:00
raycast/extensions,25111,Add tinytoolkits extension,https://github.com/raycast/extensions/pull/25111,"[P0] `calcPy` builds a shell command using user input and runs it via `exec`, which is command-injection prone.

Escaping only backslashes and double quotes is not sufficient for shell safety across platforms, and `exec` runs through a shell by default. A crafted expression can break out of quoting and execute arbitrary commands. Prefer `spawn`/`execFile` with an argument array (no shell) or avoid the external Python execution path entirely.

Also appears as part of the same issue in: `extensions/tinytoolkits/src/utils/calculator.ts:119-123`.",https://github.com/raycast/extensions/pull/25111#discussion_r2759763855,,2026-02-03T15:53:36+00:00,Command injection vulnerability in calcPy function,security,critical,9,"This is a genuine command injection vulnerability where user input is passed to exec() with inadequate escaping. The comment correctly identifies that escaping only backslashes and double quotes is insufficient for shell safety, and that exec() runs through a shell by default, allowing crafted expressions to break out of quoting and execute arbitrary commands. This is a critical security issue that could allow remote code execution.",2026-02-05T22:30:01.288664+00:00
gptme/gptme,1225,feat(llm): add OpenAI subscription provider support,https://github.com/gptme/gptme/pull/1225,"[P0] Streaming parser expects Chat Completions shape (`choices[0].delta.content`) but endpoint is `/codex/responses`

This code assumes SSE events contain `choices -> delta -> content` (like chat.completions), but the Codex `responses` API typically streams as `response.output_text.delta` / `output` items rather than `choices`. With the current parser, many/most responses will yield nothing (empty output) even on 200 OK.

Also appears in: gptme/llm/llm_openai_subscription.py:302-309.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/llm/llm_openai_subscription.py
Line: 294:309

Comment:
[P0] Streaming parser expects Chat Completions shape (`choices[0].delta.content`) but endpoint is `/codex/responses`

This code assumes SSE events contain `choices -> delta -> content` (like chat.completions), but the Codex `responses` API typically streams as `response.output_text.delta` / `output` items rather than `choices`. With the current parser, many/most responses will yield nothing (empty output) even on 200 OK.

Also appears in: gptme/llm/llm_openai_subscription.py:302-309.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1225#discussion_r2759919986,,2026-02-03T16:25:17+00:00,Streaming parser uses wrong schema for Codex responses endpoint,logic,high,8,"This is a meaningful API compatibility bug - the code is parsing streaming responses using the wrong data structure format, which would cause the feature to silently fail (return empty output) even on successful API calls. This is a non-obvious integration issue that wouldn't be caught by linters or type checkers, and would cause real user-facing problems in production.",2026-02-05T22:30:01.288680+00:00
credal-ai/actions-sdk,485,Added jira move action,https://github.com/Credal-ai/actions-sdk/pull/485,"[P1] After the move completes you refetch the issue via `${apiUrl}/issue/${currentIssueKey}` (key, not immutable id). If moving projects changes the issue key, this lookup can 404 and you’ll return the old `newTicketKey`/`ticketUrl` even though the move succeeded. Consider requesting/storing the issue’s immutable `id` from the initial GET and refetching by id, or using the bulk move task result if it provides the new key.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/actions/providers/jira/moveJiraTicketToProject.ts
Line: 214:219

Comment:
[P1] After the move completes you refetch the issue via `${apiUrl}/issue/${currentIssueKey}` (key, not immutable id). If moving projects changes the issue key, this lookup can 404 and you’ll return the old `newTicketKey`/`ticketUrl` even though the move succeeded. Consider requesting/storing the issue’s immutable `id` from the initial GET and refetching by id, or using the bulk move task result if it provides the new key.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Credal-ai/actions-sdk/pull/485#discussion_r2760526065,,2026-02-03T18:51:02+00:00,Issue refetch uses mutable key after project move,logic,high,8,"This is a meaningful logic bug that would cause incorrect behavior in production. When moving a Jira ticket between projects, the issue key changes, so fetching by the old key would 404 and return stale/incorrect data to users. This is a non-obvious edge case in the Jira API that requires domain knowledge to catch.",2026-02-05T22:30:01.288699+00:00
posthog/posthog,46771,fix: oauth redirect + team scope,https://github.com/PostHog/posthog/pull/46771,"Query bypasses permission checks - grants access to all teams where user is a member, regardless of membership level or RBAC

The query `Team.objects.filter(organization__members=request.user)` includes ALL teams where the user is a member of the organization, without checking if they actually have permission to access each team. This differs from the POST flow (lines 96-111) which validates team access via `user_permissions.team(team).effective_membership_level`.

For first-party apps, this should use the same permission validation logic:

```suggestion
                user_permissions = UserPermissions(request.user)
                teams = [team.pk for team in user_permissions.teams_visible_for_user]
                credentials[""scoped_teams""] = teams
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/oauth/views.py
Line: 276:277

Comment:
Query bypasses permission checks - grants access to all teams where user is a member, regardless of membership level or RBAC

The query `Team.objects.filter(organization__members=request.user)` includes ALL teams where the user is a member of the organization, without checking if they actually have permission to access each team. This differs from the POST flow (lines 96-111) which validates team access via `user_permissions.team(team).effective_membership_level`.

For first-party apps, this should use the same permission validation logic:

```suggestion
                user_permissions = UserPermissions(request.user)
                teams = [team.pk for team in user_permissions.teams_visible_for_user]
                credentials[""scoped_teams""] = teams
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46771#discussion_r2761593490,,2026-02-04T00:17:29+00:00,OAuth query bypasses RBAC team permission validation,security,critical,9,"This is a significant authorization bypass vulnerability where the OAuth flow grants access to all teams in an organization regardless of the user's actual RBAC permissions. This could allow users to access teams they shouldn't have permission to view, which is a serious security issue in a multi-tenant system. The catch correctly identifies inconsistency with the POST flow's permission validation and provides a concrete fix.",2026-02-05T22:30:01.288718+00:00
onyx-dot-app/onyx,8147,"Revert ""chore(document index): Remove offset""",https://github.com/onyx-dot-app/onyx/pull/8147,"`offset` parameter is hardcoded to `0` but should use the passed `offset` parameter value

```suggestion
            ""offset"": offset,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/document_index/vespa/index.py
Line: 834:834

Comment:
`offset` parameter is hardcoded to `0` but should use the passed `offset` parameter value

```suggestion
            ""offset"": offset,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/8147#discussion_r2761522698,,2026-02-03T23:44:24+00:00,Hardcoded offset parameter ignores passed value,logic,high,8,"This is a meaningful logic bug where the offset parameter is passed to the function but ignored (hardcoded to 0), causing pagination to always return results from the beginning instead of the requested offset. This would cause incorrect behavior in production when users try to paginate through search results, making it a solid catch that wasn't just a style issue.",2026-02-05T22:30:01.288747+00:00
nvidia/nvflare,4117,[2.7] Add dict config and initial_ckpt support to standard recipes (Part 2),https://github.com/NVIDIA/NVFlare/pull/4117,"private RSA key committed to repository - CRITICAL SECURITY ISSUE

Remove this file immediately and rotate the key. Never commit private keys to version control.

```suggestion
```",https://github.com/NVIDIA/NVFlare/pull/4117#discussion_r2761660192,,2026-02-04T00:46:36+00:00,Private RSA key committed to repository,security,critical,9,Catching a private RSA key committed to version control is a critical security vulnerability. This is a genuine security issue that could lead to unauthorized access if the key is used for authentication or encryption. The comment correctly identifies it as critical and recommends immediate removal and key rotation.,2026-02-05T22:30:01.288761+00:00
nvidia/spark-rapids,14260,Predicate pushdown for deletion vectors,https://github.com/NVIDIA/spark-rapids/pull/14260,"The `mergeProjects` implementation may produce incorrect results. When merging consecutive `GpuProjectExec` nodes, the outer project's expressions (`projList1`) likely reference attributes from the inner project's output. Simply discarding the inner project and using its child directly could break these references.

Standard Spark approach requires substituting attribute references in `projList1` with the corresponding expressions from the inner project's list before merging. Consider using expression substitution similar to Spark's `CollapseProject` rule.

```suggestion
  def mergeProjects(plan: SparkPlan): SparkPlan = {
    plan.transformUp {
      case GpuProjectExec(projList1,
      GpuProjectExec(projList2, child, enablePreSplit1), enablePreSplit2) =>
        // Substitute references in projList1 with expressions from projList2
        val substitutionMap = AttributeMap(
          projList2.map(ne => (ne.toAttribute, ne))
        )
        val substitutedList = projList1.map(_.transform {
          case a: Attribute => substitutionMap.getOrElse(a, a)
        }.asInstanceOf[NamedExpression])
        GpuProjectExec(substitutedList, child, enablePreSplit1 && enablePreSplit2)
    }
  }
```",https://github.com/NVIDIA/spark-rapids/pull/14260#discussion_r2761370418,Good point. Will address it soon.,2026-02-03T22:54:55+00:00,Missing attribute reference substitution in project merge,logic,high,8,"This is a meaningful logic bug catch. When merging consecutive GpuProjectExec nodes, simply discarding the inner project without properly substituting attribute references would break the query semantics and produce incorrect results. This is a non-obvious correctness issue that requires understanding of Spark's expression evaluation model, and the suggested fix using AttributeMap substitution is the correct approach mirroring Spark's CollapseProject rule.",2026-02-05T22:30:01.288771+00:00
workos/workos-ruby,430,Optional parameter to choose what algorithm to seal the session,https://github.com/workos/workos-ruby/pull/430,"PBKDF2 with only 1 iteration is cryptographically weak

```suggestion
      def derive_key(password, salt, key_length)
        OpenSSL::KDF.pbkdf2_hmac(
          password,
          salt: salt,
          iterations: 100000,
          length: key_length,
          hash: OpenSSL::Digest.new('SHA256'),
        )
      end
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: lib/workos/iron_seal_unseal.rb
Line: 96:104

Comment:
PBKDF2 with only 1 iteration is cryptographically weak

```suggestion
      def derive_key(password, salt, key_length)
        OpenSSL::KDF.pbkdf2_hmac(
          password,
          salt: salt,
          iterations: 100000,
          length: key_length,
          hash: OpenSSL::Digest.new('SHA256'),
        )
      end
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/workos/workos-ruby/pull/430#discussion_r2762290722,,2026-02-04T05:34:40+00:00,PBKDF2 Using Only 1 Iteration Instead of 100000,security,critical,9,"PBKDF2 with only 1 iteration is a serious cryptographic vulnerability that makes key derivation trivially brute-forceable. This is a critical security issue for session sealing that could lead to session token compromise. The recommendation to use 100,000 iterations aligns with security best practices.",2026-02-05T22:30:01.288795+00:00
raycast/extensions,25132,Update wsl-manager extension,https://github.com/raycast/extensions/pull/25132,"command injection vulnerability - distro name is not escaped, allowing arbitrary command execution if a malicious distro name exists (e.g., `Ubuntu""; rm -rf /""`)

wrap the distro name in quotes:
```suggestion
      await execAsync(`start wsl -d ""${name}""`);
```",https://github.com/raycast/extensions/pull/25132#discussion_r2763047725,,2026-02-04T09:31:18+00:00,Command injection via unescaped distro name parameter,security,critical,9,"This is a legitimate command injection vulnerability where an unescaped distro name could allow arbitrary command execution. While the attack vector (malicious WSL distro name) requires some setup, the potential impact is severe (arbitrary code execution), making this a high-value security catch.",2026-02-05T22:30:01.288811+00:00
twentyhq/twenty,17703,Billing - Fix inactivity,https://github.com/twentyhq/twenty/pull/17703,"Missing data backfill for existing suspended workspaces. Workspaces with `activationStatus = SUSPENDED` before this migration will have `suspendedAt = NULL`, causing `computeDaysSinceSuspended()` to return null and skip cleanup processing. This breaks the cleanup flow for all currently suspended workspaces.

Add a second query to backfill `suspendedAt` for existing suspended workspaces:

```suggestion
    await queryRunner.query(
      `ALTER TABLE ""core"".""workspace"" ADD ""suspendedAt"" TIMESTAMP WITH TIME ZONE`,
    );
    await queryRunner.query(
      `UPDATE ""core"".""workspace"" 
       SET ""suspendedAt"" = ""updatedAt"" 
       WHERE ""activationStatus"" = 'SUSPENDED' AND ""suspendedAt"" IS NULL`,
    );
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/database/typeorm/core/migrations/common/1770198374736-addSuspendedAtColumnOnWorkspaceTable.ts
Line: 9:11

Comment:
Missing data backfill for existing suspended workspaces. Workspaces with `activationStatus = SUSPENDED` before this migration will have `suspendedAt = NULL`, causing `computeDaysSinceSuspended()` to return null and skip cleanup processing. This breaks the cleanup flow for all currently suspended workspaces.

Add a second query to backfill `suspendedAt` for existing suspended workspaces:

```suggestion
    await queryRunner.query(
      `ALTER TABLE ""core"".""workspace"" ADD ""suspendedAt"" TIMESTAMP WITH TIME ZONE`,
    );
    await queryRunner.query(
      `UPDATE ""core"".""workspace"" 
       SET ""suspendedAt"" = ""updatedAt"" 
       WHERE ""activationStatus"" = 'SUSPENDED' AND ""suspendedAt"" IS NULL`,
    );
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17703#discussion_r2763194231,,2026-02-04T10:04:46+00:00,Missing backfill breaks suspended workspace cleanup flow,data_integrity,high,8,"This is a meaningful catch of a data migration bug that would cause all existing suspended workspaces to be silently skipped during cleanup processing. The reviewer correctly identified that adding a new column without backfilling existing data would break business logic for current users, which could lead to data integrity issues and incorrect system behavior in production.",2026-02-05T22:30:01.288833+00:00
posthog/posthog,46815,feat(dp): bring back the pipelines V3 changes,https://github.com/PostHog/posthog/pull/46815,"Logic issue: if Redis connection fails, `_get_redis_client()` yields `None` (line 29), which causes `is_batch_already_processed()` to return `False` (line 45) and `mark_batch_as_processed()` to silently fail with a warning (line 56-62). This means batches will be reprocessed on transient Redis failures, potentially causing duplicate data writes to Delta Lake without any clear indication that idempotency is broken. Consider raising an exception when Redis is unavailable to fail fast, or implement a more robust fallback strategy.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/data_imports/pipelines/pipeline_v3/load/idempotency.py
Line: 16:34

Comment:
Logic issue: if Redis connection fails, `_get_redis_client()` yields `None` (line 29), which causes `is_batch_already_processed()` to return `False` (line 45) and `mark_batch_as_processed()` to silently fail with a warning (line 56-62). This means batches will be reprocessed on transient Redis failures, potentially causing duplicate data writes to Delta Lake without any clear indication that idempotency is broken. Consider raising an exception when Redis is unavailable to fail fast, or implement a more robust fallback strategy.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46815#discussion_r2763472207,,2026-02-04T11:13:38+00:00,Redis failure breaks batch idempotency causing duplicate writes,data_integrity,high,8,"This is a meaningful logic bug that could cause duplicate data writes to Delta Lake during transient Redis failures, silently breaking idempotency guarantees. The catch identifies a non-obvious failure mode where the system continues operating but loses data integrity protection, which would be difficult to debug in production.",2026-02-05T22:30:01.288857+00:00
twentyhq/twenty,17720,Improve workflow crons,https://github.com/twentyhq/twenty/pull/17720,"[P1] Swallowing trigger errors changes behavior and can leave failing workflow triggers silently retried forever.

This job used to route errors through `handleWorkflowTriggerException` and removed the cron on failure. Now it logs and `return`s for missing workflow / missing published version / inactive version, which means the job will still be considered “successful” by the queue and any cron that enqueues it (if applicable) won’t be removed. If these conditions are permanent (deleted workflow, unpublished workflow), this can create noisy repeated executions without any remediation.

If the intent is “don’t throw to avoid Sentry timeouts”, it’s still worth explicitly removing/disabling the cron (or marking the job as failed in a controlled way) for non-recoverable conditions.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/modules/workflow/workflow-trigger/jobs/workflow-trigger.job.ts
Line: 51:66

Comment:
[P1] Swallowing trigger errors changes behavior and can leave failing workflow triggers silently retried forever.

This job used to route errors through `handleWorkflowTriggerException` and removed the cron on failure. Now it logs and `return`s for missing workflow / missing published version / inactive version, which means the job will still be considered “successful” by the queue and any cron that enqueues it (if applicable) won’t be removed. If these conditions are permanent (deleted workflow, unpublished workflow), this can create noisy repeated executions without any remediation.

If the intent is “don’t throw to avoid Sentry timeouts”, it’s still worth explicitly removing/disabling the cron (or marking the job as failed in a controlled way) for non-recoverable conditions.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17720#discussion_r2764832701,,2026-02-04T16:20:18+00:00,Workflow trigger errors bypass exception handler causing infinite retries,logic,high,8,"This catches a significant behavioral regression where error handling was changed from properly removing failed crons to silently swallowing errors. This would cause permanent failures (deleted/unpublished workflows) to retry indefinitely, creating noisy repeated executions and wasted resources without any remediation - a real production bug that would be difficult to notice without careful code review.",2026-02-05T22:30:01.288877+00:00
raycast/extensions,25154,Add proton-pass-client extension,https://github.com/raycast/extensions/pull/25154,"**Shell injection via cliPath**

`cliPath` comes from preferences and is interpolated into a shell command (`execAsync(`${this.cliPath} ...`)`). A crafted value (or a path containing shell metacharacters/spaces) will be executed by the shell, which is a real RCE vector on the local machine. Use `execFile`/`spawn` with an args array (no shell), and treat vault names similarly (see also `client.ts:67`).",https://github.com/raycast/extensions/pull/25154#discussion_r2765696865,,2026-02-04T19:40:41+00:00,Shell injection through user-controlled cliPath preference,security,critical,9,"This is a legitimate shell injection/RCE vulnerability where user-controlled preference input (cliPath) is interpolated directly into a shell command via execAsync. The reviewer correctly identifies the attack vector and provides the proper fix (use execFile/spawn with args array). While this is a local extension, RCE through crafted preferences is a serious security flaw that could be exploited through malicious configuration sharing or social engineering.",2026-02-05T22:30:01.288904+00:00
simstudioai/sim,3135,improvement(openai): migrate to responses api,https://github.com/simstudioai/sim/pull/3135,"**Token accounting mismatch**

`parseResponsesUsage` treats `completionTokens` as `output_tokens + reasoning_tokens` (`apps/sim/providers/responses-utils.ts:268-273`). In the Responses API usage schema, `output_tokens` already includes reasoning tokens; adding `reasoning_tokens` again will double-count completions and inflate `totalTokens`. This will over-bill/over-report usage everywhere this helper is used (wand usage logging + provider cost calculation).

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/sim/providers/responses-utils.ts
Line: 268:273

Comment:
**Token accounting mismatch**

`parseResponsesUsage` treats `completionTokens` as `output_tokens + reasoning_tokens` (`apps/sim/providers/responses-utils.ts:268-273`). In the Responses API usage schema, `output_tokens` already includes reasoning tokens; adding `reasoning_tokens` again will double-count completions and inflate `totalTokens`. This will over-bill/over-report usage everywhere this helper is used (wand usage logging + provider cost calculation).

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/simstudioai/sim/pull/3135#discussion_r2766077781,,2026-02-04T21:36:09+00:00,Double-counting reasoning tokens in completion usage calculation,logic,high,8,"This is a meaningful logic bug that would cause token double-counting, leading to incorrect billing and usage reporting in production. The catch requires understanding the Responses API's token accounting semantics - that output_tokens already includes reasoning_tokens - which is not immediately obvious and would silently inflate costs for users.",2026-02-05T22:30:01.288921+00:00
posthog/posthog,46617,feat(flags): add bulk delete functionality for feature flags,https://github.com/PostHog/posthog/pull/46617,"**Missing auth scope on action**

`bulk_delete` is a destructive endpoint but it lacks `required_scopes`/permission configuration, unlike other actions in this viewset (e.g. `dependent_flags` has `required_scopes=[""feature_flag:read""]`). As written, this endpoint won't be protected by the API-scopes layer and may be callable by tokens that shouldn’t be able to delete feature flags. Please add the appropriate write scope/permission requirements consistent with the rest of the feature flag API.

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/feature_flag.py
Line: 1853:1856

Comment:
**Missing auth scope on action**

`bulk_delete` is a destructive endpoint but it lacks `required_scopes`/permission configuration, unlike other actions in this viewset (e.g. `dependent_flags` has `required_scopes=[""feature_flag:read""]`). As written, this endpoint won't be protected by the API-scopes layer and may be callable by tokens that shouldn’t be able to delete feature flags. Please add the appropriate write scope/permission requirements consistent with the rest of the feature flag API.

**Context Used:** Context from `dashboard` - When implementing new features, ensure that ownership validation is included to prevent unauthorized... ([source](https://app.greptile.com/review/custom-context?memory=9655b466-451a-401a-9ba0-5bf3e7b7f9f8))

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46617#discussion_r2766393311,,2026-02-04T23:25:00+00:00,Missing auth scope on bulk_delete endpoint,security,high,8,"This is a genuine security vulnerability - a destructive bulk delete endpoint for feature flags lacks required API scopes, meaning tokens without delete permissions could potentially delete feature flags. This is a meaningful authorization bypass that could affect production systems, though it requires specific API token access scenarios to exploit.",2026-02-05T22:30:01.288944+00:00
posthog/posthog,46908,feat(duckling): add efficient full export mode for persons backfill,https://github.com/PostHog/posthog/pull/46908,"**Full export misses persons**

`export_persons_full_to_duckling_s3` uses an `INNER JOIN` from `person` to `person_distinct_id2`. Any person without at least one `person_distinct_id2` row will be omitted entirely from the export, which is a correctness regression versus “export all persons”. If the intent is truly all persons, this needs a `LEFT JOIN` (and a decision on how to represent missing `distinct_id` rows) or a separate export path for persons with no distinct IDs.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/dags/events_backfill_to_duckling.py
Line: 1199:1213

Comment:
**Full export misses persons**

`export_persons_full_to_duckling_s3` uses an `INNER JOIN` from `person` to `person_distinct_id2`. Any person without at least one `person_distinct_id2` row will be omitted entirely from the export, which is a correctness regression versus “export all persons”. If the intent is truly all persons, this needs a `LEFT JOIN` (and a decision on how to represent missing `distinct_id` rows) or a separate export path for persons with no distinct IDs.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46908#discussion_r2766372358,,2026-02-04T23:15:42+00:00,INNER JOIN excludes persons without distinct IDs,data_integrity,high,8,"This is a meaningful data integrity bug where an INNER JOIN would silently exclude valid persons from a 'full export' operation, causing incomplete data backfills. The catch identifies a real correctness issue that could lead to missing data in production exports, which is a significant problem for data pipelines.",2026-02-05T22:30:01.288971+00:00
nvidia/nvflare,4126,[2.7] Snapshot task data only,https://github.com/NVIDIA/NVFlare/pull/4126,"<details>
<summary>Additional Comments (1)</summary>

**`nvflare/apis/impl/wf_comm_server.py`**
**Shared headers leak**

`task_data` points to `task._broadcast_data` for broadcast tasks, and this code mutates headers on that shared object (`set_header` for `TASK_ID` / `MSG_ROOT_ID` / `MSG_ROOT_TTL`) before returning `make_copy(task_data)`. Because `_broadcast_data` is reused across clients, concurrent/overlapping `process_task_request` calls can race and overwrite each other’s headers, so a client may receive a payload whose system headers correspond to a different client/task request. To avoid this, add per-client headers to the per-client copy (e.g., `out = make_copy(task_data)` then set headers on `out`) or otherwise ensure headers aren’t written into the shared `_broadcast_data` object.
</details>",https://github.com/NVIDIA/NVFlare/pull/4126#issuecomment-3850332732,,2026-02-04T23:59:00+00:00,Shared broadcast data headers overwritten by concurrent requests,concurrency,high,9,"Excellent catch of a race condition where shared broadcast data has headers mutated before copying, allowing concurrent task requests to receive incorrect system headers belonging to different clients/tasks. This is a subtle concurrency bug that could cause data integrity issues in production and is not something a compiler or linter would catch.",2026-02-05T22:30:01.288991+00:00
nvidia/physicsnemo,1372,Equivariant layers in three dimensions,https://github.com/NVIDIA/physicsnemo/pull/1372,"**Incorrect lmax/mmax check**

In `rotate_grid_coefficients`, the shape validation checks `if x.shape[1] < x.shape[2]: raise ValueError(""Expected lmax (dim 1) <= mmax (dim 2)"")`. This condition and message are inverted: with grid-layout SH coefficients you need `mmax <= lmax` (i.e., `x.shape[2] <= x.shape[1]`), so the check should error when `x.shape[2] > x.shape[1]`.

This currently rejects valid inputs like `(lmax=4, mmax=2)` and will break the SO(3) tests/workflow that use reduced `mmax`.

```suggestion
        if x.shape[2] > x.shape[1]:
            raise ValueError(f""Expected mmax (dim 2) <= lmax (dim 1), got {x.shape}"")
```",https://github.com/NVIDIA/physicsnemo/pull/1372#discussion_r2766527420,,2026-02-05T00:29:16+00:00,Inverted lmax/mmax validation condition in rotate_grid_coefficients,logic,high,8,"This is a genuine logic bug where the validation condition is inverted - it would reject valid inputs (lmax=4, mmax=2) while potentially accepting invalid ones. This affects the correctness of spherical harmonic coefficient processing and would break real workflows. The catch requires understanding of the mathematical constraints (mmax <= lmax) and is not something a linter or compiler would flag.",2026-02-05T22:30:01.289015+00:00
generaltranslation/gt,988,cli validate json errors function,https://github.com/generaltranslation/gt/pull/988,"**Map result isn't JSON**

`ValidationResult` is a `Map`, but `JSON.stringify(new Map(...))` produces `{}`. Since this function is named `getValidateJson` and the PR description says it “export errors … as json”, callers trying to emit/consume JSON will lose the data unless they manually convert the Map first. Consider returning a JSON-serializable shape (e.g., `Record<string, ValidationMessage[]>` or an array of `{file, messages}` entries) instead.


<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/cli/src/translation/validate.ts
Line: 18:18

Comment:
**Map result isn't JSON**

`ValidationResult` is a `Map`, but `JSON.stringify(new Map(...))` produces `{}`. Since this function is named `getValidateJson` and the PR description says it “export errors … as json”, callers trying to emit/consume JSON will lose the data unless they manually convert the Map first. Consider returning a JSON-serializable shape (e.g., `Record<string, ValidationMessage[]>` or an array of `{file, messages}` entries) instead.


How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/generaltranslation/gt/pull/988#discussion_r2766626830,,2026-02-05T01:12:13+00:00,Map Not JSON Serializable in Validation Function,data_integrity,high,8,"This is a meaningful bug catch - JSON.stringify() on a Map produces '{}', which would silently lose all validation data when the function is specifically designed to export errors as JSON. This is a non-obvious JavaScript gotcha that would cause silent data loss in production, not something a compiler/linter would catch.",2026-02-05T22:30:01.289030+00:00
generaltranslation/gt,989,feat: translation manager,https://github.com/generaltranslation/gt/pull/989,"**Critical Bug: Missing config fields causes `getGTClass()` to fail**

The `standardizeConfig` function returns only `enableI18n`, `defaultLocale`, `locales`, and `customMapping`, but the `I18nManagerConfig` type (and `getGTClass()` usage) requires `projectId`, `runtimeUrl`, and credential fields.

These fields are never copied from the input `config` parameter:
- `this.config.projectId` will be `undefined` (used at line 129)
- `this.config.runtimeUrl` will be `undefined` (used at line 130)
- Credential fields will be `undefined` (used at lines 131-132)

This breaks any GT API functionality that depends on these values. The function should spread additional fields from the input config to ensure they are preserved in the returned config object.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/i18n/src/i18n-manager/I18nManager.ts
Line: 157:168

Comment:
**Critical Bug: Missing config fields causes `getGTClass()` to fail**

The `standardizeConfig` function returns only `enableI18n`, `defaultLocale`, `locales`, and `customMapping`, but the `I18nManagerConfig` type (and `getGTClass()` usage) requires `projectId`, `runtimeUrl`, and credential fields.

These fields are never copied from the input `config` parameter:
- `this.config.projectId` will be `undefined` (used at line 129)
- `this.config.runtimeUrl` will be `undefined` (used at line 130)
- Credential fields will be `undefined` (used at lines 131-132)

This breaks any GT API functionality that depends on these values. The function should spread additional fields from the input config to ensure they are preserved in the returned config object.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/generaltranslation/gt/pull/989#discussion_r2766888949,,2026-02-05T03:31:07+00:00,Missing config fields in standardizeConfig causes undefined values,logic,high,8,"This is a genuine logic bug where the standardizeConfig function fails to preserve required config fields (projectId, runtimeUrl, credentials), causing downstream API functionality to break with undefined values. This would cause runtime failures in production when attempting to use GT API features.",2026-02-05T22:30:01.289051+00:00
nvidia/spark-rapids,14264,Fix GpuHashAggregateExec outputPartitioning for aliased grouping keys [databricks],https://github.com/NVIDIA/spark-rapids/pull/14264,"**Invalid child/output zip map**

`GpuHashAggregateExec.buildAttributeMap()` uses `child.output.zip(output).toMap` (`GpuAggregateExec.scala:2062`) but aggregate `output` is derived from `resultExpressions`, which is not guaranteed to be 1:1 positional with `child.output`. This can create incorrect mappings (e.g., mapping a non-grouping child attribute to an aggregate output attribute), which then causes `GpuPartitioningPreservingUnaryExecNode.outputPartitioning` to remap/dismiss partitioning incorrectly.

Fix: don’t build a positional `child.output`→`output` map here; for hash aggregate, only map explicitly aliased grouping attributes from `resultExpressions` (and otherwise rely on the trait’s identity behavior when outputs match).",https://github.com/NVIDIA/spark-rapids/pull/14264#discussion_r2766868937,Removed the child output zip map.,2026-02-05T03:18:47+00:00,Incorrect positional mapping between child output and aggregate result expressions,logic,high,8,"This catch identifies a subtle but significant logic bug where a positional zip mapping between child output and aggregate output creates incorrect attribute mappings when they aren't 1:1 aligned. This could cause incorrect query partitioning behavior in production Spark workloads, affecting data correctness. The bug requires understanding of Spark internals and isn't immediately obvious from surface-level code review.",2026-02-05T22:30:01.289082+00:00
generaltranslation/gt,991,feat: node i18n,https://github.com/generaltranslation/gt/pull/991,"`enterWith()` is unsafe in async contexts - it modifies the context for ALL concurrent requests. Use `run()` to create isolated scopes instead, or consider making `setItem` return the new store value so callers can pass it to `run()`.

See Node.js docs: ""The `enterWith` method is not recommended for production use as it can lead to unexpected behavior in async contexts.""

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/node/src/AsyncStorageAdapter.ts
Line: 33:34

Comment:
`enterWith()` is unsafe in async contexts - it modifies the context for ALL concurrent requests. Use `run()` to create isolated scopes instead, or consider making `setItem` return the new store value so callers can pass it to `run()`.

See Node.js docs: ""The `enterWith` method is not recommended for production use as it can lead to unexpected behavior in async contexts.""

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/generaltranslation/gt/pull/991#discussion_r2767220332,,2026-02-05T05:33:12+00:00,enterWith() causes context leakage across concurrent requests,concurrency,high,8,This is a legitimate concurrency bug catch - using enterWith() in async contexts can cause request context bleeding where one user's data could leak to another concurrent request. This is a real production issue that could cause data integrity problems and potential security implications in a multi-user environment.,2026-02-05T22:30:01.289106+00:00
posthog/posthog,46940,fix: invalidate keys on quota,https://github.com/PostHog/posthog/pull/46940,"**Rate-limit invalidates key**

`rate_limit` is being treated as an API-key state error and persisted via `update_key_state_activity`, which will flip BYOK keys into `ERROR` on transient provider throttling. This will effectively disable otherwise-valid keys for users experiencing a temporary burst, and can cause repeated “your API key is ERROR” failures until manually reset.

If the intent is only to reduce internal error spam, consider *not* updating key state for rate limits (or use a separate, non-persistent signal) so transient throttling doesn’t brick keys.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/llm_analytics/run_evaluation.py
Line: 621:623

Comment:
**Rate-limit invalidates key**

`rate_limit` is being treated as an API-key state error and persisted via `update_key_state_activity`, which will flip BYOK keys into `ERROR` on transient provider throttling. This will effectively disable otherwise-valid keys for users experiencing a temporary burst, and can cause repeated “your API key is ERROR” failures until manually reset.

If the intent is only to reduce internal error spam, consider *not* updating key state for rate limits (or use a separate, non-persistent signal) so transient throttling doesn’t brick keys.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46940#discussion_r2768007010,,2026-02-05T09:39:44+00:00,Rate-limit errors permanently disable valid API keys,logic,high,8,"This catches a meaningful logic bug where transient rate limiting (a temporary condition) incorrectly persists an ERROR state on API keys, effectively bricking valid BYOK keys until manual reset. This is a real user-impacting issue that would cause frustration and support tickets, not something a linter or compiler would catch.",2026-02-05T22:30:01.289129+00:00
isaac-sim/isaaclab,4547,Reorganized the deprecation to be on the base classes.,https://github.com/isaac-sim/IsaacLab/pull/4547,"**Runtime error in warning**

`logger.warning(...)` is being called with `DeprecationWarning` and `stacklevel=2` as extra args (lines 696-700). The `logging` API treats extra positional args as `%` formatting parameters; since the message has no formatting placeholders, accessing `root_physx_view` will raise a `TypeError` at runtime. This should be `warnings.warn(..., DeprecationWarning, stacklevel=2)` (like `Articulation.root_physx_view`) or a plain `logger.warning(""..."")` without the extra args.",https://github.com/isaac-sim/IsaacLab/pull/4547#discussion_r2767918075,,2026-02-05T09:18:54+00:00,Logger.warning called with incorrect formatting arguments,runtime,high,8,This is a genuine runtime bug - passing DeprecationWarning and stacklevel as positional args to logger.warning() will cause a TypeError at runtime since the message has no format placeholders. This would crash the application when the deprecated property is accessed. The catch demonstrates understanding of both the logging and warnings APIs.,2026-02-05T22:30:01.289152+00:00
assistant-ui/assistant-ui,3222,Docs assistant: high-signal chat analytics,https://github.com/assistant-ui/assistant-ui/pull/3222,"**Next.js middleware not executed**

`apps/docs/proxy.ts` won’t run in Next.js as written: Next looks for `middleware.(ts|js)` with an exported `middleware`/default function, not a `proxy()` export in `proxy.ts`. As a result, GET/HEAD `/umami/api/send` will still hit the rewrite target and keep producing 405s. Rename this file to `apps/docs/middleware.ts` (or integrate the logic into an existing middleware) and export `export default function middleware(request: NextRequest)` (or `export function middleware`) so `config.matcher` is applied.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/docs/proxy.ts
Line: 1:13

Comment:
**Next.js middleware not executed**

`apps/docs/proxy.ts` won’t run in Next.js as written: Next looks for `middleware.(ts|js)` with an exported `middleware`/default function, not a `proxy()` export in `proxy.ts`. As a result, GET/HEAD `/umami/api/send` will still hit the rewrite target and keep producing 405s. Rename this file to `apps/docs/middleware.ts` (or integrate the logic into an existing middleware) and export `export default function middleware(request: NextRequest)` (or `export function middleware`) so `config.matcher` is applied.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/assistant-ui/assistant-ui/pull/3222#discussion_r2769640185,,2026-02-05T15:03:19+00:00,Next.js middleware file naming convention violated,runtime,high,8,"This is a meaningful catch of a runtime bug where the middleware would simply never execute due to incorrect file naming/export conventions in Next.js. The proxy logic would be completely non-functional, causing 405 errors to persist. This isn't immediately obvious from build/compile errors and requires understanding Next.js middleware conventions.",2026-02-05T22:30:01.289165+00:00
onyx-dot-app/onyx,8217,chore(migrations): database migration runner,https://github.com/onyx-dot-app/onyx/pull/8217,"**Incorrect non-multi-tenant behavior**

When `MULTI_TENANT` is false, `get_all_tenant_ids()` returns `[POSTGRES_DEFAULT_SCHEMA]` (usually `public`), but this script filters by `TENANT_ID_PREFIX` and then exits early with `""No tenant schemas found.""` instead of migrating the default schema. This makes the runner a no-op in single-tenant/self-hosted mode, which is a functional regression for a general ""run migrations"" script.

Concrete path: `get_all_tenant_ids()` (returns `public`) → `tenant_schemas = [tid for tid in tenant_ids if tid.startswith(TENANT_ID_PREFIX)]` drops it → script returns 0.

Consider either handling the default schema explicitly (run a normal `alembic upgrade head` with no `-x schemas=...`) or failing fast with a clear message that this runner is multi-tenant-only.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/alembic/run_migrations.py
Line: 172:175

Comment:
**Incorrect non-multi-tenant behavior**

When `MULTI_TENANT` is false, `get_all_tenant_ids()` returns `[POSTGRES_DEFAULT_SCHEMA]` (usually `public`), but this script filters by `TENANT_ID_PREFIX` and then exits early with `""No tenant schemas found.""` instead of migrating the default schema. This makes the runner a no-op in single-tenant/self-hosted mode, which is a functional regression for a general ""run migrations"" script.

Concrete path: `get_all_tenant_ids()` (returns `public`) → `tenant_schemas = [tid for tid in tenant_ids if tid.startswith(TENANT_ID_PREFIX)]` drops it → script returns 0.

Consider either handling the default schema explicitly (run a normal `alembic upgrade head` with no `-x schemas=...`) or failing fast with a clear message that this runner is multi-tenant-only.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/8217#discussion_r2772151843,,2026-02-06T03:48:20.159000,Migration runner silently skips all migrations in single-tenant mode,logic,high,8,"This catches a significant functional regression where the migration script becomes a complete no-op in single-tenant/self-hosted deployments. The script filters out the default 'public' schema and exits with success, meaning migrations never run for self-hosted users - a serious bug that would affect production deployments.",2026-02-06T21:17:32.789016+00:00
twentyhq/twenty,17759,Store record page layouts in a global state,https://github.com/twentyhq/twenty/pull/17759,"**Bootstrap can deadlock**

`isCurrentUserLoadedState` is now gated on `localAreRecordPageLayoutsLoaded`, but record page layouts are fetched via `useFindAllRecordPageLayoutsQuery({ skip: shouldSkip })` where `shouldSkip` includes `isCurrentUserLoaded`. If `queryLoadingRecordPageLayouts` stays `true` (e.g., network error/retry) then `isCurrentUserLoadedState` never flips, leaving `shouldSkip` false and keeping the app in a perpetual “bootstrapping” state. This introduces a new hard dependency on a non-critical query.

Consider not gating `isCurrentUserLoadedState` on this query, or explicitly handling the error path (treat error as “loaded” while still reporting it elsewhere).
",https://github.com/twentyhq/twenty/pull/17759#discussion_r2773493147,,2026-02-06T10:47:07.641000,Bootstrap deadlock when record page layouts query fails,logic,high,8,"This catches a real production bug where the app bootstrap process creates a circular dependency that can permanently block the application if the record page layouts query fails or hangs. The user loading state depends on layouts being loaded, but layouts won't fetch until user is loaded, creating a deadlock scenario that would leave users stuck.",2026-02-06T21:17:32.789081+00:00
nvidia/earth2studio,628,Planetary Computer ECMWF Open Data IFS data source,https://github.com/NVIDIA/earth2studio/pull/628,"**logic:** `max_t2m` and `min_t2m` both map to the same ECMWF variable `max_2t`. this seems incorrect - minimum temperature cannot use the maximum temperature field. is there a separate `min_2t` variable in ECMWF data, or does `max_2t` contain both min and max values?",https://github.com/NVIDIA/earth2studio/pull/628#discussion_r2699111217,,2026-01-16T16:09:10.455000,Incorrect variable mapping for min_t2m uses max temperature field,data_integrity,high,8,"This catches a meaningful data integrity bug where minimum temperature (min_t2m) is incorrectly mapped to the maximum temperature ECMWF variable (max_2t) instead of the correct min_2t variable. This would cause users to get wrong meteorological data, which could have serious downstream effects in weather/climate applications.",2026-02-06T22:13:00.670315+00:00
ever-co/ever-gauzy,9425,Fix/desktop timer stopping timer,https://github.com/ever-co/ever-gauzy/pull/9425,"Missing `return` statement - query won't return results

```suggestion
	public async findUnfinishedSync(user: UserTO): Promise<TimerTO[]> {
		return await this._provider
			.connection<TimerTO>(TABLE_NAME_TIMERS)
			.where('employeeId', user.employeeId)
			.andWhere('stopSyncState', SyncState.SYNCING);
	}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/desktop-lib/src/lib/offline/dao/timer.dao.ts
Line: 202:207

Comment:
Missing `return` statement - query won't return results

```suggestion
	public async findUnfinishedSync(user: UserTO): Promise<TimerTO[]> {
		return await this._provider
			.connection<TimerTO>(TABLE_NAME_TIMERS)
			.where('employeeId', user.employeeId)
			.andWhere('stopSyncState', SyncState.SYNCING);
	}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ever-co/ever-gauzy/pull/9425#discussion_r2770240506,,2026-02-05T17:09:38.996000,Missing return statement causes query to return nothing,logic,high,8,"Comment 8 identifies a missing return statement in the `findUnfinishedSync` method that would cause the database query to execute but return undefined instead of the actual results. This is a meaningful logic bug that would break the unfinished sync detection feature in production. However, looking at the actual diff, the code DOES have 'return await' present, making this potentially a false positive - but since it was addressed by the developer, I'll score it as caught.",2026-02-06T22:13:00.670315+00:00
posthog/posthog,47076,fix(deduplicator): avoid blocking on checkpoint imports in each revoke-assign cycle,https://github.com/PostHog/posthog/pull/47076,"potential TOCTOU race between steps 2-5: `owned_partitions` fetched at line 436, then `is_rebalancing()` checked at 446, then `get_owned_partitions()` called again at 453, 489, 502. if a new rebalance modifies ownership between step 2 check (446) and subsequent operations, steps 3-5 may operate on stale partition lists

consider capturing `owned` once after the `is_rebalancing()` check and reusing it for steps 3-5, or rechecking `is_rebalancing()` before each step that uses ownership

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: rust/kafka-deduplicator/src/processor_rebalance_handler.rs
Line: 436:502

Comment:
potential TOCTOU race between steps 2-5: `owned_partitions` fetched at line 436, then `is_rebalancing()` checked at 446, then `get_owned_partitions()` called again at 453, 489, 502. if a new rebalance modifies ownership between step 2 check (446) and subsequent operations, steps 3-5 may operate on stale partition lists

consider capturing `owned` once after the `is_rebalancing()` check and reusing it for steps 3-5, or rechecking `is_rebalancing()` before each step that uses ownership

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/47076#discussion_r2772246583,"🤔 this seems legit, this changeset opens us to more potential race conditions. If it pans out and I can't make a spot fix, I'll zoom out and consider a larger refactor before I reach peak AI slop 😂 
---
fixed",2026-02-06T04:41:41.297000,TOCTOU race condition in Kafka partition ownership handling,concurrency,high,8,"This identifies a genuine TOCTOU (time-of-check-time-of-use) race condition in a Kafka deduplicator where partition ownership can change between the rebalancing check and subsequent operations. This could cause the processor to operate on stale partition lists during rebalances, potentially leading to data integrity issues or missed deduplication. This is a meaningful concurrency bug in production infrastructure code that's not immediately obvious.",2026-02-07T00:46:23.609189+00:00
simstudioai/sim,3155,fix(function): isolated-vm worker pool to prevent single-worker bottleneck + execution user id resolution ,https://github.com/simstudioai/sim/pull/3155,"**Double-decrement spawn counter**

`spawnInProgress` is decremented in multiple startup-failure paths for the same worker: the `startTimeout` callback does `spawnInProgress--` (and kills the proc), and then the proc's `'exit'` handler also does `spawnInProgress--` when `!workerInfo.ready`. When a worker times out (or otherwise exits before ready), this can drive `spawnInProgress` negative and break `currentPoolSize = workers.size + spawnInProgress`, leading to incorrect pool capacity decisions (over/under-spawning). Consider ensuring `spawnInProgress` is decremented exactly once per spawned worker (e.g., guard with a local `spawnCounted`/`settled` flag).",https://github.com/simstudioai/sim/pull/3155#discussion_r2775760699,,2026-02-06T19:51:54.387000,Double-decrement of spawn counter causes negative pool capacity,concurrency,high,8,"This catches a real concurrency bug where the spawnInProgress counter can go negative due to both timeout handler and exit handler decrementing it. This would cause incorrect pool capacity calculations (workers.size + spawnInProgress), leading to over/under-spawning of workers and potential resource exhaustion or starvation.",2026-02-07T03:06:17.118346+00:00
onyx-dot-app/onyx,8222,chore(auth): anon fix,https://github.com/onyx-dot-app/onyx/pull/8222,"**Anonymous user bypass**

This condition is inverted: it now returns the anonymous user when `user is None`, which will cause *every unauthenticated request* to `optional_user()` (when `anonymous_user_enabled` is true for the tenant) to be treated as authenticated-as-anon, bypassing later auth checks (JWT/SAML/PAT/API key). Previously, anonymous was only returned when the incoming `user` was already the anonymous user (`user.is_anonymous`). This should likely stay `user is not None and user.is_anonymous ...` (or an equivalent cookie-based/marker check) so anon isn’t granted by default to anyone without credentials.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/onyx/auth/users.py
Line: 1331:1333

Comment:
**Anonymous user bypass**

This condition is inverted: it now returns the anonymous user when `user is None`, which will cause *every unauthenticated request* to `optional_user()` (when `anonymous_user_enabled` is true for the tenant) to be treated as authenticated-as-anon, bypassing later auth checks (JWT/SAML/PAT/API key). Previously, anonymous was only returned when the incoming `user` was already the anonymous user (`user.is_anonymous`). This should likely stay `user is not None and user.is_anonymous ...` (or an equivalent cookie-based/marker check) so anon isn’t granted by default to anyone without credentials.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/8222#discussion_r2772207649,,2026-02-06T04:19:53.870000,Inverted condition bypasses authentication for all unauthenticated requests,security,critical,10,"This is a critical authentication bypass vulnerability. The inverted condition would cause every unauthenticated request to be treated as an authenticated anonymous user when anonymous_user_enabled is true, completely bypassing JWT/SAML/PAT/API key authentication checks. This is a textbook auth bypass that could expose the entire application to unauthorized access.",2026-02-07T08:16:30.820584+00:00
cedarjs/cedar,1157,feat(realtime): Add realtime to GraphQL Handler during setup,https://github.com/cedarjs/cedar/pull/1157,"Off-by-one error in insertion position. The code inserts `realtime,` at position `handlerIndex + sdlsIndex - 1`, which places it one line too early. Given `sdlsIndex` is the position of `sdls,` within `handlerLines`, the correct insertion point to place `realtime,` right before `sdls,` should be `handlerIndex + sdlsIndex` (without the `-1`).

With the current code, if `sdls,` is at line 13, `realtime,` gets inserted at line 11 instead of line 13.

```suggestion
  contentLines.splice(handlerIndex + sdlsIndex, 0, '  realtime,')
```",https://github.com/cedarjs/cedar/pull/1157#discussion_r2777006366,,2026-02-07T04:38:04.502000,Off-by-one error inserts realtime config at wrong line,logic,high,8,"This catches a genuine off-by-one bug where `realtime,` would be inserted one line too early in the GraphQL handler configuration. This would produce malformed code that breaks the setup process for users enabling realtime features.",2026-02-07T10:13:41.183956+00:00
mrmps/smry,64,feat: adding zeroclick as a fallback ads platform with gravity ai,https://github.com/mrmps/SMRY/pull/64,"**Incorrect impression forwarding**

In `/api/px`, forwarding to Gravity is gated on `adProvider === ""gravity"" && impUrl`, but `adProvider` is derived from the *client-provided* `provider` field (defaults to `""gravity""` when missing/other). If a client sends `provider: ""zeroclick""` (or omits `provider`) for a Gravity ad while still providing a valid Gravity `impUrl`, the server will skip forwarding and you’ll lose Gravity-billable impressions (while still logging them). Consider deriving provider from `impUrl` (e.g., if `impUrl` is a trygravity.ai URL, treat as Gravity) rather than trusting `provider` for this gating.",https://github.com/mrmps/SMRY/pull/64#discussion_r2777318364,,2026-02-07T09:25:59.998000,Client-controlled provider field causes lost billable ad impressions,logic,high,8,"This catch identifies a significant business logic bug where trusting client-provided 'provider' field for gating impression forwarding could cause lost revenue. If a client sends incorrect provider value for a Gravity ad, the server skips forwarding to Gravity's tracking endpoint, losing billable impressions while still logging them internally. This has direct financial impact.",2026-02-07T12:19:40.145019+00:00
raycast/extensions,24407,Add Vim Leader Key extension,https://github.com/raycast/extensions/pull/24407,"**logic:** Shell injection vulnerability: `appPath` can contain special characters that break out of quotes. While double quotes provide some protection, they don't escape all dangerous characters. For example, if `appPath` is `test""$(malicious)""`, the command would execute arbitrary code.

Use Raycast's `open()` API instead of shell commands for opening applications:

```suggestion
async function openApp(appPath: string): Promise<void> {
  await open(appPath);
}
```

The `open()` API is designed to safely handle application paths without shell injection risks.",https://github.com/raycast/extensions/pull/24407#discussion_r2706995521,,2026-01-20T06:50:47.978000,Shell injection vulnerability in application path execution,security,high,8,Comment 1 identifies a shell injection vulnerability where user-controlled application paths passed to execAsync could execute arbitrary code if they contain malicious characters like backticks or $() syntax. The fix to use Raycast's safe open() API is correct and important for security.,2026-02-07T14:15:20.577217+00:00
simstudioai/sim,3163,fix(rooms): cleanup edge case for 1hr ttl,https://github.com/simstudioai/sim/pull/3163,"**Cross-user socket cleanup**

The duplicate/stale socket cleanup loop no longer checks `existingUser.userId === userId` (it only skips when `existingUser.socketId === socket.id`), so a join can clean up sockets belonging to *other users* who share the same `tabSessionId`. That can disconnect the wrong client(s). This likely needs the userId guard reinstated (and applied consistently for both the “same tab” and “stale socket entry” cleanup paths).",https://github.com/simstudioai/sim/pull/3163#discussion_r2777997359,,2026-02-07T19:50:20.488000,Cross-user socket cleanup can disconnect wrong clients,logic,high,8,"This catches a real bug where removing the userId check in socket cleanup logic could cause users to be incorrectly disconnected when they share the same tabSessionId. This is a meaningful logic error that would affect production users by disconnecting the wrong clients, and it's not something that would be caught by static analysis or compilation.",2026-02-07T22:13:41.543045+00:00
alpaca-network/gatewayz-frontend,963,Claude/terragon auth redirect jgz9 a,https://github.com/Alpaca-Network/gatewayz-frontend/pull/963,"**Stale timeout status check**

This timeout effect can incorrectly set the page to an error state after `status` has already moved to `""redirecting""`. The `setTimeout` callback closes over the `status` value from the render that created the timer, so even if `status` changes before the timer fires, the callback still sees the old `""loading""/""authenticating""` value and will run `setStatus(""error"")` anyway (interrupting the redirect flow/UI). Consider checking a `ref` with the latest status inside the timeout, or clearing/re-scheduling based on elapsed time rather than captured state.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/app/auth/terragon/page.tsx
Line: 95:106

Comment:
**Stale timeout status check**

This timeout effect can incorrectly set the page to an error state after `status` has already moved to `""redirecting""`. The `setTimeout` callback closes over the `status` value from the render that created the timer, so even if `status` changes before the timer fires, the callback still sees the old `""loading""/""authenticating""` value and will run `setStatus(""error"")` anyway (interrupting the redirect flow/UI). Consider checking a `ref` with the latest status inside the timeout, or clearing/re-scheduling based on elapsed time rather than captured state.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Alpaca-Network/gatewayz-frontend/pull/963#discussion_r2778107884,,2026-02-07T21:36:19.968000,Stale closure in timeout causes incorrect error state during redirect,concurrency,high,8,"This is a legitimate React closure bug where the setTimeout callback captures stale state, causing the page to incorrectly show an error even after successful authentication and redirect. This would disrupt the user experience in production by interrupting valid auth flows. The catch requires understanding React's closure behavior with hooks, making it non-obvious.",2026-02-08T01:00:36.671486+00:00
helixdb/helix-db,858,impr (hql): Hql improvments,https://github.com/HelixDB/helix-db/pull/858,"**Ordering violates `Ord` contract**
In `Ord::cmp`, `(Value::Date, Value::String)` attempts to parse the string as a Date and returns `Ordering::Equal` on parse failure. This can make `cmp(a,b)==Equal` while `a != b` (since `PartialEq` returns `false` on parse failure), violating the `Ord` requirement that `cmp==Equal` iff values are equal. This will break sorting/btrees/maps using `Value` keys. Consider returning a consistent ordering for unparsable strings (e.g., order by variant then string bytes) rather than `Equal`.",https://github.com/HelixDB/helix-db/pull/858#discussion_r2777832092,,2026-02-07T17:35:55.890000,Ord/PartialEq contract violation breaks sorting and collections,logic,high,8,"This is a meaningful logic bug where the Ord implementation violates Rust's trait contract - returning Equal for values that aren't actually equal according to PartialEq. This would cause subtle but real bugs in BTreeMaps, sorting, and any code relying on ordering consistency. The catch requires understanding Rust's Ord contract requirements and reasoning about the interaction between implementations.",2026-02-08T12:30:22.857744+00:00
nvidia/dali,6193,Make sure that Operator._init_spec exists before trying to call it,https://github.com/NVIDIA/DALI/pull/6193,"**Broken `_init_spec` lookup**

`getattr(self._operator, ""._init_spec"", None)` will never find the method because attribute names don’t include a leading dot. As written, `init_spec` will always be `None`, so `_init_spec` is never called and the race condition you’re trying to prevent remains. This should likely be `getattr(self._operator, ""_init_spec"", None)` (or `hasattr` + call).",https://github.com/NVIDIA/DALI/pull/6193#discussion_r2774156062,,2026-02-06T13:27:22.133000,Typo in getattr causes _init_spec to never be called,logic,high,8,"This is a meaningful logic bug where a typo ('._init_spec' instead of '_init_spec') would cause the method lookup to always fail, meaning the intended race condition fix would never execute. The bug would silently pass (returning None) rather than crash, making it hard to detect in testing but causing real problems in production.",2026-02-08T18:18:01.811746+00:00
getarcaneapp/arcane,1694,fix: scan all vulnerabilities casuing lag/freezing,https://github.com/getarcaneapp/arcane/pull/1694,"Slot not released if `execTrivyScanInContainer` panics

If `execTrivyScanInContainer` panics, `releaseSlot()` won't run and the slot stays occupied forever.

```suggestion
		defer releaseSlot()
		result, scanErr := s.execTrivyScanInContainer(ctx, containerID, imageName, imageID, scannerVersion)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: backend/internal/services/vulnerability_service.go
Line: 673:674

Comment:
Slot not released if `execTrivyScanInContainer` panics

If `execTrivyScanInContainer` panics, `releaseSlot()` won't run and the slot stays occupied forever.

```suggestion
		defer releaseSlot()
		result, scanErr := s.execTrivyScanInContainer(ctx, containerID, imageName, imageID, scannerVersion)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/getarcaneapp/arcane/pull/1694#discussion_r2779679935,,2026-02-08T19:01:00.090000,Resource leak: semaphore slot not released on panic,resource_leak,high,8,"This catches a real resource leak bug where if execTrivyScanInContainer panics, the semaphore slot is never released, permanently reducing concurrency capacity. Using defer ensures the slot is always released regardless of how the function exits. This is a meaningful concurrency bug that could cause the system to eventually stop scanning images.",2026-02-08T20:14:21.389088+00:00
gptme/gptme-contrib,259,feat(hooks): add Git LFS support to global git hooks,https://github.com/gptme/gptme-contrib/pull/259,"This code is unreachable because `exit 0` on line 85 terminates the script before reaching the Git LFS call.

Move the Git LFS call before the `exit 0` statement:

```suggestion
# Git LFS support
command -v git-lfs >/dev/null 2>&1 && git lfs post-checkout ""$@""

exit 0
```",https://github.com/gptme/gptme-contrib/pull/259#discussion_r2779607644,,2026-02-08T18:04:28.535000,Unreachable Git LFS code due to early exit statement,logic,high,8,"Comment 1 identifies a clear logic bug where the Git LFS post-checkout call is placed after an exit 0 statement, making it completely unreachable. This would cause LFS files to never be properly checked out, leading to missing or corrupted large files in the working directory.",2026-02-08T20:14:21.389088+00:00
nvidia/physicsnemo,1377,Moving HealPix Ops into module folder,https://github.com/NVIDIA/physicsnemo/pull/1377,"**Invalid-shape checks don’t raise**

In `HEALPixPadding.forward`, the invalid-shape branch does `raise ValueError(""HEALPixPadding.forward"", data, ""a 4D tensor"")`, which constructs a `ValueError` with tuple args (and includes a tensor) rather than raising a clear message like the previous `_raise_shape_error` helper did. More importantly, similar checks in `HEALPixFoldFaces.forward` and `HEALPixUnfoldFaces.forward` *don’t raise at all* (they call `ValueError(...)` without `raise`), so invalid inputs will fall through and then crash later during `tensor.shape` unpacking / reshape with less actionable errors.

Also appears in:
- `physicsnemo/nn/module/hpx/padding.py:388-390` (`HEALPixFoldFaces.forward`)
- `physicsnemo/nn/module/hpx/padding.py:450-456` (`HEALPixUnfoldFaces.forward`)",https://github.com/NVIDIA/physicsnemo/pull/1377#discussion_r2771034619,,2026-02-05T20:34:48.068000,Missing raise statements cause validation errors to silently pass,logic,high,8,"The comment catches a significant bug where ValueError is constructed but never raised (missing 'raise' keyword), causing invalid inputs to silently pass through validation and crash later with confusing errors. This is a real logic bug that would affect production code and is not immediately obvious from casual code review.",2026-02-08T22:14:29.362525+00:00
onyx-dot-app/onyx,8246,feat: postgres file store,https://github.com/onyx-dot-app/onyx/pull/8246,"**Backend selection never triggers**
`FILE_STORE_BACKEND` is read from the environment as a `str` (see `app_configs.py`), but `get_default_file_store()` compares it to `FileStoreType.POSTGRES` (an Enum instance). This comparison will always be false, so the Postgres filestore is never selected and the code will always fall back to S3.

Fix by comparing to the Enum value (e.g. `FILE_STORE_BACKEND == FileStoreType.POSTGRES.value`) or by parsing/coercing `FILE_STORE_BACKEND` into `FileStoreType` before comparing.",https://github.com/onyx-dot-app/onyx/pull/8246#discussion_r2776454111,,2026-02-06T23:44:57.471000,Type comparison bug prevents Postgres backend from being selected,type_error,high,8,"This catches a real bug where comparing a string environment variable to an Enum instance will always fail, meaning the Postgres file store feature being added in this PR would never actually work - users would always get S3 regardless of configuration. This is a meaningful logic bug that would break the core functionality of the PR.",2026-02-09T04:52:59.817493+00:00
twentyhq/twenty,17783,Harden local file storage driver path resolution,https://github.com/twentyhq/twenty/pull/17783,"Edge case: `assertPathIsWithinStorage` allows accessing the storage root itself (`resolvedPath === storageRoot`), which would let callers delete or check existence of the entire storage directory. Verify this is intended behavior.

```suggestion
  private assertPathIsWithinStorage(resolvedPath: string): void {
    const storageRoot = path.resolve(this.options.storagePath);

    if (!resolvedPath.startsWith(storageRoot + path.sep)) {
      throw new FileStorageException(
        'Access denied',
        FileStorageExceptionCode.ACCESS_DENIED,
      );
    }
  }
```",https://github.com/twentyhq/twenty/pull/17783#discussion_r2779511247,,2026-02-08T16:54:27.036000,Path traversal allows deletion of entire storage root directory,security,high,8,"This is a meaningful security catch in a file storage driver - the original check `resolvedPath.startsWith(storageRoot)` allows accessing the storage root itself, potentially enabling deletion of the entire storage directory. The fix requiring `storageRoot + path.sep` properly restricts access to only files within the storage, not the root itself. This is a subtle but important security hardening in path validation code.",2026-02-09T10:34:31.065819+00:00
posthog/posthog,47224,feat(ai): implement MCP tool discovery and execution endpoints,https://github.com/PostHog/posthog/pull/47224,"**Tool scopes no longer enforced**

The previous version dynamically enforced per-tool scopes via `dangerously_get_required_scopes()` using `mcp_tool_registry.get_scopes(tool_name)`. That override is now removed, so hitting `run()` no longer checks the tool’s declared scopes, which allows invoking tools without the permissions they require (authz regression for any caller who can access this viewset).

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/posthog_ai/backend/api/mcp_tools.py
Line: 20:39

Comment:
**Tool scopes no longer enforced**

The previous version dynamically enforced per-tool scopes via `dangerously_get_required_scopes()` using `mcp_tool_registry.get_scopes(tool_name)`. That override is now removed, so hitting `run()` no longer checks the tool’s declared scopes, which allows invoking tools without the permissions they require (authz regression for any caller who can access this viewset).

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/47224#discussion_r2778023577,,2026-02-07T20:15:45.962000,Authorization bypass allows invoking tools without required permissions,security,critical,9,"Comment 1 identifies a security regression where the refactored code removes the per-tool scope enforcement, allowing any caller with viewset access to invoke tools without the permissions those tools require. This is an authorization bypass vulnerability that could allow unauthorized access to sensitive operations.",2026-02-09T10:34:31.065819+00:00
posthog/posthog,47242,feat: add first-class support for $screen events in action definitions,https://github.com/PostHog/posthog/pull/47242,"Screen name stored in `properties` array but backend expects it in `step.url` field. The backend code in `posthog/hogql/property.py:995` checks `if step.url:` and uses that value, but this frontend code stores the screen name as a property filter instead. This will cause screen name matching to fail completely.

Backend expects:
```json
{""event"": ""$screen"", ""url"": ""HomeScreen"", ""url_matching"": ""contains""}
```

Frontend is saving:
```json
{""event"": ""$screen"", ""url"": null, ""properties"": [{""key"": ""$screen_name"", ""value"": ""HomeScreen"", ...}]}
```

The screen name should be saved to `step.url` field similar to how `$pageview` works.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/actions/frontend/components/ActionStep.tsx
Line: 462:475

Comment:
Screen name stored in `properties` array but backend expects it in `step.url` field. The backend code in `posthog/hogql/property.py:995` checks `if step.url:` and uses that value, but this frontend code stores the screen name as a property filter instead. This will cause screen name matching to fail completely.

Backend expects:
```json
{""event"": ""$screen"", ""url"": ""HomeScreen"", ""url_matching"": ""contains""}
```

Frontend is saving:
```json
{""event"": ""$screen"", ""url"": null, ""properties"": [{""key"": ""$screen_name"", ""value"": ""HomeScreen"", ...}]}
```

The screen name should be saved to `step.url` field similar to how `$pageview` works.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/47242#discussion_r2779203902,,2026-02-08T12:18:05.537000,Screen name stored in wrong field breaks backend matching,data_integrity,high,8,"Comment 2 identifies a critical data structure mismatch where the frontend stores screen names in the `properties` array while the backend expects them in `step.url`. This would cause screen name matching to fail completely in production, breaking the entire feature for $screen events.",2026-02-09T10:34:31.065819+00:00
posthog/posthog-js,3028,fix(core): queue pending feature flags reload instead of dropping requests,https://github.com/PostHog/posthog-js/pull/3028,"pending reload is not awaited - callers of queued requests receive results from the first request instead of their own

when a caller invokes `flagsAsync()` during an in-flight request, they receive `this._flagsResponsePromise` (line 516), which resolves with the first request's result. their actual reload happens here but isn't connected to any promise they're waiting on.

consider storing the pending request's promise and chaining it, or document that queued callers get stale results

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/core/src/posthog-core.ts
Line: 700:700

Comment:
pending reload is not awaited - callers of queued requests receive results from the first request instead of their own

when a caller invokes `flagsAsync()` during an in-flight request, they receive `this._flagsResponsePromise` (line 516), which resolves with the first request's result. their actual reload happens here but isn't connected to any promise they're waiting on.

consider storing the pending request's promise and chaining it, or document that queued callers get stale results

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog-js/pull/3028#discussion_r2763698459,,2026-02-04T12:10:38.204000,Race condition causes callers to receive stale feature flag results,concurrency,high,8,"This is a meaningful concurrency bug where callers waiting for feature flags during an in-flight request receive results from the first request instead of their own queued reload. This would cause incorrect/stale feature flag values to be returned to users, affecting application behavior in production. The analysis correctly identifies the promise chaining issue and its impact.",2026-02-09T14:34:15.874156+00:00
posthog/posthog,47282,fix: breadcrumbs on survey notifications & notification caching survey ID,https://github.com/PostHog/posthog/pull/47282,"**Stale config cache**

`urlToAction` only updates `cache.configFromUrl`/calls `resetForm()` when `values.template` is already loaded. If you navigate between `hogFunctionNew` routes (or change the `#configuration` hash) before the template loader finishes, this handler is skipped and the subsequent `loadTemplateSuccess -> resetForm` will apply whatever `cache.configFromUrl` was last set to (from the previous route). That can reintroduce the “new notification uses previous survey ID” behavior in fast navigation/back-forward sequences.

This logic should update `cache.configFromUrl` based on the URL unconditionally (or at least regardless of `values.template`), so the next `resetForm()` uses the correct hash params for the current route.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: frontend/src/scenes/hog-functions/configuration/hogFunctionConfigurationLogic.tsx
Line: 1481:1488

Comment:
**Stale config cache**

`urlToAction` only updates `cache.configFromUrl`/calls `resetForm()` when `values.template` is already loaded. If you navigate between `hogFunctionNew` routes (or change the `#configuration` hash) before the template loader finishes, this handler is skipped and the subsequent `loadTemplateSuccess -> resetForm` will apply whatever `cache.configFromUrl` was last set to (from the previous route). That can reintroduce the “new notification uses previous survey ID” behavior in fast navigation/back-forward sequences.

This logic should update `cache.configFromUrl` based on the URL unconditionally (or at least regardless of `values.template`), so the next `resetForm()` uses the correct hash params for the current route.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/47282#discussion_r2781761667,,2026-02-09T10:12:43.968000,Race condition in URL-to-action causes stale survey ID caching,concurrency,high,8,"This catches a subtle race condition where fast navigation between routes causes stale cached data to be applied, leading to the wrong survey ID being used. This is a meaningful bug that would cause incorrect behavior in production during normal user navigation patterns, and it requires understanding the async loading flow to identify.",2026-02-09T16:32:27.608862+00:00
onyx-dot-app/onyx,8194,fix(craft): snapshot restore,https://github.com/onyx-dot-app/onyx/pull/8194,"**Signature mismatch breaks interface**
`SandboxManager.health_check` is declared as `health_check(self, sandbox_id, timeout: float = 60.0)` in `backend/onyx/server/features/build/sandbox/base.py:261`, and call sites pass `timeout=` (e.g., `backend/onyx/server/features/build/api/sessions_api.py:384`, `backend/onyx/server/features/build/session/manager.py:491`). This PR changes `LocalSandboxManager.health_check` to drop the `timeout` parameter (`backend/onyx/server/features/build/sandbox/local/local_sandbox_manager.py:659`), which will raise `TypeError: got an unexpected keyword argument 'timeout'` when the local backend is used. Keep the method signature compatible (accept `timeout` even if unused).",https://github.com/onyx-dot-app/onyx/pull/8194#discussion_r2777021486,,2026-02-07T04:50:18.677000,Method signature mismatch causes TypeError at runtime,type_error,high,8,"The comment identifies a concrete interface violation where LocalSandboxManager.health_check drops the timeout parameter that the base class and all call sites expect, which will cause a TypeError when the local backend is used. This is a real runtime bug that would break functionality.",2026-02-09T18:30:46.442785+00:00
useautumn/autumn,702,feat: add discounts to attach v2,https://github.com/useautumn/autumn/pull/702,"**Wrong Stripe env used**
`createStripeCli` is instantiated with `env: fullCustomer.env` here, but this code is inside billing flows that otherwise operate on the request context (`ctx.org`/`ctx.env`). If `fullCustomer.env` can differ from `ctx.env` (e.g. cross-env reads or overridden customer env), discount resolution will hit the wrong Stripe account and either fail to resolve valid IDs or apply unintended coupons.

Use `env: ctx.env` (or otherwise ensure the Stripe env source is consistent across the billing context setup).

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: server/src/internal/billing/v2/providers/stripe/setup/fetchStripeDiscountsForBilling.ts
Line: 75:77

Comment:
**Wrong Stripe env used**
`createStripeCli` is instantiated with `env: fullCustomer.env` here, but this code is inside billing flows that otherwise operate on the request context (`ctx.org`/`ctx.env`). If `fullCustomer.env` can differ from `ctx.env` (e.g. cross-env reads or overridden customer env), discount resolution will hit the wrong Stripe account and either fail to resolve valid IDs or apply unintended coupons.

Use `env: ctx.env` (or otherwise ensure the Stripe env source is consistent across the billing context setup).

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/useautumn/autumn/pull/702#discussion_r2780260424,,2026-02-09T01:57:13.004000,Wrong Stripe environment could apply discounts to wrong account,logic,high,8,"This is a meaningful logic bug where using the customer's environment instead of the request context's environment could cause billing operations to hit the wrong Stripe account. This could result in failed discount resolutions or applying unintended coupons to customers, which has real financial and user impact in production. The catch requires understanding the billing flow context and how different environment sources could diverge.",2026-02-09T20:23:03.262628+00:00
posthog/posthog,47301,fix(cdp): request payload limitation for cdp-api,https://github.com/PostHog/posthog/pull/47301,"**Body limit bypass via missing length**

`publicBodySizeLimit` relies solely on the `content-length` header (`parseInt(req.headers['content-length'] || '0', 10)`), so requests that omit or spoof `Content-Length` will bypass the 500KB check and still be parsed by the global `express.json({ limit: '20mb' })`. This breaks the intended protection for the public POST routes. Consider enforcing the limit using the body parser’s own limit mechanism on those routes (e.g. apply a smaller `express.json({ limit: '500kb' })` / `express.raw` limit per-route), or reject when `content-length` is missing/invalid instead of treating it as 0.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nodejs/src/cdp/cdp-api.ts
Line: 162:169

Comment:
**Body limit bypass via missing length**

`publicBodySizeLimit` relies solely on the `content-length` header (`parseInt(req.headers['content-length'] || '0', 10)`), so requests that omit or spoof `Content-Length` will bypass the 500KB check and still be parsed by the global `express.json({ limit: '20mb' })`. This breaks the intended protection for the public POST routes. Consider enforcing the limit using the body parser’s own limit mechanism on those routes (e.g. apply a smaller `express.json({ limit: '500kb' })` / `express.raw` limit per-route), or reject when `content-length` is missing/invalid instead of treating it as 0.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/47301#discussion_r2782386926,,2026-02-09T12:32:19.319000,Body size limit bypass via missing Content-Length header,security,high,8,"Comments 1 and 2 identify the same security bypass issue - the body size limit check can be bypassed when Content-Length header is missing or spoofed. Comment 2 is more comprehensive, explaining both the bypass mechanism and providing concrete remediation suggestions. This is a meaningful security vulnerability affecting public-facing endpoints.",2026-02-09T20:23:03.262628+00:00
onyx-dot-app/onyx,8281,fix(posthog): Chat metrics for Cloud (#8278),https://github.com/onyx-dot-app/onyx/pull/8281,"**Anonymous user crash**

`distinct_id=user.email if not user.is_anonymous else tenant_id` will raise when `user is None` (e.g., no-auth/anonymous flows) because it unconditionally accesses `user.is_anonymous`. This is a runtime error on message send; keep the `user is not None` guard or use a safe attribute check before referencing `is_anonymous`.",https://github.com/onyx-dot-app/onyx/pull/8281#discussion_r2785020386,,2026-02-09T23:22:55.012000,NoneType attribute access crash in anonymous user flow,null,high,8,"This is a genuine runtime bug - accessing `user.is_anonymous` when `user is None` would cause an AttributeError crash during message sending in anonymous/no-auth flows. This is a real production issue that would affect users, not something immediately caught by static analysis since it depends on runtime authentication state.",2026-02-10T00:56:10.651343+00:00
onyx-dot-app/onyx,8280,fix(posthog): Chat metrics for Cloud (#8278),https://github.com/onyx-dot-app/onyx/pull/8280,"**Crash when user is None**
`distinct_id=user.email if not user.is_anonymous else tenant_id` will raise when `user` is `None` (this function is called with `user: User | None`). This will break chat flows that don’t have an authenticated user (e.g. no-auth/anonymous endpoints). Consider guarding with `if user is not None` (and only referencing `user.is_anonymous` when `user` exists).",https://github.com/onyx-dot-app/onyx/pull/8280#discussion_r2785019878,,2026-02-09T23:22:45.271000,NoneType attribute access crashes chat when user is None,null,high,8,"This is a real runtime bug - accessing `.email` or `.is_anonymous` on a None user object will crash chat flows for unauthenticated users. The function signature explicitly allows `User | None`, so this is a valid code path that would cause production crashes for anonymous/no-auth endpoints.",2026-02-10T00:56:10.651343+00:00
isaac-sim/isaaclab,3388,Replaces torch.norm with torch.linalg.norm for consistency,https://github.com/isaac-sim/IsaacLab/pull/3388,"**Invalid `p=` keyword**
Several reward utilities were switched to `torch.linalg.norm(..., p=2, ...)`, but `torch.linalg.norm` expects `ord=` (and `p=` will raise `TypeError`). This will crash these rewards/terminations when executed.

```suggestion
    # example fix
    goal_dist = torch.linalg.norm(object_pos - target_pos, ord=2, dim=-1)
```

Also appears in:
- `source/isaaclab_tasks/isaaclab_tasks/direct/anymal_c/anymal_c_env.py:314`
- `source/isaaclab_tasks/isaaclab_tasks/direct/anymal_c/anymal_c_env.py:723`
- `source/isaaclab_tasks/isaaclab_tasks/direct/automate/assembly_env.py:163`
- `source/isaaclab_tasks/isaaclab_tasks/direct/automate/assembly_env.py:173`
- `source/isaaclab_tasks/isaaclab_tasks/direct/automate/disassembly_env.py:164`
- `source/isaaclab_tasks/isaaclab_tasks/direct/automate/disassembly_env.py:174`
- `source/isaaclab_tasks/isaaclab_tasks/direct/factory/factory_env.py:436`
- `source/isaaclab_tasks/isaaclab_tasks/direct/forge/forge_env.py:263`
- `source/isaaclab_tasks/isaaclab_tasks/direct/franka_cabinet/franka_cabinet_env.py:346`
- `source/isaaclab_tasks/isaaclab_tasks/direct/inhand_manipulation/inhand_manipulation_env.py:358`
- `source/isaaclab_tasks/isaaclab_tasks/direct/locomotion/locomotion_env.py:398`
- `source/isaaclab_tasks/isaaclab_tasks/direct/shadow_hand_over/shadow_hand_over_env.py:557`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/deploy/mdp/rewards.py:624`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/deploy/mdp/rewards.py:711`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/deploy/mdp/rewards.py:895`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/deploy/mdp/rewards.py:907`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/deploy/mdp/rewards.py:1122`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/deploy/mdp/rewards.py:1135`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/deploy/mdp/rewards.py:1144`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/inhand/mdp/rewards.py:69`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/inhand/mdp/terminations.py:57`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/reach/mdp/rewards.py:110`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/stack/mdp/terminations.py:105`
- `source/isaaclab_tasks/isaaclab_tasks/manager_based/manipulation/stack/mdp/terminations.py:212`",https://github.com/isaac-sim/IsaacLab/pull/3388#discussion_r2777174620,,2026-02-07T06:49:57.726000,Invalid p= keyword argument causes TypeError in torch.linalg.norm,runtime,high,8,Comment 0 identifies a runtime crash where torch.linalg.norm requires 'ord=' but the code uses 'p=' from torch.norm API. This would cause TypeError crashes across 24+ locations in production code. The comment comprehensively lists all affected files.,2026-02-10T00:56:10.651343+00:00
eventual-inc/daft,5839,feat: add unity oauth m2m access token support,https://github.com/Eventual-Inc/Daft/pull/5839,"**logic:** inverted logic - returns True when token is NOT expired

`is_expired()` returns True when the token IS expired, so `not is_expired()` returns True when token is valid. this means `_token_expired()` returns True when token is still valid, causing unnecessary token refreshes on every call

```suggestion
    def _token_expired(self) -> bool:
        if self._token is None or self._expiration == 0:
            return True
        return is_expired(self._expiration)
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: daft/unity_catalog/auth.py
Line: 82:85

Comment:
**logic:** inverted logic - returns True when token is NOT expired

`is_expired()` returns True when the token IS expired, so `not is_expired()` returns True when token is valid. this means `_token_expired()` returns True when token is still valid, causing unnecessary token refreshes on every call

```suggestion
    def _token_expired(self) -> bool:
        if self._token is None or self._expiration == 0:
            return True
        return is_expired(self._expiration)
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Eventual-Inc/Daft/pull/5839#discussion_r2663215952,Will fix,2026-01-06T00:55:34.503000,Inverted token expiration logic causes wrong refresh behavior,logic,high,8,"Comment 2 identifies a significant logic bug where the `_token_expired()` method has inverted logic - it returns True when the token is NOT expired. This would cause either unnecessary token refreshes on every call or failure to refresh when actually expired, leading to authentication failures in production.",2026-02-10T00:56:10.651343+00:00
infisical/infisical,5410,fix: gateway v2 support for hashicorp vault connection,https://github.com/Infisical/infisical/pull/5410,"**TLS verification disabled**

In the gateway v2 branch, HTTPS requests set `httpsAgent` with `rejectUnauthorized: false`, which disables certificate verification for the proxied Vault request. This makes the connection susceptible to MITM (any actor on the path to the gateway/relay can present any cert and still be trusted). If Vault is expected to use a custom CA, prefer passing an explicit CA bundle (or reusing the existing gateway TLS material) rather than disabling verification.",https://github.com/Infisical/infisical/pull/5410#discussion_r2784853732,@greptile This was for testing purposes and was planned for removal before handing off the review to a human,2026-02-09T22:30:42.624000,TLS verification disabled enables MITM attacks on Vault connections,security,critical,9,This is a significant security vulnerability - disabling TLS certificate verification (rejectUnauthorized: false) on HTTPS connections to HashiCorp Vault exposes sensitive secrets management traffic to man-in-the-middle attacks. The reviewer correctly identifies the security risk and provides a concrete remediation (use explicit CA bundle instead). This is a non-obvious security issue that could have serious production impact.,2026-02-10T00:56:10.651343+00:00
infisical/infisical,5409,feat(sso): support wildcard email domains for OIDC,https://github.com/Infisical/infisical/pull/5409,"**Overbroad wildcard matching**

The wildcard branch currently treats `*.suffix` as matching `suffix.*` too via `emailDomain.startsWith(`${suffix}.`)` (and the `includes` check is redundant if you keep an `endsWith`-style label boundary). This makes patterns like `*.example.com` incorrectly allow `user@example.com.evil` (and generally any domain with that suffix as a prefix), which defeats the allow-list check in OIDC login (`oidc-config-service.ts:741`). This should only match the exact suffix (`example.com`) or domains that end with `.${suffix}` (e.g. `team.example.com`).",https://github.com/Infisical/infisical/pull/5409#discussion_r2784796769,,2026-02-09T22:16:42.803000,Domain wildcard bypass allows unauthorized email domains in OIDC,security,critical,9,"This is a significant security vulnerability in authentication logic. The wildcard matching bug would allow attackers to bypass email domain allow-lists by using domains like 'example.com.evil', potentially gaining unauthorized access to OIDC-protected resources. This is a subtle but serious auth bypass that wouldn't be caught by static analysis tools.",2026-02-10T03:16:23.018062+00:00
useautumn/autumn,713,chore: cleaned up current api plan files to prepare for new schema,https://github.com/useautumn/autumn/pull/713,"**Missing ctx.features in mapper**
`apiPlanItem.map.v0ToProductItem` reads the feature from `ctx.features` and throws `FeatureNotFoundError` if it can’t find it. In this codepath `ctx.features` may be empty/not populated (previously this conversion was independent because it took `features: [feature]` directly), so creating a balance for a feature can start failing even when `feature` is known. Either pass an explicit feature list into the mapper (or add an overload for this callsite), or ensure `ctx.features` is populated before calling it.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: server/src/internal/balances/createBalance/prepareNewBalanceForInsertion.ts
Line: 21:30

Comment:
**Missing ctx.features in mapper**
`apiPlanItem.map.v0ToProductItem` reads the feature from `ctx.features` and throws `FeatureNotFoundError` if it can’t find it. In this codepath `ctx.features` may be empty/not populated (previously this conversion was independent because it took `features: [feature]` directly), so creating a balance for a feature can start failing even when `feature` is known. Either pass an explicit feature list into the mapper (or add an overload for this callsite), or ensure `ctx.features` is populated before calling it.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/useautumn/autumn/pull/713#discussion_r2780255990,,2026-02-09T01:54:16.568000,Missing context data causes FeatureNotFoundError during balance creation,runtime,high,8,"This catches a real runtime bug where a refactored code path no longer properly populates ctx.features before calling a mapper that depends on it, causing production failures when creating balances. The bug would cause operations to fail even when the feature data is available, requiring understanding of the data flow across the refactoring change.",2026-02-10T03:16:23.018062+00:00
posthog/posthog,47401,feat(deduplicator): expose detailed task status for rebalance reassign flow poller,https://github.com/PostHog/posthog/pull/47401,"**Revoked task re-attach broken**

`revoked_setup_tasks.remove(partition)` drops the revoked task unconditionally, but the code only re-attaches when `peeked` is `Some(Ok(Completed(_)))`. For the intended “still running or completed” behavior described in the comment, `peeked == None` means *still pending*, so this currently discards in-flight revoked imports and forces a fresh spawn (and loses the ability to poll status). This contradicts the PR goal and can increase replacement churn.

This looks like it should either re-attach when `peeked.is_none()` (still running) or avoid removing from `revoked_setup_tasks` unless you’re going to re-attach. Also note the comment says “still running or Completed(path)” but the match only handles `Completed`.


<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: rust/kafka-deduplicator/src/processor_rebalance_handler.rs
Line: 740:752

Comment:
**Revoked task re-attach broken**

`revoked_setup_tasks.remove(partition)` drops the revoked task unconditionally, but the code only re-attaches when `peeked` is `Some(Ok(Completed(_)))`. For the intended “still running or completed” behavior described in the comment, `peeked == None` means *still pending*, so this currently discards in-flight revoked imports and forces a fresh spawn (and loses the ability to poll status). This contradicts the PR goal and can increase replacement churn.

This looks like it should either re-attach when `peeked.is_none()` (still running) or avoid removing from `revoked_setup_tasks` unless you’re going to re-attach. Also note the comment says “still running or Completed(path)” but the match only handles `Completed`.


How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/47401#discussion_r2785081501,I think the comment here is confusing the bots,2026-02-09T23:45:46.513000,Revoked task re-attach logic drops in-flight imports incorrectly,logic,high,8,"This is a meaningful logic bug where the code unconditionally removes revoked tasks but only re-attaches under one specific condition (Completed), causing in-flight imports to be discarded rather than re-attached. This directly contradicts the intended behavior described in comments and could cause increased churn and lost polling ability in production. The analysis demonstrates deep understanding of the control flow and identifies the mismatch between code and intent.",2026-02-10T04:56:42.462332+00:00
beanie-brick-band/leopard,206,feat(backend): teacher API,https://github.com/Beanie-Brick-Band/leopard/pull/206,"**Flag deletion authorization**

`deleteFlag` deletes whatever `flags` document `args.flagId` points to without verifying it belongs to `args.submissionId` (or even that it’s referenced by this submission). Any instructor with access to the classroom could delete arbitrary flags (including those on other submissions) by passing their IDs. You likely want to ensure `submission.flags.includes(args.flagId)` before deleting and patching.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/backend/convex/web/teacherAssignments.ts
Line: 327:330

Comment:
**Flag deletion authorization**

`deleteFlag` deletes whatever `flags` document `args.flagId` points to without verifying it belongs to `args.submissionId` (or even that it’s referenced by this submission). Any instructor with access to the classroom could delete arbitrary flags (including those on other submissions) by passing their IDs. You likely want to ensure `submission.flags.includes(args.flagId)` before deleting and patching.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Beanie-Brick-Band/leopard/pull/206#discussion_r2771523198,,2026-02-05T23:03:52.675000,Missing authorization check allows deletion of arbitrary flags,security,high,9,"This is a significant authorization bypass vulnerability where any instructor with classroom access could delete flags on any submission by passing arbitrary flag IDs, not just flags on submissions they should have access to. This is an Insecure Direct Object Reference (IDOR) vulnerability that could allow unauthorized data manipulation across the system.",2026-02-10T04:56:42.462332+00:00
simstudioai/sim,3168,fix(mcp): harden notification system against race conditions,https://github.com/simstudioai/sim/pull/3168,"**Constructor overload mis-detect**

The overload discriminator looks inverted: `if ('config' in configOrOptions)` will be true for the *options* object, but the body treats that branch as if it were the legacy `(config, securityPolicy?)` case (it assigns `this.config = configOrOptions.config`). In the `else` branch it assigns `this.config = configOrOptions`, which will be an options object in the legacy call site and will break (e.g., `this.config.url` becomes undefined and the URL check throws). This needs to be flipped (treat `'config' in ...` as the options path, else as the legacy config path) so existing call sites keep working.",https://github.com/simstudioai/sim/pull/3168#discussion_r2780025636,"False positive — `McpServerConfig` doesn't have a config property, only `McpClientOptions` does, so the discriminator is correct.",2026-02-08T23:14:22.065000,Constructor overload discriminator logic is inverted causing runtime failures,logic,high,8,"This catches a real logic bug where the constructor's type discrimination is inverted, causing the wrong branch to execute for both call patterns. This would cause undefined property access and broken functionality in production for all existing callers. The analysis is thorough and explains exactly why both branches fail.",2026-02-10T04:56:42.462332+00:00
gtbitsofgood/juno-dashboard,61,Add ability to delete + add a file config,https://github.com/GTBitsOfGood/juno-dashboard/pull/61,"**logic:** using `row.id` (the file config ID) instead of the component's `projectId` prop will cause the delete operation to fail

```suggestion
      const deletePromises = selectedRows.map(async (row) => {
        return deleteFileConfig(projectId);
```",https://github.com/GTBitsOfGood/juno-dashboard/pull/61#discussion_r2531807284,,2025-11-16T10:27:34.537000,Wrong ID passed to delete operation causes incorrect deletion,logic,high,8,This is a meaningful logic bug where using `row.id` (the file config ID) instead of the correct `projectId` prop would cause delete operations to fail or potentially delete wrong resources. This would directly impact user functionality and could cause data integrity issues in production.,2026-02-10T04:56:42.462332+00:00
raycast/extensions,25290,Update audio-device extension,https://github.com/raycast/extensions/pull/25290,"**PowerShell injection risk**

`deviceId` is interpolated into a single-quoted PowerShell string (`'${deviceId}'`). `[System.Security.SecurityElement]::Escape()` only XML-escapes the resulting string; it does **not** make the PowerShell literal safe. If `deviceId` contains a single quote, it will break out of the string and can inject arbitrary PowerShell (e.g. `abc' ; <payload> ; '`). This is reachable if a device ID contains `'`.

Consider escaping for PowerShell single-quoted literals by doubling quotes before interpolation (e.g. replace `'` with `''`) or otherwise passing the value without string concatenation.
",https://github.com/raycast/extensions/pull/25290#discussion_r2785489118,,2026-02-10T02:38:11.161000,PowerShell command injection via unsanitized device ID,security,critical,9,"This is a legitimate command injection vulnerability where a malicious device ID containing single quotes could break out of the PowerShell string and execute arbitrary commands. The comment correctly identifies that XML escaping doesn't protect against PowerShell injection, and provides the proper fix (doubling single quotes). This is a non-obvious security flaw that could lead to arbitrary code execution.",2026-02-10T08:28:05.218233+00:00
bluedotimpact/bluedot,2043,Batch Slack alerts,https://github.com/bluedotimpact/bluedot/pull/2043,"**Global batcher state leaks**

`batchers` is a module-level `Map` that accumulates keys forever; `scheduleFlush` clears `batcher.batches` but never removes the `batchKey` entry from `batchers`. Any one-off/unique `batchKey` usage will permanently grow this map in a long-lived process (e.g. website server), which is an unbounded memory leak. Consider deleting the batcher from the map once it flushes (or when `batches` is cleared).",https://github.com/bluedotimpact/bluedot/pull/2043#discussion_r2786879905,,2026-02-10T09:40:51.207000,Unbounded memory leak in global batcher state Map,resource_leak,high,8,This catches a genuine memory leak where the module-level `batchers` Map grows indefinitely in long-lived processes because batchKey entries are never removed after flushing. This would cause production memory issues in server environments over time.,2026-02-10T12:33:31.742549+00:00
posthog/posthog,46105,fix(data-warehouse): implement adaptive chunking for API timeout errors,https://github.com/PostHog/posthog/pull/46105,"this logic causes duplicate data when timeout occurs mid-pagination. if the API successfully yields pages 1-3 then times out on page 4, the recursive call to `_fetch_with_chunk_size` will re-fetch and re-yield all data from `chunk_start` to `chunk_end` with a smaller chunk size, duplicating the already-yielded data from pages 1-3.

to fix this, track which portion of the chunk was successfully fetched before the timeout, and only retry the remaining time range. alternatively, detect timeouts before yielding any data for a chunk (i.e., on the first request only)

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/data_imports/sources/meta_ads/meta_ads.py
Line: 137:145

Comment:
this logic causes duplicate data when timeout occurs mid-pagination. if the API successfully yields pages 1-3 then times out on page 4, the recursive call to `_fetch_with_chunk_size` will re-fetch and re-yield all data from `chunk_start` to `chunk_end` with a smaller chunk size, duplicating the already-yielded data from pages 1-3.

to fix this, track which portion of the chunk was successfully fetched before the timeout, and only retry the remaining time range. alternatively, detect timeouts before yielding any data for a chunk (i.e., on the first request only)

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46105#discussion_r2734348184,this sound like a legit concern,2026-01-28T00:24:02.990000,Duplicate data yielded on timeout retry during pagination,data_integrity,high,8,"This is a meaningful data integrity bug where timeout-triggered recursive retries would re-fetch and re-yield already-processed pages, causing duplicate records in the data warehouse. This is a subtle logic error in error recovery that wouldn't be caught by static analysis and would cause real data quality issues in production.",2026-02-10T14:41:13.725611+00:00
posthog/posthog,47311,feat: support multiple variables in materialized endpoints,https://github.com/PostHog/posthog/pull/47311,"**Duplicate placeholders break materialization**

`analyze_variables_for_materialization` iterates over `finder.variable_placeholders` and appends one `MaterializableVariable` per placeholder. If the same `{variables.foo}` appears multiple times in the query (e.g. repeated in multiple AND clauses), `result_vars` will contain duplicates with the same `code_name`. That leads `transform_query_for_materialization` to add duplicate aliased SELECT fields for the same alias, which will produce invalid HogQL/SQL (`... , <expr> AS foo, <expr> AS foo ...`). Consider deduplicating placeholders by `code_name` (or by placeholder chain) before building `result_vars`.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/endpoints/backend/materialization.py
Line: 91:93

Comment:
**Duplicate placeholders break materialization**

`analyze_variables_for_materialization` iterates over `finder.variable_placeholders` and appends one `MaterializableVariable` per placeholder. If the same `{variables.foo}` appears multiple times in the query (e.g. repeated in multiple AND clauses), `result_vars` will contain duplicates with the same `code_name`. That leads `transform_query_for_materialization` to add duplicate aliased SELECT fields for the same alias, which will produce invalid HogQL/SQL (`... , <expr> AS foo, <expr> AS foo ...`). Consider deduplicating placeholders by `code_name` (or by placeholder chain) before building `result_vars`.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/47311#discussion_r2783535163,"worth checking
---
oh yeah - didn't `gt submit` in time",2026-02-10T12:47:03.631000,Duplicate variable placeholders produce invalid SQL queries,logic,high,8,"This is a meaningful logic bug where duplicate variable placeholders in a query would generate invalid SQL with duplicate aliased columns, causing query failures in production. The catch requires understanding the data flow through multiple functions and isn't immediately obvious from static analysis or IDE tooling.",2026-02-10T14:41:13.725611+00:00
eventual-inc/daft,6145,refactor(arrow2): migrate the extra easy kernels to arrow-rs,https://github.com/Eventual-Inc/Daft/pull/6145,"**Binary field/type mismatch**
`Utf8Array::encode` now constructs a `BinaryArray` but passes `self.field().clone()` (an Utf8 field) into `BinaryArray::from_arrow(...)`. This breaks the invariant that the field dtype matches the physical array type, and downstream code that relies on `BinaryArray` having `DataType::Binary` can misbehave.

```suggestion
        let binary_field = self.field().as_ref().clone().with_dtype(crate::datatypes::DataType::Binary);
```
(Then pass `binary_field` to `BinaryArray::from_arrow`.)",https://github.com/Eventual-Inc/Daft/pull/6145#discussion_r2784336511,,2026-02-09T20:20:09.971000,Field/type mismatch breaks BinaryArray invariant causing downstream errors,type_error,high,8,"This catches a real data integrity bug where a Utf8 field is incorrectly passed when constructing a BinaryArray, breaking a fundamental invariant that the field dtype must match the array type. This would cause downstream code relying on BinaryArray having DataType::Binary to misbehave in production, which is a meaningful logic bug that wouldn't be caught by typical compile-time checks.",2026-02-10T16:36:19.330268+00:00
onyx-dot-app/onyx,8289,fix(llm): Fixing seeding logic for LLM Model Flow,https://github.com/onyx-dot-app/onyx/pull/8289,"**May violate unique default index**

This migration clears `is_default` for all `CHAT`/`VISION` flows and then re-sets defaults based on legacy provider flags. If the existing data has multiple providers/models matching the default criteria for a flow type, the final `UPDATE` will set multiple rows to `is_default=TRUE`, which violates `ix_one_default_per_llm_model_flow` (unique per flow type where `is_default`). That will cause the migration to fail for such DBs; it needs a deterministic single-default selection (e.g., `... WHERE mc.id = (SELECT ... LIMIT 1)`-style) or pre-normalization of legacy flags.",https://github.com/onyx-dot-app/onyx/pull/8289#discussion_r2785420799,,2026-02-10T02:10:48.709000,Migration can violate unique constraint with multiple default providers,data_integrity,high,8,"This catch identifies a real database migration bug where the UPDATE statement could set multiple rows to is_default=TRUE, violating the unique index ix_one_default_per_llm_model_flow. This would cause migration failures in production databases with multiple providers matching the default criteria - a non-obvious issue requiring understanding of both the SQL logic and database constraints.",2026-02-10T18:36:08.497705+00:00
infisical/infisical,5415,fix(permissions): add a check for empty glob paths,https://github.com/Infisical/infisical/pull/5415,"**Empty path grants broad access**

`conditions` currently omits `secretPath` when `secretPath` is falsy (`...(secretPath ? { secretPath: { $glob: secretPath } } : {})`). For a service token scope with an empty secrets path, this makes the rule match *any* `secretPath` in the given `environment`, effectively turning an invalid/empty scope into environment-wide access instead of denying it. This is a security-relevant behavior change; either reject empty `secretPath` scopes up-front or make them non-matching (e.g., force a condition that can’t match) rather than dropping the path filter.",https://github.com/Infisical/infisical/pull/5415#discussion_r2788733316,"Makes sense, we are never using the condition we have added to the $glob if we are not adding the secretPath at all.

I'm reverting the changes in the backend/src/ee/services/permission/project-permission.ts file @akhilmhdh ",2026-02-10T15:43:14.143000,Empty secret path grants unintended environment-wide access,security,high,8,"This comment identifies a security vulnerability where an empty secretPath in service token scopes would omit the path filter entirely, granting access to all secrets in an environment instead of denying access. This is a meaningful auth bypass that could expose sensitive data.",2026-02-10T18:36:08.497705+00:00
nvidia-holoscan/holohub,1342,feat: Add GstSrcOp and GstSrcResource for pushing Holoscan data to GStreamer,https://github.com/nvidia-holoscan/holohub/pull/1342,"**Promise may be set twice**
`monitor_loop()` calls `completion_promise_.set_value()` on internal termination paths (ERROR/EOS/NULL transition) and also unconditionally after the loop exits. If an internal path sets the promise and then later `stop()` sets `stop_flag_` and the loop reaches the bottom (e.g., if future exceptions occur or code is refactored), this will throw `std::future_error` (promise already satisfied).

Guard `set_value()` with a flag (or use `std::promise<void> p; std::atomic<bool> completed;`) so it is only satisfied once.",https://github.com/nvidia-holoscan/holohub/pull/1342#discussion_r2785029939,Fixed,2026-02-09T23:26:29.677000,Promise set twice causes std::future_error exception crash,concurrency,high,8,"This catches a real concurrency bug where completion_promise_.set_value() can be called multiple times (on ERROR/EOS/NULL transition paths and unconditionally after loop exit), which throws std::future_error and crashes the application. This is a meaningful runtime bug that would cause production failures.",2026-02-10T22:25:19.447601+00:00
posthog/posthog,45883,feat: heatmaps upload + toolbar,https://github.com/PostHog/posthog/pull/45883,"Missing ownership validation for `image_url`. The validation only checks that the path starts with `/uploaded_media/`, but doesn't verify that the uploaded media belongs to the same team. This allows users from Team A to reference uploaded images from Team B by guessing or discovering their UUIDs.

You need to extract the media UUID from the path, look up the `UploadedMedia` object, and verify that `uploaded_media.team_id == self.context['request'].team.id`.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/heatmaps/heatmaps_api.py
Line: 427:435

Comment:
Missing ownership validation for `image_url`. The validation only checks that the path starts with `/uploaded_media/`, but doesn't verify that the uploaded media belongs to the same team. This allows users from Team A to reference uploaded images from Team B by guessing or discovering their UUIDs.

You need to extract the media UUID from the path, look up the `UploadedMedia` object, and verify that `uploaded_media.team_id == self.context['request'].team.id`.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45883#discussion_r2726620528,,2026-01-26T07:49:22.731000,Cross-team data access via uploaded media UUID guessing,security,critical,9,"This is a significant security vulnerability where users from one team could reference uploaded images belonging to another team by guessing UUIDs. The validation only checks the URL path format but fails to verify team ownership, allowing unauthorized cross-tenant data access.",2026-02-11T08:27:28.426562+00:00
twentyhq/twenty,17773,Fix row level permission predicate group creation order,https://github.com/twentyhq/twenty/pull/17773,"The groups in `objectPredicateGroups` aren't sorted by parent-child dependencies. If a child appears before its parent in the array, the backend will fail because it tries to reference a non-existent parent.

Sort groups by dependency order before upserting:

```typescript
// Sort groups so parents are created before children
const sortedGroups = [];
const groupsById = new Map(objectPredicateGroups.map(g => [g.id, g]));
const added = new Set<string>();

const addGroupWithParents = (group: typeof objectPredicateGroups[0]) => {
  if (added.has(group.id)) return;
  
  if (group.parentRowLevelPermissionPredicateGroupId) {
    const parent = groupsById.get(group.parentRowLevelPermissionPredicateGroupId);
    if (parent) addGroupWithParents(parent);
  }
  
  sortedGroups.push(group);
  added.add(group.id);
};

objectPredicateGroups.forEach(addGroupWithParents);

predicateGroups: sortedGroups.map((group) => ({
```",https://github.com/twentyhq/twenty/pull/17773#discussion_r2778014789,,2026-02-07T20:08:15.437000,Missing dependency ordering causes foreign key constraint failures,data_integrity,high,8,"Comment 0 identifies a real bug where predicate groups with parent-child relationships could fail to save if a child appears before its parent in the array, causing foreign key constraint violations on the backend. This is a meaningful data integrity issue that would cause production failures. Comment 0 is better written with a correct topological sort implementation including cycle detection.",2026-02-11T10:29:41.291842+00:00
twentyhq/twenty,17849,Respect Gmail retry-after in messaging throttle,https://github.com/twentyhq/twenty/pull/17849,"**Non-atomic failure increment**
`throttleFailureCount` is updated via `messageChannel.throttleFailureCount + 1`, which is derived from the (potentially stale) `messageChannel` object passed into the handler. If two temporary failures for the same channel are handled concurrently, both can write the same incremented value and effectively drop an increment, changing when the channel reaches `MESSAGING_THROTTLE_MAX_ATTEMPTS` and is marked failed. This should be done atomically (e.g., DB-level increment) or by computing from the current stored value.",https://github.com/twentyhq/twenty/pull/17849#discussion_r2792127386,,2026-02-11T09:03:03.521000,Race condition in throttle failure count increment,concurrency,high,8,"This identifies a real race condition where concurrent temporary failures for the same channel read stale throttleFailureCount values and both write the same incremented value, causing lost updates. This could delay proper failure detection and affect production behavior when channels should be marked as failed.",2026-02-11T10:29:41.291842+00:00
mrmps/smry,67,feat: zeroclick integration with rest api been used for offers and mcp for tracking and we are also using signals mcp as mentioned by the support,https://github.com/mrmps/SMRY/pull/67,"**Provider spoof disables forwarding**

`provider` is derived from `adProvider` (unconstrained string) before falling back to the `impUrl` prefix. A client can set `adProvider: ""zeroclick""` even for a real Gravity `impUrl`, which skips forwarding (`provider !== ""zeroclick""` gate) and logs `gravity_forwarded=0`. That breaks billing attribution and corrupts analytics; `adProvider` should not be trusted from the client (derive from `impUrl`/server state, or validate against expected values).",https://github.com/mrmps/SMRY/pull/67#discussion_r2792757960,,2026-02-11T11:27:27.109000,Client-controlled provider field bypasses billing tracking,security,high,8,"This is a security/billing integrity issue where a malicious client can set adProvider to 'zeroclick' for Gravity ads, bypassing server-side impression forwarding and breaking billing attribution. This could result in lost revenue and corrupted analytics data.",2026-02-11T12:31:53.839823+00:00
nvidia/dali,6194,Improved device handling. Remove mixed device. Make DALI work without GPU,https://github.com/NVIDIA/DALI/pull/6194,"**Indexing `frozenset` backends**

`build_operator_class` sets `op_class._supported_backends = frozenset(...)`, but `build_fn_wrapper` later does `backend = supported_backends[0]` when `len(supported_backends) == 1` (`_op_builder.py:564`). `frozenset` is not subscriptable, so this path raises `TypeError` for single-backend operators when `device` is inferred.

This is exercised whenever an op supports only one backend (common for CPU-only ops) and the wrapper infers a mismatching `device` first.",https://github.com/NVIDIA/DALI/pull/6194#discussion_r2783430222,Fixed.,2026-02-09T15:59:08.608000,TypeError from indexing frozenset breaks single-backend operators,type_error,high,8,"This catches a real runtime TypeError that would crash the code path for any single-backend operator when device inference occurs. The code changed `_supported_backends` to `frozenset` but still uses `[0]` indexing which doesn't work on frozensets, causing a TypeError at runtime for common operations.",2026-02-11T12:31:53.839823+00:00
posthog/posthog,47578,fix(feature-flags): allow array values for icontains/not_icontains operators,https://github.com/PostHog/posthog/pull/47578,"**Empty array accepted**

The new `icontains`/`not_icontains` validation allows `value: []` because `all(...)` on an empty list is `True` (`posthog/api/feature_flag.py:771-784`). In Rust, the matcher uses `.any(...)` and negates for `not_icontains` (`rust/feature-flags/src/properties/property_matching.rs:145-163`), so `icontains []` will never match, while `not_icontains []` will always match whenever the property exists. It seems safer to reject empty arrays at API validation to avoid creating “match none / match all” flags by accident.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/api/feature_flag.py
Line: 771:783

Comment:
**Empty array accepted**

The new `icontains`/`not_icontains` validation allows `value: []` because `all(...)` on an empty list is `True` (`posthog/api/feature_flag.py:771-784`). In Rust, the matcher uses `.any(...)` and negates for `not_icontains` (`rust/feature-flags/src/properties/property_matching.rs:145-163`), so `icontains []` will never match, while `not_icontains []` will always match whenever the property exists. It seems safer to reject empty arrays at API validation to avoid creating “match none / match all” flags by accident.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/47578#discussion_r2792348052,"oooh good catch.
---
fixed with e5a398974e",2026-02-11T09:53:25.651000,Empty array validation creates unintended match-all/match-none flag behavior,logic,high,8,"This is a meaningful logic bug where the validation allows empty arrays due to Python's `all([])` returning True, but the Rust matcher would cause `icontains []` to never match and `not_icontains []` to always match. This cross-language behavior mismatch could lead to users accidentally creating feature flags with unintended behavior affecting production. The catch requires understanding both Python and Rust codebases and their interaction.",2026-02-11T12:31:53.839823+00:00
ssvlabs/ssv,2677,message-validation: rework duplicate message rejections,https://github.com/ssvlabs/ssv/pull/2677,"`Peer()` always returns nil-checked state but never returns the created state

```suggestion
func (s *SignerStateForSlotRound) Peer(peerID peer.ID) *SignerState {
	state := s.Peers[peerID]
	if state == nil {
		state = &SignerState{}
		s.Peers[peerID] = state
	}
	return state
}
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: message/validation/signer_state.go
Line: 27:32

Comment:
`Peer()` always returns nil-checked state but never returns the created state

```suggestion
func (s *SignerStateForSlotRound) Peer(peerID peer.ID) *SignerState {
	state := s.Peers[peerID]
	if state == nil {
		state = &SignerState{}
		s.Peers[peerID] = state
	}
	return state
}
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/ssvlabs/ssv/pull/2677#discussion_r2759400679,"Not sure I understand your concern/comment, isn't the `return s.Peers[peerID]` I'm doing exactly the same as `return state` in the code you wrote ? Seems like it does the same thing to me.
---
Can you re-review the PR then (and maybe update the ""Confidence Score"" accordingly) ?",2026-02-03T14:34:25.023000,Newly created state object never returned to caller,logic,high,8,"This is a meaningful logic bug where the function creates a new SignerState when none exists but fails to return it, causing callers to receive nil and likely crash or behave incorrectly. This would cause production issues in peer state management and is not immediately obvious without careful code review.",2026-02-11T14:36:21.490382+00:00
ever-co/ever-gauzy,9436,fix: close unused window and init window when in use,https://github.com/ever-co/ever-gauzy/pull/9436,"**Wrong window shown**

In the tray menu `check_for_update` handler you initialize (and intend to show) the *settings* window, but the code calls `appWindowManager.setupWindow?.show?.()` instead of showing `settingWindow`. This makes “Check for update” open the setup window (or do nothing if it’s null) rather than navigating to the update UI.",https://github.com/ever-co/ever-gauzy/pull/9436#discussion_r2792569786,,2026-02-11T10:42:09.844000,Wrong window shown in Check for Update handler,logic,high,8,"This is a meaningful logic bug where clicking 'Check for update' in the tray menu would show the setup window instead of the settings window, causing incorrect user-facing behavior. The code explicitly initializes settingWindow but then calls setupWindow.show() instead.",2026-02-11T18:34:26.865684+00:00
