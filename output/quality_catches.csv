repo,pr_number,pr_title,pr_url,comment_body,comment_url,reply_body,created_at,bug_category,severity,llm_reasoning,evaluated_at
PostHog/posthog,45413,chore: Use pre-migrated database and update comment when containers start,https://github.com/PostHog/posthog/pull/45413,"**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: bin/hobby-ci.py
Line: 1045:1046

Comment:
**logic:** `update_containers_started_comment()` silently fails if no comment exists yet. The function returns early when `existing_comment` is `None`, but at the time cloud-init finishes (line 678), `update_smoke_test_comment()` hasn't been called yet to create the initial comment. This means the early notification feature doesn't work - users won't get the ""instance is accessible"" update.

Either create the comment here if it doesn't exist (similar to `update_smoke_test_comment()` at lines 1205-1214), or ensure the comment is pre-created before this function is called.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45413#discussion_r2706357612,"Fixed in aa3a4c5. The function now creates the comment if it doesn't exist yet (similar to `update_smoke_test_comment()`), ensuring users get the early notification that the instance is accessible even when called before the final smoke test comment is created.",2026-01-20T00:08:16+00:00,logic,medium,Greptile correctly identified that update_containers_started_comment() would silently fail because it tries to update a PR comment that doesn't exist yet at that point in the execution flow. The developer confirmed and fixed this bug. This is a real logic bug that would cause the 'early notification' feature to completely not work.,2026-01-28T22:49:05.093404+00:00
PostHog/posthog,45414,chore: Optimize Dagster CI tests: use pre-migrated schema and pytest-xdist,https://github.com/PostHog/posthog/pull/45414,"**logic:** Missing sqlx migrations for `posthog_persons` database. Tests that use `@pytest.mark.django_db(databases=[""default"", ""persons_db_writer""])` will fail because `posthog/conftest.py:run_persons_sqlx_migrations()` requires sqlx-cli to run migrations.

The old workflow ran sqlx commands to create the persons database and run migrations from `rust/persons_migrations/`. The persons migrations are not included in the `migrated-schema` artifact (which only dumps the `posthog` database per `ci-backend.yml:421-422`).

You need to either:
1. Keep the sqlx-cli installation and run the sqlx migrations here, OR
2. Include the persons database in the migrated-schema artifact in ci-backend.yml

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: .github/workflows/ci-dagster.yml
Line: 153:161

Comment:
**logic:** Missing sqlx migrations for `posthog_persons` database. Tests that use `@pytest.mark.django_db(databases=[""default"", ""persons_db_writer""])` will fail because `posthog/conftest.py:run_persons_sqlx_migrations()` requires sqlx-cli to run migrations.

The old workflow ran sqlx commands to create the persons database and run migrations from `rust/persons_migrations/`. The persons migrations are not included in the `migrated-schema` artifact (which only dumps the `posthog` database per `ci-backend.yml:421-422`).

You need to either:
1. Keep the sqlx-cli installation and run the sqlx migrations here, OR
2. Include the persons database in the migrated-schema artifact in ci-backend.yml

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/45414#discussion_r2706362315,,2026-01-20T00:12:40+00:00,logic,high,"This comment identifies a concrete missing functionality - the persons database migrations are removed but still required by tests. The comment correctly identifies that the old workflow ran sqlx migrations but the new workflow doesn't, and the migrated-schema artifact only contains the posthog database, not the posthog_persons database. This will cause test failures for any tests marked with `databases=[""default"", ""persons_db_writer""]`.",2026-01-28T22:49:05.093447+00:00
posthog/posthog,46208,fix(sig): re-use ongoing video export workflow,https://github.com/PostHog/posthog/pull/46208,"<details>
<summary>Additional Comments (1)</summary>

**`posthog/temporal/ai/session_summary/activities/a1_export_session_video.py`**
when catching `WorkflowAlreadyStartedError`, the code waits for the existing workflow but then returns the NEW `exported_asset.id` that was never processed. The existing workflow is processing a DIFFERENT asset ID (from the first request). This means the returned asset will never have its `content_location` or `content` populated.

need to either:
1. query for the existing asset (checking lines 46-54) BEFORE creating a new one when a workflow is already running, OR
2. look up and return the existing asset's ID that the already-running workflow is processing

the workflow_id is based on `team_id` and `session_id`, but each request creates its own unique `exported_asset`. when the second request catches the exception, it should return the FIRST request's asset ID, not its own.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: posthog/temporal/ai/session_summary/activities/a1_export_session_video.py
Line: 130:148

Comment:
when catching `WorkflowAlreadyStartedError`, the code waits for the existing workflow but then returns the NEW `exported_asset.id` that was never processed. The existing workflow is processing a DIFFERENT asset ID (from the first request). This means the returned asset will never have its `content_location` or `content` populated.

need to either:
1. query for the existing asset (checking lines 46-54) BEFORE creating a new one when a workflow is already running, OR
2. look up and return the existing asset's ID that the already-running workflow is processing

the workflow_id is based on `team_id` and `session_id`, but each request creates its own unique `exported_asset`. when the second request catches the exception, it should return the FIRST request's asset ID, not its own.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/PostHog/posthog/pull/46208#issuecomment-3814257126,,2026-01-28T22:28:21+00:00,data_integrity,high,"This is a genuine concurrency bug where catching WorkflowAlreadyStartedError leads to returning a newly created asset ID that will never be populated with data, since the existing workflow is processing a different asset. This would cause users to receive an empty/incomplete exported asset in normal concurrent usage scenarios. The analysis is correct and identifies a real data integrity issue.",2026-01-28T23:04:50.907478+00:00
nvidia-nemo/curator,1437,ci: Optimize docker layer for caching and add remote cache,https://github.com/NVIDIA-NeMo/Curator/pull/1437,"`${{ env.REPO_LOWER }}` is empty at line 130-131 - it's set at line 92 but composite actions don't make `env` variables available to subsequent steps. Cache and tag references will be malformed like `nemoci.azurecr.io/:abc123`

Set REPO_LOWER as output from the normalize step and reference `${{ steps.normalize.outputs.repo_lower }}` instead",https://github.com/NVIDIA-NeMo/Curator/pull/1437#discussion_r2738911992,,2026-01-28T22:38:52+00:00,runtime,high,"Comment 9 correctly identifies that `${{ env.REPO_LOWER }}` will be empty when used in the Docker build step because GitHub Actions composite actions don't propagate environment variables set in one step to subsequent steps. This will cause malformed image references like `nemoci.azurecr.io/:abc123` instead of `nemoci.azurecr.io/curator:abc123`, breaking the entire container build and push workflow. The comment also provides a concrete solution.",2026-01-28T23:04:50.907610+00:00
nvidia/numba-cuda,760,Replace legacy wheels-build.yaml with build-wheel.yml in publish workflow,https://github.com/NVIDIA/numba-cuda/pull/760,"The source distribution (sdist) is no longer being built or published. The old workflow downloaded `sdist-py3.13-amd64` artifact and published it to PyPI. The new workflow only publishes wheels.

The PR description states ""Add dedicated build-sdist job for source distribution"", but the actual implementation in commit 521da34 removed sdist building entirely with the note ""sdist not needed"". This contradicts the PR description and removes functionality that was present before.

Verify whether sdist is truly not needed for PyPI publishing, or if it should be restored.",https://github.com/NVIDIA/numba-cuda/pull/760#discussion_r2738854040,Updated initial comment,2026-01-28T22:18:53+00:00,logic,medium,"Greptile caught a discrepancy between the PR description claiming to 'Add dedicated build-sdist job for source distribution' and the actual implementation which removed sdist building entirely. This is a legitimate functionality removal that contradicts the stated PR intent. The developer acknowledged this by saying 'Updated initial comment', confirming the catch was valid.",2026-01-28T23:04:50.907653+00:00
onyx-dot-app/onyx,7962,fix(craft): auto set best model instead of checking for visibility,https://github.com/onyx-dot-app/onyx/pull/7962,"Missing model availability check - the function returns hardcoded model names without verifying they exist in the provider's `model_configurations`. The old implementation checked `matchingProvider.model_configurations.some((m) => m.name === model)` before selecting a model. This will cause users to be assigned non-existent models, breaking LLM functionality.

```suggestion
  // Try each priority provider in order
  for (const { provider, modelName } of LLM_SELECTION_PRIORITY) {
    const matchingProvider = llmProviders.find((p) => p.provider === provider);
    if (matchingProvider) {
      // Check if the model exists in the provider's configurations
      const hasModel = matchingProvider.model_configurations.some(
        (m) => m.name === modelName
      );
      if (hasModel) {
        return {
          providerName: matchingProvider.name,
          provider: matchingProvider.provider,
          modelName,
        };
      }
    }
  }
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/craft/onboarding/constants.ts
Line: 35:44

Comment:
Missing model availability check - the function returns hardcoded model names without verifying they exist in the provider's `model_configurations`. The old implementation checked `matchingProvider.model_configurations.some((m) => m.name === model)` before selecting a model. This will cause users to be assigned non-existent models, breaking LLM functionality.

```suggestion
  // Try each priority provider in order
  for (const { provider, modelName } of LLM_SELECTION_PRIORITY) {
    const matchingProvider = llmProviders.find((p) => p.provider === provider);
    if (matchingProvider) {
      // Check if the model exists in the provider's configurations
      const hasModel = matchingProvider.model_configurations.some(
        (m) => m.name === modelName
      );
      if (hasModel) {
        return {
          providerName: matchingProvider.name,
          provider: matchingProvider.provider,
          modelName,
        };
      }
    }
  }
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/onyx-dot-app/onyx/pull/7962#discussion_r2739148674,,2026-01-29T00:07:29+00:00,logic,high,"The comment correctly identifies that the new implementation removed a critical validation check - the code returns hardcoded model names (like 'claude-opus-4-5', 'gpt-5.2') without verifying these models actually exist in the provider's configuration. This is a regression from the old implementation that checked model availability. Users could be assigned non-existent models, breaking LLM functionality during normal usage of the onboarding flow.",2026-01-29T00:48:55.408962+00:00
credal-ai/actions-sdk,482,Create action that reads comments from docs,https://github.com/Credal-ai/actions-sdk/pull/482,"Google Drive API returns `id` not `commentId`. This will cause all comment IDs to be undefined.

```suggestion
      res.data.comments?.map((c: any) => ({
        commentId: c.id,
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: src/actions/providers/google-oauth/readCommentsOnDoc.ts
Line: 40:41

Comment:
Google Drive API returns `id` not `commentId`. This will cause all comment IDs to be undefined.

```suggestion
      res.data.comments?.map((c: any) => ({
        commentId: c.id,
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/Credal-ai/actions-sdk/pull/482#discussion_r2739225309,,2026-01-29T00:33:49+00:00,logic,high,"Greptile correctly identifies that the Google Drive API returns 'id' not 'commentId' for comments, which means all comment IDs would be undefined in the output. This is a real bug that would affect every user of this function in normal usage, making comment identification impossible. The catch demonstrates knowledge of the external API contract.",2026-01-29T00:48:55.409045+00:00
davis7dotsh/better-context,134,"update: use ""better result"" for handling errors, instead of try catch spam",https://github.com/davis7dotsh/better-context/pull/134,"The result is matched but not returned, causing `gitClone` to return `undefined` instead of throwing on error

```suggestion
	return result.match({
		ok: () => undefined,
		err: (error) => {
			throw error;
		}
	});
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: apps/server/src/resources/impls/git.ts
Line: 306:311

Comment:
The result is matched but not returned, causing `gitClone` to return `undefined` instead of throwing on error

```suggestion
	return result.match({
		ok: () => undefined,
		err: (error) => {
			throw error;
		}
	});
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/davis7dotsh/better-context/pull/134#discussion_r2739888680,,2026-01-29T04:21:28+00:00,logic,high,"Greptile correctly identifies that the `result.match()` call's return value is not being returned from the function, causing `gitClone` to silently return `undefined` instead of throwing errors on failure. This is a real bug introduced during the refactoring from try-catch to Result pattern - errors will be swallowed and git clone failures won't be properly propagated to callers, affecting normal usage.",2026-01-29T04:44:12.144348+00:00
raycast/extensions,24954,Add markdown-docs extension,https://github.com/raycast/extensions/pull/24954,"This PR adds a Git submodule reference instead of the actual extension files. Extensions should be submitted directly to the repository, not as submodules. Copy the extension files directly into `extensions/markdown-docs/` instead of adding a submodule pointer.",https://github.com/raycast/extensions/pull/24954#discussion_r2740056067,,2026-01-29T05:44:11+00:00,data_integrity,critical,"This catch identifies that the PR adds a Git submodule reference instead of actual extension files, which means the extension won't work at all when the PR is merged. This is a fundamental submission error that would completely break the extension in the Raycast extensions repository. The diff clearly shows only a submodule commit hash being added rather than actual code files.",2026-01-29T06:28:57.078799+00:00
nvidia/matx,1125,Avoid warning about unused variables,https://github.com/NVIDIA/MatX/pull/1125,"`unfused_range` is now undefined. `MATX_NVTX_START_RANGE` was changed to not assign to a variable, but line 36 still references `unfused_range` in `MATX_NVTX_END_RANGE(unfused_range)`.

```suggestion
    [[maybe_unused]] int unfused_range = MATX_NVTX_START_RANGE(""Unfused Kernels"");
    (result = cos(C)).run(exec);
    (result = result / D).run(exec);
    (result = result * B).run(exec);
    MATX_NVTX_END_RANGE(unfused_range);
```",https://github.com/NVIDIA/MatX/pull/1125#discussion_r2740872358,,2026-01-29T10:05:25+00:00,runtime,high,"Greptile correctly identified that removing the variable assignment from MATX_NVTX_START_RANGE creates undefined variable references. The code still uses `unfused_range` and `fused_range` in MATX_NVTX_END_RANGE calls, but these variables no longer exist after the PR changes. This will cause a compilation error, making it a clear bug introduced by the PR.",2026-01-29T10:25:30.134993+00:00
twentyhq/twenty,17530,Refactor `delete` `objectMetadata` action type and handler to be workspace agnostic,https://github.com/twentyhq/twenty/pull/17530,"inconsistent error handling: `update-field-action-handler.service.ts` uses `WorkspaceMigrationActionExecutionException` for this same check (line 104-107), but this throws a generic `Error`

```suggestion
    if (!isDefined(objectMetadataUniversalIdentifier)) {
      throw new WorkspaceMigrationActionExecutionException({
        message: `objectMetadataUniversalIdentifier is not defined for field metadata ${universalIdentifier}`,
        code: WorkspaceMigrationActionExecutionExceptionCode.FLAT_ENTITY_NOT_FOUND,
      });
    }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/twenty-server/src/engine/workspace-manager/workspace-migration/workspace-migration-runner/action-handlers/field/services/delete-field-action-handler.service.ts
Line: 72:76

Comment:
inconsistent error handling: `update-field-action-handler.service.ts` uses `WorkspaceMigrationActionExecutionException` for this same check (line 104-107), but this throws a generic `Error`

```suggestion
    if (!isDefined(objectMetadataUniversalIdentifier)) {
      throw new WorkspaceMigrationActionExecutionException({
        message: `objectMetadataUniversalIdentifier is not defined for field metadata ${universalIdentifier}`,
        code: WorkspaceMigrationActionExecutionExceptionCode.FLAT_ENTITY_NOT_FOUND,
      });
    }
```

<sub>Note: If this suggestion doesn't match your team's coding style, reply to this and let me know. I'll remember it for next time!</sub>

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/twentyhq/twenty/pull/17530#discussion_r2741102919,Nice catch,2026-01-29T11:04:56+00:00,logic,low,"Greptile caught an inconsistency in error handling where the delete handler throws a generic Error while the update handler uses the proper WorkspaceMigrationActionExecutionException for the same check. This was confirmed by the developer with 'Nice catch'. While this is a code consistency issue rather than a bug that would cause incorrect behavior, the developer confirmation makes it a valid catch.",2026-01-29T12:27:12.487687+00:00
gptme/gptme,1182,fix(llm): handle OpenRouter subprovider suffix in model lookup,https://github.com/gptme/gptme/pull/1182,"Suffix not preserved when model found via dynamic fetch. When `model_meta.model` matches `lookup_model_name`, returning `model_meta` directly loses the original suffix. Should create new ModelMeta with original `model_name` like the static path does:

```suggestion
                        # Check both full name (with suffix) and base name (without suffix)
                        if (
                            model_meta.model == model_name
                            or model_meta.model == lookup_model_name
                        ):
                            return ModelMeta(
                                provider, model_name, **model_meta.to_dict()
                            )
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: gptme/llm/models.py
Line: 566:570

Comment:
Suffix not preserved when model found via dynamic fetch. When `model_meta.model` matches `lookup_model_name`, returning `model_meta` directly loses the original suffix. Should create new ModelMeta with original `model_name` like the static path does:

```suggestion
                        # Check both full name (with suffix) and base name (without suffix)
                        if (
                            model_meta.model == model_name
                            or model_meta.model == lookup_model_name
                        ):
                            return ModelMeta(
                                provider, model_name, **model_meta.to_dict()
                            )
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme/pull/1182#discussion_r2741612984,"✅ Fixed in commit 6bc865aa - Now creates new ModelMeta with original `model_name` preserving the suffix, similar to the static path.",2026-01-29T13:19:20+00:00,logic,medium,"Greptile correctly identified that when a model is found via dynamic fetch using the base name (lookup_model_name), returning model_meta directly would lose the original suffix (e.g., @moonshotai) that the user requested. This is a real bug that would cause incorrect model name preservation in API calls. The developer confirmed and fixed it.",2026-01-29T14:29:03.640787+00:00
posthog/posthog,46271,feat: multiselect for statuses and priorities,https://github.com/PostHog/posthog/pull/46271,"test expects `NULL` priority tickets to be included when filtering by specific priorities, but the backend filter logic in `tickets.py:115-116` uses `priority__in` which won't match `NULL` values. Either fix the backend to include `NULL` priorities with `Q(priority__in=priorities) | Q(priority__isnull=True)`, or update the test expectation to exclude the `NULL` priority ticket

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: products/conversations/backend/api/tests/test_tickets.py
Line: 307:333

Comment:
test expects `NULL` priority tickets to be included when filtering by specific priorities, but the backend filter logic in `tickets.py:115-116` uses `priority__in` which won't match `NULL` values. Either fix the backend to include `NULL` priorities with `Q(priority__in=priorities) | Q(priority__isnull=True)`, or update the test expectation to exclude the `NULL` priority ticket

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46271#discussion_r2742332062,Fixed,2026-01-29T15:51:11+00:00,logic,medium,"Greptile correctly identified that the test expects NULL priority tickets to be included when filtering by specific priorities, but the backend's `priority__in` filter won't match NULL values in SQL. This is a real logic bug that was confirmed by the developer with 'Fixed'. The catch demonstrates understanding of both Django ORM behavior and SQL NULL semantics.",2026-01-29T16:28:21.841777+00:00
posthog/posthog,46262,feat(logs): json parse log lines in log-ingestion based on team settings,https://github.com/PostHog/posthog/pull/46262,"Double stringification - `value` is already a string from `flattenJson` (line 117-145 converts everything to strings), so `value.toString()` returns the same string, then `JSON.stringify()` adds extra quotes around it. This means a value like `""info""` becomes `""\""info\""""`.

```suggestion
        newAttributes[key] = value
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: nodejs/src/logs-ingestion/log-record-avro.ts
Line: 176:176

Comment:
Double stringification - `value` is already a string from `flattenJson` (line 117-145 converts everything to strings), so `value.toString()` returns the same string, then `JSON.stringify()` adds extra quotes around it. This means a value like `""info""` becomes `""\""info\""""`.

```suggestion
        newAttributes[key] = value
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/PostHog/posthog/pull/46262#discussion_r2742153502,,2026-01-29T15:14:36+00:00,data_integrity,high,"This catch identifies a double stringification bug where values already converted to strings get wrapped in extra quotes via JSON.stringify(), causing data corruption (e.g., 'info' becomes '""info""'). This would affect all users in normal usage and corrupt log attribute values in production. The analysis is correct - flattenJson already converts values to strings, so the additional JSON.stringify is incorrect.",2026-01-29T16:28:21.841900+00:00
nvidia/numba-cuda,762,test(refactor): clean up `run_in_subprocess`,https://github.com/NVIDIA/numba-cuda/pull/762,"Logic error: `check=True` (line 206) raises `CalledProcessError` on non-zero exit, making the `if proc.returncode:` check (line 212) unreachable for failures. Remove `check=True` to preserve the original `AssertionError` behavior.

```suggestion
    proc = subprocess.run(
        [sys.executable, *flags, ""-c"", code],
        env=env,
        capture_output=True,
        text=True,
        timeout=timeout,
    )
    out, err = proc.stdout, proc.stderr
    if proc.returncode:
        raise AssertionError(
            f""process failed with code {proc.returncode:d}: stderr:\n{err}\n""
        )
```",https://github.com/NVIDIA/numba-cuda/pull/762#discussion_r2742143212,"This review comment seems legitimate, but I imagine we'd be better off catching the Python exception instead of removing the `check=True`?
---
Seems legit.",2026-01-29T15:12:21+00:00,logic,medium,"Greptile correctly identified that `check=True` makes the subsequent `if proc.returncode:` check unreachable dead code, since `subprocess.run` will raise `CalledProcessError` before reaching that line. This changes the error behavior from raising `AssertionError` to raising `CalledProcessError`, which could break test expectations. The developer confirmed this was legitimate.",2026-01-29T16:28:21.841987+00:00
onyx-dot-app/onyx,7977,fix: input masking,https://github.com/onyx-dot-app/onyx/pull/7977,"<details>
<summary>Additional Comments (1)</summary>

**`web/src/app/admin/embeddings/modals/ChangeCredentialsModal.tsx`**
Missing `data-ph-no-capture` attribute. Since PostHog's `maskAllInputs` is now set to `false` (in `providers.tsx:21`), this API key input needs the explicit `data-ph-no-capture` attribute to prevent session recordings from capturing sensitive credentials.

```suggestion
                    <input
                      type=""password""
                      data-ph-no-capture
                      className=""border border-border rounded w-full py-2 px-3 bg-background-emphasis""
                      value={apiKey}
                      onChange={(e: any) => setApiKey(e.target.value)}
                      placeholder=""Paste your API key here""
                    />
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: web/src/app/admin/embeddings/modals/ChangeCredentialsModal.tsx
Line: 206:212

Comment:
Missing `data-ph-no-capture` attribute. Since PostHog's `maskAllInputs` is now set to `false` (in `providers.tsx:21`), this API key input needs the explicit `data-ph-no-capture` attribute to prevent session recordings from capturing sensitive credentials.

```suggestion
                    <input
                      type=""password""
                      data-ph-no-capture
                      className=""border border-border rounded w-full py-2 px-3 bg-background-emphasis""
                      value={apiKey}
                      onChange={(e: any) => setApiKey(e.target.value)}
                      placeholder=""Paste your API key here""
                    />
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>
</details>",https://github.com/onyx-dot-app/onyx/pull/7977#issuecomment-3819293947,,2026-01-29T17:57:22+00:00,security,critical,"This is a legitimate security vulnerability catch. The PR disabled global input masking (maskAllInputs: false) for PostHog session recordings, but failed to add the data-ph-no-capture attribute to an API key input field. This means sensitive credentials would be captured in session recordings and sent to PostHog, exposing them to anyone with access to those recordings. This is a real data exposure issue that directly contradicts the PR's intent to fix input masking.",2026-01-29T18:29:27.407222+00:00
openreplay/openreplay,4265,refactor(chalice): changed displayName&description handling refactor(DB): changed displayName&description handling,https://github.com/openreplay/openreplay/pull/4265,"Missing data migration - existing `display_name`, `description`, and `status` values should be migrated to the new `_customized` tables before dropping columns

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: ee/scripts/schema/db/init_dbs/clickhouse/1.25.0/1.25.0.sql
Line: 148:158

Comment:
Missing data migration - existing `display_name`, `description`, and `status` values should be migrated to the new `_customized` tables before dropping columns

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/openreplay/openreplay/pull/4265#discussion_r2742737361,,2026-01-29T17:25:46+00:00,data_integrity,critical,"This catch identifies a critical data loss issue - the migration script drops columns containing existing user data (display_name, description, status) before migrating that data to the new _customized tables. Running this migration in production would permanently destroy user customizations. This is a guaranteed data loss bug that would affect all existing users.",2026-01-29T18:29:27.407387+00:00
gptme/gptme-contrib,225,feat(gptodo): add effective command to show computed task state,https://github.com/gptme/gptme-contrib/pull/225,"`blocked` state missing from `STATE_STYLES` dict

The virtual `blocked` state returned by `compute_effective_state()` isn't defined in the `STATE_STYLES` dictionary (utils.py:157-181), so it falls back to `white` styling. Consider adding an entry like:

```
""blocked"": (""red"", ""blocked""),
```

to utils.py STATE_STYLES for consistent styling of blocked tasks.

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/cli.py
Line: 263:263

Comment:
`blocked` state missing from `STATE_STYLES` dict

The virtual `blocked` state returned by `compute_effective_state()` isn't defined in the `STATE_STYLES` dictionary (utils.py:157-181), so it falls back to `white` styling. Consider adding an entry like:

```
""blocked"": (""red"", ""blocked""),
```

to utils.py STATE_STYLES for consistent styling of blocked tasks.

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/225#discussion_r2742594440,"✅ Fixed in commit 93c0ba0 - added `blocked` virtual state with red styling to STATE_STYLES.

Good catch! The comment distinguishes it from stored frontmatter states.",2026-01-29T16:50:36+00:00,logic,low,"Greptile correctly identified that the new 'blocked' virtual state returned by compute_effective_state() was missing from STATE_STYLES, causing it to fall back to default white styling instead of appropriate red styling. The developer confirmed this with '✅ Fixed' and 'Good catch!'. While not a critical bug, it's a valid issue that would affect user experience by making blocked tasks visually indistinct.",2026-01-29T18:29:27.407441+00:00
greptileai/react-mirror,335,[Flight] Forward early debug info for lazy chunks,https://github.com/greptileai/react-mirror/pull/335,"Field name mismatch - initialized as `forwardedDebugInfos` but type definition at line 617 declares `partialDebugInfoProgress`. This will cause the WeakMap to never be initialized, breaking the progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/react-server/src/ReactFlightServer.js
Line: 742:742

Comment:
Field name mismatch - initialized as `forwardedDebugInfos` but type definition at line 617 declares `partialDebugInfoProgress`. This will cause the WeakMap to never be initialized, breaking the progressive debug info tracking.

```suggestion
    this.partialDebugInfoProgress = null;
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/greptileai/react-mirror/pull/335#discussion_r2744122368,,2026-01-30T00:39:33+00:00,type_error,high,Greptile correctly identified a field name mismatch where the type definition declares `partialDebugInfoProgress` but the initialization uses `forwardedDebugInfos`. This is a clear bug that would cause the progressive debug info tracking feature to completely fail since the WeakMap would never be properly initialized under the expected field name. The code would attempt to access `partialDebugInfoProgress` but find null/undefined instead of the WeakMap.,2026-01-30T00:50:50.252520+00:00
gptme/gptme-contrib,230,feat(plugins): add gptme-retrieval plugin for automatic context retrieval,https://github.com/gptme/gptme-contrib/pull/230,"Shell injection vulnerability - user-controlled `backend` value passed directly to shell. If backend comes from untrusted config, this allows arbitrary command execution.

```suggestion
    cmd = [backend, query, ""--mode"", mode, ""-n"", str(max_results)]
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: plugins/gptme-retrieval/src/gptme_retrieval/__init__.py
Line: 201:201

Comment:
Shell injection vulnerability - user-controlled `backend` value passed directly to shell. If backend comes from untrusted config, this allows arbitrary command execution.

```suggestion
    cmd = [backend, query, ""--mode"", mode, ""-n"", str(max_results)]
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/230#discussion_r2744032789,,2026-01-30T00:02:55+00:00,security,critical,"This is a legitimate shell injection vulnerability where user-controlled config values (backend) are passed to subprocess execution. The suggestion to use a list format instead of shell=True prevents command injection attacks. While the config comes from toml files (somewhat trusted), allowing arbitrary custom commands makes this a real security concern.",2026-01-30T00:50:50.252618+00:00
gptme/gptme-contrib,229,fix(gptodo): ensure tmux sessions inherit environment variables,https://github.com/gptme/gptme-contrib/pull/229,"`workspace` is not shell-escaped, creating command injection vulnerability if workspace path contains special characters

```suggestion
        full_cmd = f""bash -l -c {shlex.quote(f'cd {shlex.quote(str(workspace))} && {env_setup}{shell_cmd}')}""
```

<details><summary>Prompt To Fix With AI</summary>

`````markdown
This is a comment left during a code review.
Path: packages/gptodo/src/gptodo/subagent.py
Line: 173:173

Comment:
`workspace` is not shell-escaped, creating command injection vulnerability if workspace path contains special characters

```suggestion
        full_cmd = f""bash -l -c {shlex.quote(f'cd {shlex.quote(str(workspace))} && {env_setup}{shell_cmd}')}""
```

How can I resolve this? If you propose a fix, please make it concise.
`````
</details>",https://github.com/gptme/gptme-contrib/pull/229#discussion_r2743888735,✅ Fixed in commit ee90d31 - added `safe_workspace = shlex.quote(str(workspace))` before using it in the command.,2026-01-29T23:01:24+00:00,security,high,"Greptile correctly identified a command injection vulnerability where the workspace path was not shell-escaped when used inside the nested shell command. The developer confirmed and fixed the issue. While workspace paths containing special characters aren't common, this is a real security bug that could cause incorrect behavior or exploitation in production environments.",2026-01-30T00:50:50.252678+00:00
